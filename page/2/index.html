<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  <title>lightsmile&#39;s Blog</title>
  
  <meta name="keywords" content="Python,哲学,NLP,自然语言处理,lightsmile,李德方">
  
  
  <meta name="description" content="this is a description">
  

  <link rel="alternate" href="/atom.xml" title="lightsmile's Blog">

  <meta name="HandheldFriendly" content="True" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <!-- meta -->
  
  
  <meta name="theme-color" content="#f24e32">
  
  <meta name="msapplication-TileColor" content="#f24e32">
  
  <meta name="msapplication-config" content="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/browserconfig.xml">
  
  
  <!-- link -->
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.css">
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.6.3/css/all.min.css">

  
  
  <link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicon.ico"
   type="image/x-icon"
  
  
  
  >
  
  <link rel="icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/favicon-32x32.png"
   type="image/x-icon"
   sizes="32x32"
  
  
  >
  
  <link rel="apple-touch-icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/apple-touch-icon.png"
   type="image/png"
   sizes="180x180"
  
  
  >
  
  <link rel="mask-icon" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/safari-pinned-tab.svg"
  
  
  
  
   color="#f24e32">
  
  <link rel="manifest" href="https://cdn.jsdelivr.net/gh/xaoxuu/assets@18.12.27/favicon/favicons/site.webmanifest"
  
  
  
  
  >
  
  

  
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@1.0/css/style.css">
    
  

  



  <script>
    function setLoadingBarProgress(num) {
      document.getElementById('loading-bar').style.width=num+"%";
    }
  </script>

  
    <!-- ga -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      ga('create', 'UA-131799461-1', 'www.iamlightsmile.com');
      ga('send', 'pageview');
    </script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  
  
</head>

<body>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><!-- 小红心 -->
    <script type="text/javascript" src="/js/love.js"></script>
    <div id="loading-bar-wrapper">
  <div id="loading-bar" class="pure"></div>
</div>

    <script>setLoadingBarProgress(20)</script>
    <header class="l_header pure">
	<div class='wrapper'>
		<div class="nav-main container container--flex">
      <a class="logo flat-box" href='/' >
        
          lightsmile's Blog
        
      </a>
			<div class='menu'>
				<ul class='h-list'>
          
  					
  						<li>
								<a id="https:www.iamlightsmile.com"
								 class="nav flat-box" href="https://www.iamlightsmile.com/">
									<i class='fas fa-home fa-fw'></i>&nbsp;主页
								</a>
							</li>
      			
  						<li>
								<a id="home"
								 class="nav flat-box" href="/">
									<i class='fas fa-rss fa-fw'></i>&nbsp;博客
								</a>
							</li>
      			
  						<li>
								<a id="archives"
								 class="nav flat-box" href="/archives/">
									<i class='fas fa-archive fa-fw'></i>&nbsp;归档
								</a>
							</li>
      			
  						<li>
								<a id="friends"
								 class="nav flat-box" href="/friends/">
									<i class='fas fa-users fa-fw'></i>&nbsp;朋友
								</a>
							</li>
      			
      		
				</ul>
			</div>

			
				<div class="m_search">
					<form name="searchform" class="form u-search-form">
						<input type="text" class="input u-search-input" placeholder="搜索" />
						<span class="icon"><i class="fas fa-search fa-fw"></i></span>
					</form>
				</div>
			
			<ul class='switcher h-list'>
				
					<li class='s-search'><a class="fas fa-search fa-fw" href='javascript:void(0)'></a></li>
				
				<li class='s-menu'><a class="fas fa-bars fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>

		<div class='nav-sub container container--flex'>
			<a class="logo flat-box"></a>
			<ul class='switcher h-list'>
				<li class='s-comment'><a class="flat-btn fas fa-comments fa-fw" href='javascript:void(0)'></a></li>
				<li class='s-toc'><a class="flat-btn fas fa-list fa-fw" href='javascript:void(0)'></a></li>
			</ul>
		</div>
	</div>
</header>
	<aside class="menu-phone">
    <header>
		<nav class="menu">
      <ul>
          
              
                  <li>
										<a id="https:www.iamlightsmile.com" class="nav flat-box" href="https://www.iamlightsmile.com/">
											<i class='fas fa-home fa-fw'></i>&nbsp;主页
										</a>
                  </li>
              
                  <li>
										<a id="home" class="nav flat-box" href="/">
											<i class='fas fa-rss fa-fw'></i>&nbsp;博客
										</a>
                  </li>
              
                  <li>
										<a id="archives" class="nav flat-box" href="/archives/">
											<i class='fas fa-archive fa-fw'></i>&nbsp;归档
										</a>
                  </li>
              
                  <li>
										<a id="friends" class="nav flat-box" href="/friends/">
											<i class='fas fa-users fa-fw'></i>&nbsp;朋友
										</a>
                  </li>
              
       
      </ul>
		</nav>
    </header>
	</aside>

    <script>setLoadingBarProgress(40);</script>
    <div class="l_body">
    <div class='container clearfix'>
        <div class='l_main'>
            

<section class="post-list">
    
    
      
        <div class='post-wrapper'>
          <article class="post reveal ">
    
<section class='meta'>
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/04/17/学习Python设计模式/">
              
                  学习Python设计模式
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-04-17
      </time>
    

    
      

    

    

    

  </div>
</section>

    <section class="article typo">
        <p>本书主要参阅的书籍是《精通Python设计模式》</p>
<p>本书分为创建型模式、结构型模式、行为型模式三大类，同时又细分为16种模式。<br>具体到每个模式，则通过简单介绍、现实生活中例子、软件应用实例、应用场景、具体代码实现、小结几部分，多个角度加深对某个设计模式的理解。<br>案例贴近生活，代码简单易懂，描述清晰明白，翻译水平上佳，确实算是我认为的好书，同时翻译还将代码上传到GitHub上方便读者下载学习，这里真应该点个赞了！</p>
<p>我们在学习设计模式的时候不应当仅仅立足于软件开发这一角度，同时应该立足于实际，或者以更加抽象的角度来看待这些设计模式背后的思想。比如针对某个设计模式，我们要明白想通过它实现怎么样的功能，这样设计的好处在哪里，我要提供什么输入，我将得到什么输出，即通过函数或黑盒子的视角从外面看待这个设计模式。同时也要将视角放到该设计模式的内部，可以把具体的某个方法视为车间、某个对象视为工人，将之与现实世界某个应用场景映射起来，分析通过怎么样的一个调度实现了业务的功能，其中的结构优势何在，节省了空间还是时间上的资源等。</p>
<h1 id="笔记摘抄"><a href="#笔记摘抄" class="headerlink" title="笔记摘抄"></a>笔记摘抄</h1><ol>
<li>设计模式的本质是在已有的方案之上发现更好的方案（而不是全新发明）。</li>
<li>设计模式并非是某种高大上或者神秘的东西，而是一些常见的软件工程设计问题的最佳实践方案。</li>
<li>个人认为软件开发技术的学习都应该以实践为前提，只有理解实践过程中遇到的种种问题，才能明白那些技术的本质和目的是什么，因为每种新技术都是因某个/某些问题而出现的。</li>
<li>设计模式一般是描述如何组织代码和使用最佳实践来解决常见的设计问题。</li>
</ol>
<h1 id="书籍结构"><a href="#书籍结构" class="headerlink" title="书籍结构"></a>书籍结构</h1><h3 id="创建型模式"><a href="#创建型模式" class="headerlink" title="创建型模式"></a>创建型模式</h3><p>创建型设计模式处理对象创建相关的问题，目标是当直接创建对象不太方便时，提供更好的方式。</p>
<h4 id="工厂模式"><a href="#工厂模式" class="headerlink" title="工厂模式"></a>工厂模式</h4><p>工厂背后的思想是简化对象的创建</p>
<p>通过将创建对象的代码和使用对象的代码解耦，工场能够降低应用维护的复杂度。</p>
<p>工厂通常有两种形式：一种是工厂方法，它是一个方法（或以地道的Python数据来说，是一个函数），对不同的输入参数返回不同的对象；第二种是抽象工厂，它是一组用于创建一系列相关事物对象的工厂方法。</p>
<p>工厂方法可以在必要时创建新的对象，从而提高性能和内存使用率。</p>
<p>一个抽象工厂是（逻辑上的）一组工厂方法，其中的每个工厂方法负责产生不同种类的对象。</p>
<p>抽象工厂模式是工厂方法模式的一种泛化。</p>
<p>通常一开始时使用工厂方法，因为它更简单。</p>
<p>工厂方法和抽象工厂设计模式可适用于以下场景：</p>
<ul>
<li>想要追踪对象莱恩创建时</li>
<li>想要将对象的创建与使用解耦时</li>
<li>想要优化应用的性能和资源占用时</li>
</ul>
<h4 id="建造者模式"><a href="#建造者模式" class="headerlink" title="建造者模式"></a>建造者模式</h4><p>建造者模式将一个复杂对象的构造过程与其表现分离，这样，同一个构造过程可用于创建多个不同的表现。</p>
<p>如果我们知道一个对象必须经过多个步骤来创建，并且要求同一个构造过程可以产生不同的表现，就可以使用建造者模式。</p>
<p>建造者模式可适用于以下场景：</p>
<ul>
<li>想要创建一个复杂对象（对象由多个部分构成，且对象的创建要经过多个不同的步骤，这些步骤也许还需遵从特定的顺序）</li>
<li>要求一个对象能有不同的表现，并希望将对象的构造与表现解耦</li>
<li>想要在某个时间点创建对象，但在稍后的时间点再访问</li>
</ul>
<h4 id="原型模式"><a href="#原型模式" class="headerlink" title="原型模式"></a>原型模式</h4><p>原型设计模式用于创建对象的完全副本。</p>
<p>一般创建副本的两种方式：</p>
<ul>
<li>当创建一个浅副本时，副本依赖引用</li>
<li>当创建一个深副本时，副本复制所有东西</li>
</ul>
<p>第一种情况下，我们希望提升应用性能和优化内存使用，在对象之间引入数据共享，但需要小心地修改数据，因为所有变更对所有副本都是可见的。</p>
<p>第二种情况下，我们希望能够对一个副本进行更改而不会影响其他对象。</p>
<h3 id="结构型模式"><a href="#结构型模式" class="headerlink" title="结构型模式"></a>结构型模式</h3><p>结构型设计模式处理一个系统中不同实体（比如，类和对象）之间的关系，关注的是提供一种简单的对象组合方式来创建新功能。</p>
<h4 id="适配器模式"><a href="#适配器模式" class="headerlink" title="适配器模式"></a>适配器模式</h4><p>适配器模式帮助我们实现两个不兼容接口之间的兼容</p>
<p>开放/封闭原则（open/close principle）是面向对象设计的基本原则之一，声明一个软件实体应该对拓展是开放的，对修改则是封闭的。本质上这意味着我们应该无需修改一个软件实体的源码就能拓展其行为。适配器模式遵从开放/封闭原则。</p>
<p>适配器让一件产品在制造出来之后需要应对新需求之时还能工作。</p>
<h4 id="修饰器模式"><a href="#修饰器模式" class="headerlink" title="修饰器模式"></a>修饰器模式</h4><p>修饰器模式通常用于拓展一个对象的功能。</p>
<p>一般来说，应用中有些部件是通用的，可应用于其他部件，这样的部件被看作横切关注点。</p>
<p>修饰器模式是实现横切关注点的绝佳方案，因为横切关注点通用但不太适合使用面向对象编程范式来实现。</p>
<p>Python进一步拓展了修饰器的概念，允许我们无需使用继承或组合就能拓展任意可调用对象（函数、方法或类）的行为。</p>
<h4 id="外观模式"><a href="#外观模式" class="headerlink" title="外观模式"></a>外观模式</h4><p>外观模式有助于隐藏系统所的内部复杂性，并通过一个简化的接口向客户端暴露必要的部分。本质上，外观（Facade）是在已有复杂系统之上实现的一个抽象层。</p>
<p>使用外观模式的最常见理由是为一个复杂系统提供单个简单的入口点。</p>
<p>不把系统的内部功能暴露给客户端代码有一个额外的好处：我们可以改变系统内部代码，但客户端代码不用关心这个改变，也不会受到这个改变的影响。客户端代码不需要进行任何改变。</p>
<p>如果你的系统包含多层，外观模式也能派上用场。你可以为每一层引入一个外观入口点，并让所有层级通过它们的外观相关通信。这提高了层级之间的松耦合性，尽可能保持层级独立。</p>
<p>外观模式是一种隐藏系统复杂性的优雅方式，因为多数情况下客户端代码并不应该关心系统的这些细节。</p>
<h4 id="享元模式"><a href="#享元模式" class="headerlink" title="享元模式"></a>享元模式</h4><p>作为软件工程师，我们应该编写更好的软件来解决软件问题，而不是要求客户购买更多更好的硬件。</p>
<p>享元模式通过为相似对象引入数据共享来最小化内存使用，提升性能。</p>
<p>一个享元（Flyweight）就是一个包含状态独立的不可变（又称固有的）数据的共享对象。</p>
<p>若想要享元模式有效，需要满足GoF的《设计模式》一书罗列的以下几个条件。</p>
<ul>
<li>应用需要使用大量的对象。</li>
<li>对象太多，存储/渲染它们的代价太大。</li>
<li>对象ID对于应用不重要。</li>
</ul>
<p>一般来说，在应用需要创建大量的计算代价大但共享许多属性的对象时，可以使用享元。</p>
<h4 id="模型-视图-控制器模式"><a href="#模型-视图-控制器模式" class="headerlink" title="模型-视图-控制器模式"></a>模型-视图-控制器模式</h4><p>关注点分离（Separation of Concerns，SoC）原则是软件工程相关的设计原则之一。</p>
<p>SoC原则背后的思想是将一个应用切分成不同的部分，每个部分解决一个单独的关注点。</p>
<p>模型-视图-控制器（Model-View-Controller，MVC）模式是应用到面向对象编程的SOC原则。</p>
<p>其中，模型是核心的部分，代表着应用的信息本源，包含和管理（业务）逻辑、数据、状态以及应用的规则。视图是模型的可视化表现。视图只是展示数据，并不处理数据。控制器是模型与视图之间的链接/粘附。模型与视图之间的所有通信都通过控制器进行。</p>
<p>为了实现模型与其表现之间的解耦，每个视图通常都需要属于它的控制器。（我认为作者想表达的是没有控制器也可以，但是由于视图并不具备处理数据的功能，并且通常每个页面所要处理的业务都会不同，因此这样的模型是高度定制的模型，适用性很差）</p>
<p>在从头开始实现MVC时，请确保创建的模型很智能，控制器很瘦，视图很傻瓜。</p>
<p>可以将具有以下功能的模型视为智能模型：</p>
<ul>
<li>包含所有的校验/业务规则/逻辑</li>
<li>处理应用的状态</li>
<li>访问应用数据（数据库、云或其他）</li>
<li>不依赖UI<br>可以将符合以下条件的控制器视为瘦控制器：</li>
<li>在用户与视图交互时，更新模型</li>
<li>在模型改变时，更新视图</li>
<li>如果需要，在数据传递给模型/视图之前进行处理</li>
<li>不展示数据</li>
<li>不直接访问应用数据</li>
<li>不包含校验/业务规则/逻辑<br>可以将符合以下条件的视图视为傻瓜视图：</li>
<li>展示数据</li>
<li>允许用户与其交互</li>
<li>仅做最小的数据处理，通常由一种模板语言提供处理能力（例如，使用简单的变量和循环控制）</li>
<li>不存储任何数据</li>
<li>不直接访问应用数据</li>
<li>不包含校验/业务规则/逻辑</li>
</ul>
<p>使用MVC时，请确保创建智能的模型（核心功能）、瘦控制器（实现视图与模型之间通信的能力）以及傻瓜式的视图（外在表现，最小化逻辑处理）</p>
<h4 id="代理模式"><a href="#代理模式" class="headerlink" title="代理模式"></a>代理模式</h4><p>四种不同的知名代理类型：</p>
<ul>
<li>远程代理：实际存在于不同地址空间（例如，某个网络服务器）的对象在本地的代理者。</li>
<li>虚拟代理：用于懒初始化，将一个大计算量对象的创建延迟到真正需要的时候进行。</li>
<li>保护/防护代理：控制对敏感对象的访问。</li>
<li>智能（引用）代理：在对象被访问时执行额外的动作。此类代理的例子包括引用计数和线程安全检查。</li>
</ul>
<p>现实中，，永远不要执行以下操作：</p>
<ul>
<li>在源代码中存储密码</li>
<li>以明文形式存储密码</li>
<li>使用一种弱（例如，MD5）或自定义加密形式</li>
</ul>
<h3 id="行为型模式"><a href="#行为型模式" class="headerlink" title="行为型模式"></a>行为型模式</h3><p>行为型模式处理对象互联和算法的问题</p>
<h4 id="责任链模式"><a href="#责任链模式" class="headerlink" title="责任链模式"></a>责任链模式</h4><p>责任链（Chain of Responsibility）模式用于让多个对象来处理单个请求时，或者用于预先不知道应该由哪个对象（来自某个对象链）来处理某个特定请求时。其原则如下所示：</p>
<ol>
<li>存在一个对象链（链表、树或任何其他便捷的数据结构）。</li>
<li>我们一开始将请求发送给链中的第一个对象。</li>
<li>对象决定其是否要处理该请求。</li>
<li>对象将请求转发给下一个对象。</li>
<li>重复该过程，直到到达链尾。</li>
</ol>
<p>通过使用责任链模式，我们能让许多对象来处理一个特定请求。在我们预先不知道应该由哪个对象来处理某个请求时，这是有用的。另一个责任链可以派上用场的场景是，在我们知道可能会有多个对象都需要对同一个请求进行处理之时。</p>
<p>这一模式的价值在于解耦。</p>
<p>客户端与所有处理程序（一个处理程序与所有其他处理程序之间也是如此）之间不再是多对多关系，客户端仅需要知道如何与链的起始节点（标头）进行通信。</p>
<p>在无法预先知道处理程序的数量和类型时，该模式有助于对请求/处理事件进行建模。适合使用责任链模式的系统例子包括基于事件的系统、采购系统和运输系统。</p>
<h4 id="命令模式"><a href="#命令模式" class="headerlink" title="命令模式"></a>命令模式</h4><p>命令设计模式帮助我们将一个操作（撤销、重做、复制、粘贴）封装成一个对象。简而言之，这意味着创建一个类，包含实现该操作所需要的所有逻辑和方法。</p>
<p>这样做的优势如下所述参考：</p>
<ul>
<li>我们并不需要直接执行一个命令。命令可以按照希望执行。</li>
<li>调用命令的对象与知道如何执行命令的对象解耦。调用者无需知道命令的任何实现细节。</li>
<li>如果有意义，可以把多个命令组织起来，这样调用者能够按顺序执行它们。例如，在实现一个多层撤销命令时，这是很有用的。</li>
</ul>
<h4 id="解释器模式"><a href="#解释器模式" class="headerlink" title="解释器模式"></a>解释器模式</h4><p>解释器（Interpreter）模式仅能引起应用的高级用户的兴趣。这是因为解释器模式背后的贮存思想是让非初级用户和领域专家使用一门简单的语言来表达想法。</p>
<p>领域特定语言（Domain Specific Language，DSL）是一种针对一个特定领域的有限表达能力的计算机语言。DSL分为内部DSL和外部DSL。</p>
<p>内部DSL构建在一个宿主编程语言之上。</p>
<ul>
<li>优势：我们不必担心创建、编译及解析语法，因为这些已经被宿主语言解决掉了。</li>
<li>劣势：会受限于宿主语言的特性。如果宿主语言不具备这些特性，构建一种表达能力强、简洁而且优美的内部DSL是富有挑战性的。</li>
</ul>
<p>外部DSL不依赖某种宿主语言。DSL的创建者可以决定语言的方方面面（语法、句法等），但也要负责为其创建一个解析器和编译器。为一种新语言创建解析器和编译器是一个非常复杂、长期而又痛苦的过程。</p>
<p>解释器模式仅与内部DSL相关。</p>
<h4 id="观察者模式"><a href="#观察者模式" class="headerlink" title="观察者模式"></a>观察者模式</h4><p>观察者模式描述单个对象（发布者，又称为主持者或可观察者）与一个或多个对象（订阅者，又称为观察者）之间的发布-订阅关系。</p>
<p>观察者模式背后的思想等同于MVC和关注点分离原则背后的思想，即降低发布者与订阅者之间的耦合度，从而易于在运行时添加/删除订阅者。此外，发布者不关心它的订阅者是谁。它只是将通知发送给所有订阅者。</p>
<p>当我们希望在一个对象（主持者/发布者/可观察者）发生变化时通知/更新另一个或多个对象的时候，通常会使用观察者模式。观察者的数量以及谁是观察者可能会有所不同，也可以（在运行时）动态地改变。</p>
<h4 id="状态模式"><a href="#状态模式" class="headerlink" title="状态模式"></a>状态模式</h4><p>在很多问题中，有限状态机（通常名为有限状态机）是一个非常方便的状态转换建模（并在必要时以数学方式形式化）工具。</p>
<p>状态机是一个抽象机器，有两个关键部分，状态和转换。状态是指系统的当前（激活）状况。一个状态机在任意时间点只会有一个激活状态。转换是指从一个状态切换到另一个状态，因某个事件或条件的触发而开始。在一个转换发生之前或之后通常会执行一个或多个动作。</p>
<p>状态机带一个不错的特性是可以用图来表现（称为状态图），其中每个状态都是一个节点，每个转换都是两个节点之间的边。</p>
<p>状态模式就是应用到一个特定软件工程问题的状态机。</p>
<p>使用状态模式本质上相当于实现一个状态机来解决特定领域的一个软件问题。</p>
<p>状态设计模式解决的是一定上下文中无限数量状态的完全封装，从而实现更好的可维护性和灵活性。</p>
<h4 id="策略模式"><a href="#策略模式" class="headerlink" title="策略模式"></a>策略模式</h4><p>策略模式通常用在我们希望对同一个问题透明地使用多种方案时。如果并不存在针对所有输入数据和所有情况的完美算法，那么我们可以使用策略模式，动态地决定在每种情况下应使用哪种算法。</p>
<h4 id="模板模式"><a href="#模板模式" class="headerlink" title="模板模式"></a>模板模式</h4><p>编写优秀代码的一个要素是避免冗余。</p>
<p>模板模式关注的是消除代码冗余，其思想是我们应该无需改变算法结构就能重新定义一个算法的某个部分。</p>
<h1 id="设计模式是被发现，而不是被发明出来的。"><a href="#设计模式是被发现，而不是被发明出来的。" class="headerlink" title="设计模式是被发现，而不是被发明出来的。"></a>设计模式是被发现，而不是被发明出来的。</h1>
        

        
    </section>
</article>

        </div>
      
    
      
        <div class='post-wrapper'>
          <article class="post reveal ">
    
<section class='meta'>
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/04/07/哈工大ltp小试/">
              
                  哈工大ltp小试
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-04-07
      </time>
    

    
      

    

    

    

  </div>
</section>

    <section class="article typo">
        <p>今天开始探索学习使用哈工大的LTP（Language Technology Platform）。</p>
<p>这里是<a href="http://ltp.ai/" target="_blank" rel="noopener">官网地址</a></p>
<p>这里是<a href="https://github.com/HIT-SCIR/ltp" target="_blank" rel="noopener">GitHub地址</a></p>
<p>这里是<a href="http://pyltp.readthedocs.io/zh_CN/latest/api.html" target="_blank" rel="noopener">pyltp的使用文档</a></p>
<p>平台采用的语言是C++，但是也提供了Python和Java的封装。由于本人目前使用Python作为自然语言处理的工具语言，所以以下的探索流程都是使用本人电脑中的Window8.1操作系统的PyCharm集成开发环境，使用的Python版本是3.6。</p>
<p>使用流程很简单：</p>
<ol>
<li>下载最新版本的<a href="http://ltp.ai/download.html" target="_blank" rel="noopener">模型</a>（目前是3.4）</li>
<li>安装pyltp，在命令行输入指令：<code>pip install pyltp</code>。<br>不过我的一直安装失败，总是不成功，后来从网上找了pyltp 3.6的<a href="http://mlln.cn/2018/01/31/pyltp%E5%9C%A8windows%E4%B8%8B%E7%9A%84%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/pyltp-0.2.1-cp36-cp36m-win_amd64.whl" target="_blank" rel="noopener">whl</a>文件，然后通过<code>pip install pyltp-0.2.1-cp36-cp36m-win_amd64.whl</code>成功了。</li>
</ol>
<p>这里有一篇<a href="http://mlln.cn/2018/01/31/pyltp%E5%9C%A8windows%E4%B8%8B%E7%9A%84%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/" target="_blank" rel="noopener">博客</a>专门讲安装的，发现他的界面有点黑客风，挺炫酷的哈~</p>
<p>通过PyCharm新建Python项目后，整个工程长成这个样子：</p>
<p><img src="https://github.com/smilelight/images/raw/master/learnLTPtest/project-structure.png" alt="project-structure"></p>
<p>不过其中的data文件夹、source文件夹和test文件夹是自己新创建的，其中data文件夹内放置下载的模型，source文件夹放置那个其实也没啥东西的txt文件，test中放置编写的Python代码。</p>
<p>照着使用文档敲的<code>testltp.py</code>内容如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pyltp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分句</span></span><br><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> SentenceSplitter</span><br><span class="line">sents = SentenceSplitter.split(<span class="string">'元芳你怎么看？我就趴窗口上看呗！'</span>)</span><br><span class="line">print(<span class="string">'\n'</span>.join(sents))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词</span></span><br><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Segmentor</span><br><span class="line">segmentor = Segmentor()</span><br><span class="line">segmentor.load(<span class="string">'../data/cws.model'</span>)</span><br><span class="line">words = segmentor.segment(<span class="string">"元芳你怎么看"</span>)</span><br><span class="line">segmentor.release()</span><br><span class="line">print(list(words))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 词性标注</span></span><br><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Postagger</span><br><span class="line">postagger = Postagger()</span><br><span class="line">postagger.load(<span class="string">'../data/pos.model'</span>)</span><br><span class="line">posts = postagger.postag(list(words))</span><br><span class="line">postagger.release()</span><br><span class="line">print(list(zip(list(words),list(posts))))</span><br><span class="line"></span><br><span class="line"><span class="comment">#命名实体识别</span></span><br><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> NamedEntityRecognizer</span><br><span class="line">recognizer = NamedEntityRecognizer()</span><br><span class="line">recognizer.load(<span class="string">'../data/ner.model'</span>)</span><br><span class="line">nettags = recognizer.recognize(list(words),list(posts))</span><br><span class="line">recognizer.release()</span><br><span class="line">print(list(zip(list(words),list(nettags))))</span><br><span class="line"></span><br><span class="line"><span class="comment">#依存语法分析</span></span><br><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> Parser</span><br><span class="line">parer = Parser()</span><br><span class="line">parer.load(<span class="string">'../data/parser.model'</span>)</span><br><span class="line">arcs = parer.parse(list(words),list(posts))</span><br><span class="line">parer.release()</span><br><span class="line">print(list(zip(list(words),[(arc.head,arc.relation) <span class="keyword">for</span> arc <span class="keyword">in</span> arcs])))</span><br><span class="line"></span><br><span class="line"><span class="comment">#语义角色标注</span></span><br><span class="line"><span class="keyword">from</span> pyltp <span class="keyword">import</span> SementicRoleLabeller</span><br><span class="line">labeller = SementicRoleLabeller()</span><br><span class="line">labeller.load(<span class="string">'../data/pisrl_win.model'</span>)</span><br><span class="line">roles = labeller.label(list(words),list(posts),arcs)</span><br><span class="line">labeller.release()</span><br><span class="line"><span class="keyword">for</span> role <span class="keyword">in</span> roles:</span><br><span class="line">    print(role.index, <span class="string">""</span>.join(</span><br><span class="line">        [<span class="string">"%s:(%d,%d)"</span> % (arg.name, arg.range.start, arg.range.end) <span class="keyword">for</span> arg <span class="keyword">in</span> role.arguments]))</span><br></pre></td></tr></table></figure>
<p>其中我的pyltp不知道怎么安装的，竟然成了build-in里的东西了，而不是在site-packages里面。</p>
<p>执行以上代码的结果为：</p>
<p><img src="https://github.com/smilelight/images/raw/master/learnLTPtest/testltp-result.png" alt="testltp-result"></p>
<p>其中要注意的是这里在语义角色标注中使用的并非pisrl.model而是pisrl_win.model，前者会报错的。</p>
<p>同时由于文档中提到：</p>
<p><img src="https://github.com/smilelight/images/raw/master/learnLTPtest/no-support.png" alt="no-support"></p>
<p>于是我又去申请了哈工大的<a href="https://www.ltp-cloud.com/" target="_blank" rel="noopener">语言云</a>个人账号,经过邮箱激活之后，按照人家的<a href="https://www.ltp-cloud.com/document/" target="_blank" rel="noopener">使用文档</a>就要探索一下，不过没想到出现了问题。。。</p>
<p>由于官网提供的Python示例代码使用的是2.7版本，所以这里我使用的是另外的一个网络请求模块：<code>requests</code>，最终的代码<code>testltpCloud.py</code>长成这样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> settings <span class="keyword">import</span> APIKEY</span><br><span class="line"></span><br><span class="line">url_get_base = <span class="string">"http://api.ltp-cloud.com/analysis/"</span></span><br><span class="line">args = &#123;</span><br><span class="line">    <span class="string">'api_key'</span>: APIKEY,</span><br><span class="line">    <span class="string">'text'</span>: <span class="string">'我是中国人。'</span>,</span><br><span class="line">    <span class="string">'pattern'</span>: <span class="string">'dp'</span>,</span><br><span class="line">    <span class="string">'format'</span>: <span class="string">'plain'</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># result = urllib.request.urlopen(url_get_base, urllib.parse.urlencode(args))  # POST method</span></span><br><span class="line">result = requests.post(url_get_base,args)</span><br><span class="line">print(result.text)</span><br></pre></td></tr></table></figure>
<p>在同路径下的<code>settings.py</code>的内容为：</p>
<p><img src="https://github.com/smilelight/images/raw/master/learnLTPtest/apikey.png" alt="apikey"></p>
<p>其中APIKEY的内容是邮件发给你的api_key字符串。</p>
<p>不过运行的结果为：</p>
<p><img src="https://github.com/smilelight/images/raw/master/learnLTPtest/testltpCloud-result.png" alt="testltpCloud-result"></p>
<p>显示未授权用户，但是我的账户类型是免费的，刚注册的啊，为啥这样，搜了搜没搜出个啥，就先这样吧。</p>
<p>又向目标迈进了一步，嘿嘿，加油！</p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/Python/"><i class="fas fa-hashtag fa-fw"></i>Python</a>
                
                    <a href="/tags/自然语言处理/"><i class="fas fa-hashtag fa-fw"></i>自然语言处理</a>
                
                    <a href="/tags/ltp/"><i class="fas fa-hashtag fa-fw"></i>ltp</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class='post-wrapper'>
          <article class="post reveal ">
    
<section class='meta'>
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/04/06/Scrapy爬取知乎数据小试/">
              
                  Scrapy爬取知乎数据小试
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-04-06
      </time>
    

    
      

    

    

    

  </div>
</section>

    <section class="article typo">
        <p>啊啊啊，没时间写啦，以后有时间再写吧！</p>
<p>。。。发现今天是周五，不熄灯。。。</p>
<p>前两周一直在忙毕设的事情，由于某些原因毕设选择了相对简单的微信小程序，经过奋战之后一些主要的基本功能已经实现多半。</p>
<p>自然语言处理的一些最基本的概念已经有所了解，下面想要找点实战项目练练手。由于处理的第一步便是要获取语料，想着以后爬虫这东西肯定是要学的，于是从昨天开始学习相关视频、配置相关环境，今天看了部分，照着Demo练了练小手。</p>
<p>B站真是个好地方，上面有不少免费的好的视频可以看，虽然版权这方面%<em>&amp;@#￥</em>！@￥……@#￥%……#@￥%##￥@</p>
<p><a href="https://www.bilibili.com/video/av18202461" target="_blank" rel="noopener">这是学习爬虫的视频链接</a></p>
<p>作者是拿轮子哥vczh作为start_user的，当时还愣了一下，可以的，会玩，想当年自己也关注过轮子哥一段时间，不过看他经常给美女们点赞、抖机灵，后来便取关了。</p>
<p>废话少说，言归正传：</p>
<ol>
<li><strong>爬虫</strong>：请求网站并提取数据的自动化程序。</li>
<li>爬虫的基本流程：<ul>
<li><strong>发起请求</strong>：通过HTTP库向目标站点发起请求,即发送一个Request,情况请可以包含额外的headers等信息,等待服务器响应。</li>
<li><strong>获取响应内容</strong>：如果服务器能正常响应，会得到一个Response，Response的内容便是所要获取的页面内容，内容可能有HTML，Json字符串，二进制数据（如图片视频）等类型。</li>
<li><strong>解析内容</strong>：得到的内容可能是HTML，可以用正则表达式、网页解析库进行解析，可呢是Json，可以直接转为Json对象解析，可能是二进制数据，可以做保存或者进一步的处理。</li>
<li><strong>保存数据</strong>：保存形式多样，可以存为文本，也可以保存至数据库，或者保存特定格式的文件。</li>
</ul>
</li>
</ol>
<p>项目所实现的是从首个著名知乎用户（本项目中为vczh）个人信息及其所有关注人、所有粉丝相关信息爬取开始、一直延伸整个关注网，并将结果数据集保存在MongoDB中。</p>
<p>具体来说就是先爬轮子哥的个人信息数据，然后依次爬取他的所有关注人的个人信息以及他的所有粉丝们的个人信息，这样的策略应用到每一个爬虫经过的用户上，从而实现数据的遍历抽取。单纯从Python代码的角度上讲，类似于会重复的深度优先遍历，然而具体的Scrapy引擎内部会如何调度这些Request队列就是人家内部的算法了。</p>
<p>视频的上传时间是18年1月11日，当时从网页中获取到的用户信息相对比较简单、集中、丰富，今天我试了又试，发现可获取到的直接信息变少了。由于只是初步尝试，所以也就按部就班的照样执行，没有做得不偿失的优化了。</p>
<p>Scrapy引擎的框架大致如下：</p>
<p><img src="https://github.com/smilelight/images/raw/master/scrapy/scrapy%E6%9E%B6%E6%9E%84.jpg" alt="scrapy架构"></p>
<p>我们可以看到整个引擎主要是由四部分组成：</p>
<ul>
<li>Scheduler（调度器）</li>
<li>Downloader（下载器）</li>
<li>Spiders（爬虫）</li>
<li>Item Pipeline（项目 管道）</li>
</ul>
<p>其中Spider中定义了具体的爬虫逻辑，比如我们要怎么爬，爬什么，后面跟着的s表明这通常不是一个Spider，而是通常有多个Spider，我们可以根据不同的具体需求编写对应的Spider。</p>
<p>Spiders之上的是一些Spider Middlewares（即爬虫中间件），有点类似于Python中函数的修饰器，可以对函数进行一些增强和拓展，同样的，我们可以通过这些Spider中间件来丰富和拓展我们编写的小Spider，让它们表现的更给力一些，同时这也可以简化我们的编写逻辑，因为我们只需要在之上套些中间件就可以了，套什么中间件视具体需求而定，比如说冬天穿大衣、夏天穿衬衫等。</p>
<p>当Spider生成好之后，引擎会开始执行这个Spider的内部逻辑，如start_requests方法，和parse方法等等，具体的它会将HTTP的Request请求交给Downloader完成，由Downloader完成从Internet上下载资源数据的具体任务，而Downloader也可以有自己的中间件也就是Downloader Middlewares，可以对Downloader进行改装，增强。</p>
<p>Downloader完成下载后，Scrapy引擎会将Downloader的成果Responses交给之前的Spider，执行它的解析方法。之后视具体情况，Spider可能会爬取更多的数据，相应的会产生更多的Request请求，或者将Response中的数据进行处理，处理为Item，然后转交给Item Pipeline做最后的数据处理工作。</p>
<p>因为爬虫一般不是一个一个的爬，而是通常成百上千乃至上万的爬，Scheduler的主要作用类似于CPU的处理器对这些请求做一个规划调度，先做哪个，后做哪个等等。</p>
<p>Item Pipline 中，Item可以视为一个数据对象的容器，而Pipline则类似Unix系统中的管道，或者更通俗点，就像流水线的的工人，从网上获取原材料之后，Spider这个工人进行加工处理，之后这些Pipline们做做类似贴标签的工作，最终提交正式的产品。</p>
<p>因为我们爬虫的编写都是具有针对性的，知己知彼百战不殆嘛。所以首先要分析知乎相关的数据流通状况：</p>
<p><img src="https://github.com/smilelight/images/raw/master/scrapy/scrapy-user.png" alt="scrapy-user"></p>
<p>以上是轮子哥的知乎页面。</p>
<p>首先必须要按F12打开开发者工具，查看Network页面。</p>
<p>对于某个用户的具体信息，如关注人或粉丝列表中的，我们只需要将鼠标请放在某个人的图像上，知乎就会通过Ajax去请求这个人的数据，以Json对象的格式返回给浏览器，同样的，当我们查看关注人列表和粉丝列表时，知乎也是通过Ajax请求返回这些数据对象的，具体的如以下几张图片：</p>
<p><img src="https://github.com/smilelight/images/raw/master/scrapy/scrapy-info.png" alt="scrapy-info"></p>
<p>这是轮子哥关注的某个人的信息，观看图右侧我们发现这个人相关信息就在这个Json对象中。而如果要获取到这个Response体中的Json对象，我们只要执行最上面的网络请求就可以了：</p>
<p><img src="https://github.com/smilelight/images/raw/master/scrapy/scrapy-request-user.png" alt="scrapy-request-user"></p>
<p>还有类似的关注人列表和粉丝列表也都是大概类似的情况，不过情况的url中的参数和内容稍有不同罢了。</p>
<p>关注人列表：</p>
<p><img src="https://github.com/smilelight/images/raw/master/scrapy/scrapy-followees.png" alt="scrapy-followees"></p>
<p>相关请求：</p>
<p><img src="https://github.com/smilelight/images/raw/master/scrapy/scrapy-request-followees.png" alt="scrapy-request-followees"></p>
<p>粉丝列表：</p>
<p><img src="https://github.com/smilelight/images/raw/master/scrapy/scrapy-followers.png" alt="scrapy-followers"></p>
<p>相关请求：</p>
<p><img src="https://github.com/smilelight/images/raw/master/scrapy/scrapy-request-followers.png" alt="scrapy-request-followers"></p>
<p>其实爬虫这个东西的基本原理很简单，就是执行HTTP请求，处理响应的数据，将这个过程重复化自动化而已。</p>
<p>而基于前面我们所提到的爬取信息的相关策略，我们要做的就是爬轮子哥的数据，然后请求两个列表中其他人的数据，并拓展开来。</p>
<p>整个项目的结构如下：</p>
<p><img src="https://github.com/smilelight/images/raw/master/scrapy/project-structure.png" alt="project-structure"></p>
<p>这里我们需要编写的是zhihu.py、items.py、piplines.py、settings.py</p>
<p>在settings.py中我们进行了请求头、Item Pipline中间件和MongoDB相关的配置。</p>
<p>如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">'Accept'</span>: <span class="string">'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8'</span>,</span><br><span class="line">  <span class="string">'Accept-Language'</span>: <span class="string">'en'</span>,</span><br><span class="line">  <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 6.3; W…) Gecko/20100101 Firefox/57.0'</span>,</span><br><span class="line">  <span class="string">'authorization'</span>:<span class="string">' oauth c3cef7c66a1843f8b3a9e6a1e3160e20'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'zhihuUser.pipelines.MongoPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MONGO_URI = <span class="string">'localhost'</span></span><br><span class="line">MONGO_DATABASE = <span class="string">'zhihu'</span></span><br></pre></td></tr></table></figure>
<p>我们针对Json对象的内容编写的ZhihuUserItem对象：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Item,Field</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuUserItem</span><span class="params">(Item)</span>:</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    id = Field()</span><br><span class="line">    is_followed = Field()</span><br><span class="line">    avatar_url_template = Field()</span><br><span class="line">    user_type = Field()</span><br><span class="line">    answer_count = Field()</span><br><span class="line">    is_following = Field()</span><br><span class="line">    url = Field()</span><br><span class="line">    url_token = Field()</span><br><span class="line">    allow_message = Field()</span><br><span class="line">    articles_count = Field()</span><br><span class="line">    is_blocking = Field()</span><br><span class="line">    name = Field()</span><br><span class="line">    headline = Field()</span><br><span class="line">    badge = Field()</span><br><span class="line">    is_advertiser = Field()</span><br><span class="line">    avatar_url = Field()</span><br><span class="line">    is_org = Field()</span><br><span class="line">    gender = Field()</span><br><span class="line">    follower_count = Field()</span><br><span class="line">    employments = Field()</span><br><span class="line">    type = Field()</span><br></pre></td></tr></table></figure>
<p>因为要将数据存储到MongoDB中，所以要进行MongoDB的Item Pipline中间件的编写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    collection_name = <span class="string">'user_info'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, mongo_uri, mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db = mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls, crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> cls(</span><br><span class="line">            mongo_uri=crawler.settings.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db=crawler.settings.get(<span class="string">'MONGO_DATABASE'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self, spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        self.db[self.collection_name].insert_one(dict(item))</span><br><span class="line">        <span class="comment"># self.db[self.collection_name].update(&#123;'url_token': item['url_token']&#125;,&#123;'$set': item&#125;,True)</span></span><br></pre></td></tr></table></figure>
<p>最后是zhihu.py 中 ZhihuSpider的编写：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Request</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> zhihuUser.items <span class="keyword">import</span> ZhihuUserItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ZhihuSpider</span><span class="params">(scrapy.Spider)</span>:</span></span><br><span class="line">    name = <span class="string">'zhihu'</span></span><br><span class="line">    allowed_domains = [<span class="string">'www.zhihu.com'</span>]</span><br><span class="line">    start_urls = [<span class="string">'http://www.zhihu.com/'</span>]</span><br><span class="line"></span><br><span class="line">    start_user = <span class="string">'excited-vczh'</span></span><br><span class="line"></span><br><span class="line">    user_url = <span class="string">'https://www.zhihu.com/api/v4/members/&#123;user&#125;?include=&#123;include&#125;'</span></span><br><span class="line">    user_query = <span class="string">'allow_message,is_followed,is_following,is_org,is_blocking,employments,answer_count,follower_count,articles_count,gender,badge[?(type=best_answerer)].topics'</span></span><br><span class="line"></span><br><span class="line">    followees_url = <span class="string">'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followees?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;'</span></span><br><span class="line">    followees_query = <span class="string">'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics'</span></span><br><span class="line"></span><br><span class="line">    followers_url = <span class="string">'https://www.zhihu.com/api/v4/members/&#123;user&#125;/followers?include=&#123;include&#125;&amp;offset=&#123;offset&#125;&amp;limit=&#123;limit&#125;'</span></span><br><span class="line">    followers_query = <span class="string">'data[*].answer_count,articles_count,gender,follower_count,is_followed,is_following,badge[?(type=best_answerer)].topics'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> Request(self.user_url.format(user=self.start_user,include=self.user_query),callback=self.parse_user)</span><br><span class="line">        <span class="comment"># yield Request(self.followees_url.format(user=self.start_user,include=self.followees_query,offset=0,limit=20),callback=self.parse_followees)</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_user</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        result = json.loads(response.text)</span><br><span class="line">        item = ZhihuUserItem()</span><br><span class="line">        <span class="keyword">for</span> field <span class="keyword">in</span> item.fields:</span><br><span class="line">            <span class="keyword">if</span> field <span class="keyword">in</span> result.keys():</span><br><span class="line">                item[field] = result.get(field)</span><br><span class="line">        <span class="keyword">yield</span> item</span><br><span class="line">        <span class="keyword">yield</span> Request(self.followees_url.format(user=result.get(<span class="string">'url_token'</span>), include=self.followees_query, offset=<span class="number">0</span>, limit=<span class="number">20</span>),</span><br><span class="line">                      callback=self.parse_followees)</span><br><span class="line">        <span class="keyword">yield</span> Request(</span><br><span class="line">            self.followers_url.format(user=result.get(<span class="string">'url_token'</span>), include=self.followers_query, offset=<span class="number">0</span>, limit=<span class="number">20</span>),</span><br><span class="line">            callback=self.parse_followees)</span><br><span class="line">        <span class="comment"># print(json.loads(response.text))</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_followees</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        result = json.loads(response.text)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'data'</span> <span class="keyword">in</span> result.keys():</span><br><span class="line">            <span class="keyword">for</span> result <span class="keyword">in</span> result.get(<span class="string">'data'</span>):</span><br><span class="line">                <span class="keyword">yield</span> Request(self.user_url.format(user=result.get(<span class="string">'url_token'</span>),include=self.user_query),callback=self.parse_user)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'paging'</span> <span class="keyword">in</span> result.keys() <span class="keyword">and</span> result.get(<span class="string">'paging'</span>).get(<span class="string">'is_end'</span>) == <span class="keyword">False</span>:</span><br><span class="line">            next_page = result.get(<span class="string">'paging'</span>).get(<span class="string">'next'</span>)</span><br><span class="line">            <span class="keyword">yield</span> Request(next_page,callback=self.parse_followees)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_followers</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        result = json.loads(response.text)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'data'</span> <span class="keyword">in</span> result.keys():</span><br><span class="line">            <span class="keyword">for</span> result <span class="keyword">in</span> result.get(<span class="string">'data'</span>):</span><br><span class="line">                <span class="keyword">yield</span> Request(self.user_url.format(user=result.get(<span class="string">'url_token'</span>),include=self.user_query),callback=self.parse_user)</span><br><span class="line">        <span class="keyword">if</span> <span class="string">'paging'</span> <span class="keyword">in</span> result.keys() <span class="keyword">and</span> result.get(<span class="string">'paging'</span>).get(<span class="string">'is_end'</span>) == <span class="keyword">False</span>:</span><br><span class="line">            next_page = result.get(<span class="string">'paging'</span>).get(<span class="string">'next'</span>)</span><br><span class="line">            <span class="keyword">yield</span> Request(next_page,callback=self.parse_followers)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self, response)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<p>在终端下执行<code>scrapy crawl zhihu</code>后，爬虫便会开始启动，由于这个工程一直爬一直爬，所以让它爬一会做个样子就行了，通过<code>Ctrl+C</code>停止当前任务，随后我们可以通过可视化工具查看到存入MongoDB中的数据：</p>
<p><img src="https://github.com/smilelight/images/raw/master/scrapy/scrapy-mongo-data.png" alt="scrapy-mongo-data"></p>
<p>大功告成！虽然超级简单。。。</p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/爬虫/"><i class="fas fa-hashtag fa-fw"></i>爬虫</a>
                
                    <a href="/tags/Scrapy/"><i class="fas fa-hashtag fa-fw"></i>Scrapy</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class='post-wrapper'>
          <article class="post reveal ">
    
<section class='meta'>
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/03/28/微信小程序探索随笔/">
              
                  微信小程序的component
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-03-28
      </time>
    

    
      
    
    <div class='metatag cats'>
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/计算机/">计算机</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <ol>
<li><p>我发现无法直接在样式即wxss里通过color属性设置icon组件的颜色，是无效的，只能通过在wxml里设置它的color属性为js传入的变量值或者是通过变量值来控制具体的颜色值。</p>
</li>
<li><p>我们可以将微信小程序中的components组件视为一个对象，没错，它本来就是一个对象，只是相对而言，它的初始化方法和设置方式不同于在一般的js语言中，它的data属性里是这个对象建立时初始化时的数据，作用域和生命周期伴随着component对象实例，而properties属性效果类似，均可以在component对象内部的函数和方法中使用this.data获取到，只是相对而言，data的数据是组件内部的数据，它是属于组件本身的属性，从设计上讲不取决于外部的应用场景；而properties属性则是暴露在组件外部的属性，它的作用相当于一般的编程语言中我们在new一个对象时做的初始化工作如new People(name=”lightsmile”,sex = false)，也就是说组件的一些业务属性是要通过这些属性接口来实现的，它是根据场景所制订的，具体实例体现在如：</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">order-by</span> <span class="attr">bindtap</span>=<span class="string">"handOrderTap"</span> <span class="attr">id</span>=<span class="string">"fuck"</span> <span class="attr">test</span>=<span class="string">"&#123;&#123;5&#125;&#125;"</span> <span class="attr">data-tes</span>=<span class="string">"&#123;&#123;test&#125;&#125;"</span> <span class="attr">data-fuck</span>=<span class="string">"fuck"</span> <span class="attr">data-order</span>=<span class="string">"&#123;&#123;orders&#125;&#125;"</span> <span class="attr">data-</span>&gt;</span><span class="tag">&lt;/<span class="name">order-by</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>中id是每个标签都具有的值，而以“data-”开头的数据都是这个页面中，暴露给触发事件的值，在事件处理函数中，可以通过如</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">handOrderTap(e) &#123;</span><br><span class="line">    <span class="built_in">console</span>.log(e)</span><br><span class="line">    <span class="built_in">console</span>.log(e.currentTarget.id)</span><br><span class="line">    <span class="keyword">this</span>.fuck = <span class="keyword">this</span>.selectComponent(<span class="string">"#fuck"</span>)</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="keyword">this</span>.fuck.data)</span><br><span class="line">    <span class="built_in">console</span>.log(<span class="keyword">this</span>.fuck.dataset)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p>获取到e（event）事件的具体信息，这里的handOrderTag只是自定义的函数，e.target和e.currentTarget分别代表不同的对象。</p>
<p>其中target是指事件的原触发对象，而currentTarget是指当前事件的触发对象，这是与事件的冒泡捕获机制相关的。</p>
<p>而不以“data-”开头，也不是如class、id、style等其他的属性如在上例中属性名为test的是页面传递给组件对象的信息，这里的test对应着组件对象之前设定的test属性，即在2中提到的暴露的属性名</p>
<p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image1.png" alt="Image1"></p>
<p>而“=”号后面的“”和“”则均是属于当前页面的逻辑层Page对象内部的数据，即this.data.test和this.tata.orders（其中的this指代的是当前对象），也就是说，在与Component组件或页面Page对象对应的wxml中，其中的永远都是指的当前组件或页面对象，无法外指和内值，即在Component对应wxml中无法引用外部Page的对象（这个很合理，因为组件本来就是要被复用的，不应该出现还可以引用外部的数据的情况），Page对应的wxml中也无法引用内部Component对象的内部属性，而如果要使用内部的值，一种方法是内部定义触发的方法然后再使用如this.triggerEvent(‘change’, this)<br>触发外部的change事件，这样组件外部就可以使用bindchange=”方法名”进行handle处理了。<br>如下几个阶段：</p>
<pre><code>1. 在Page的wxml中使用组件order-by,绑定了自定义事件MyEvent，这样在MyEvent事件被触发时，Page对象的handleMyEvent方法就会被执行。
</code></pre><p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image2.png" alt="Image2"></p>
<pre><code>2. handleMyEvent方法的内容如下：
</code></pre><p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image3.png" alt="Image3"></p>
<pre><code>3.Component的wxml中的text组件绑定tap事件到Component对象的myEvent方法
</code></pre><p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image4.png" alt="Image4"></p>
<pre><code>4.myEvent方法的内容如下：
</code></pre><p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image5.png" alt="Image5"></p>
<p>因此整个的流程应当为：当我点击text文本的时候，会触发tap事件，这样它绑定的myEvent方法会被执行，然后方法内部又会主动引发MyEvent事件，这样在Page页面对其绑定的handleMyEvent方法会被执行，而该方法定义在Page对象内部。（注意不要搞混事件和方法，虽然我这里的名字比较混乱）控制台输出的结果为：</p>
<p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image6.png" alt="Image6"></p>
<p>即先触发tap事件，再触发MyEvent自定义事件，下面我们来看一看内部传递的东西：</p>
<p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image7.png" alt="Image7"></p>
<p>从中我们可以发现第一个事件e的currentTarget属性的dataset属性是一个空对象，对应着我们并没有在wxml的text组件内部填写“data-xx”属性，而第二个事件e的currentTarget属性的dataset属性是一个包含了fuck、order、tes三个属性的对象，尽管其中order和tes的内容为空（因为对应的数据在Page对象中没有，我错误的写成了Component对象内部的了，发现获取不到，这才有了这篇文章）。</p>
<p>因此一般情况下，我们可以通过这种事件的方式来实现数据的传递工作，并且 this.triggerEvent() 方法接收自定义事件名称外，还接收两个对象，eventDetail 和 eventOptions。这也就是说，我们完全可以不传递this，而传递任意自己定义的对象数据，比如在myEvent中我可以不传this，接着传e，我也可以定义只与业务相关的数据对象来处理，由于方法定义在页面或组件对象内部，可以访问内部数据，而通过事件传递后可以通过参数访问数据，以此就实现了组件向页面的数据传递工作。</p>
<p>当然，这样的一个特点是事件绑定是放在视图view层，而事件处理传递是放在逻辑js层，可能在做一些其他业务时还是需要相应的业务转化工作才可以，不够直接和方便，因为页面还是无法做到直接访问组件数据。</p>
<p>后来发现果然页面提供了这么一个方法：this.selectComponent(“#fuck”)<br>可以在页面js中使用selectComponent选择某个component组件对象的实例，在此之上可以继续访问到它的data属性、dataset属性和properties属性，分别对应的是组件的properties属性、data属性和properties属性，如下图所示：</p>
<p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image8.png" alt="Image8"></p>
<p>这里我们将组件的id设为fuck，handleOrderTap方法的内容如下：</p>
<p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image9.png" alt="Image9"></p>
<p>打印的内容为：</p>
<p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image10.png" alt="Image10"></p>
<p>我们可以发现它的id是自己定义的fuck,而is是项目的绝对路径，还有上面提到的data、properties、dataset属性（显而易见，properties和data属性指向同一个属性对象）（然而，经过测试发现虽然内容相同，不过并非指向同一个对象，如下图所示：真是奇了怪了。。。）。</p>
<p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image11.png" alt="Image11"></p>
<p>如果打开<strong>proto</strong>属性，会发现更多的东西，比如说方法：</p>
<p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image12.png" alt="Image12"></p>
<p><img src="https://github.com/smilelight/images/raw/master/wxapp_component/Image13.png" alt="Image13"></p>
<p>通过this.fuck.loghaha()即可完成对方法的调用，当然了，这里获取完全可以写成let fuck = this.selectComponent(“#fuck”)，然后通过fuck.loghaha()来调用，这里简单沿袭了网上搜的文章的实例。以上，也算自己学习的小经历总结吧。</p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/微信小程序/"><i class="fas fa-hashtag fa-fw"></i>微信小程序</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class='post-wrapper'>
          <article class="post reveal ">
    
<section class='meta'>
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/03/19/learnNLTKbyWatchVideo/">
              
                  learnNLTKbyWatchVideo
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-03-19
      </time>
    

    
      
    
    <div class='metatag cats'>
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/计算机/">计算机</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <h1 id="The-following-is-learning-from-the-video-NLTK-with-Python-3-for-Natural-Language-Processing"><a href="#The-following-is-learning-from-the-video-NLTK-with-Python-3-for-Natural-Language-Processing" class="headerlink" title="The following is learning from the video:NLTK with Python 3 for Natural Language Processing."></a>The following is learning from the video:NLTK with Python 3 for Natural Language Processing.</h1><p>You can watch the videos in YouTube,iliibili and the author’s website: <a href="http://pythonprogramming.net" target="_blank" rel="noopener">pythonprogramming.net</a></p>
<p>I use jupyter notebook to write and run the python code,the python version is 3.4.4.</p>
<p>Frist,we need to import the <code>nltk</code> module to use it</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> sent_tokenize,word_tokenize</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">text = <span class="string">r"hello,how are you! I am lightsmile. My github link is www.github.com/smilelight. My persoanl website is www.iamlightsmile.com"</span></span><br></pre></td></tr></table></figure>
<h3 id="1-Tokenizing-words-and-entences-分词和分句"><a href="#1-Tokenizing-words-and-entences-分词和分句" class="headerlink" title="1. Tokenizing words and entences(分词和分句)"></a>1. Tokenizing words and entences(分词和分句)</h3><p>use the <code>sent_tokenize</code> method to tokenize the texts to sentenses(分句)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sent_tokenize(text)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;hello,how are you!&#39;,
 &#39;I am lightsmile.&#39;,
 &#39;My github link is www.github.com/smilelight.&#39;,
 &#39;My persoanl website is www.iamlightsmile.com&#39;]
</code></pre><p>use the <code>word_tokenize</code> method to tokenize the texts to words(分词)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word_tokenize(text)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;hello&#39;,
 &#39;,&#39;,
 &#39;how&#39;,
 &#39;are&#39;,
 &#39;you&#39;,
 &#39;!&#39;,
 &#39;I&#39;,
 &#39;am&#39;,
 &#39;lightsmile&#39;,
 &#39;.&#39;,
 &#39;My&#39;,
 &#39;github&#39;,
 &#39;link&#39;,
 &#39;is&#39;,
 &#39;www.github.com/smilelight&#39;,
 &#39;.&#39;,
 &#39;My&#39;,
 &#39;persoanl&#39;,
 &#39;website&#39;,
 &#39;is&#39;,
 &#39;www.iamlightsmile.com&#39;]
</code></pre><h3 id="2-Stop-Words-停用词"><a href="#2-Stop-Words-停用词" class="headerlink" title="2. Stop Words(停用词)"></a>2. Stop Words(停用词)</h3><p>Then,import the <code>stopwords</code>(停用词) from the <code>nltk.corpus</code> module</p>
<p>The stopwords are the words which are used commonly in the daliy life but usefulless for we to analyze the texts,so we need to remove them from the texts before we do the next steps.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">example_sentense = <span class="string">"This is an example showing off stop word filtration"</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter_sentense = [w <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(example_sentense) <span class="keyword">if</span>  w <span class="keyword">not</span> <span class="keyword">in</span> stopwords.words(<span class="string">'english'</span>)]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">filter_sentense</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;This&#39;, &#39;example&#39;, &#39;showing&#39;, &#39;stop&#39;, &#39;word&#39;, &#39;filtration&#39;]
</code></pre><h3 id="3-Stemming-提取词干"><a href="#3-Stemming-提取词干" class="headerlink" title="3. Stemming(提取词干)"></a>3. Stemming(提取词干)</h3><p>Use <code>PorterStemmer()</code> to get the stems of words(提取词干)</p>
<p>In some situations there are different expressions which have the same meanings.For example,the words:good,better,well have the similar meanings in the most situations.So,on the purpose to simplify the texts,we can get the stems of words in the texts. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> PorterStemmer</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ps = PorterStemmer()</span><br><span class="line">example_words = [<span class="string">"python"</span>,<span class="string">"pythoner"</span>,<span class="string">"pythoning"</span>,<span class="string">"pythoned"</span>,<span class="string">"pythonly"</span>]</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> example_words:</span><br><span class="line">    print(ps.stem(w))</span><br></pre></td></tr></table></figure>
<pre><code>python
python
python
python
pythonli
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">new_text = <span class="string">"It is very important to be pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once."</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(new_text):</span><br><span class="line">    print(ps.stem(w))</span><br></pre></td></tr></table></figure>
<pre><code>It
is
veri
import
to
be
pythonli
while
you
are
python
with
python
.
all
python
have
python
poorli
at
least
onc
.
</code></pre><h3 id="4-Part-of-speech-tagging-词性标注"><a href="#4-Part-of-speech-tagging-词性标注" class="headerlink" title="4. Part of speech tagging(词性标注)"></a>4. Part of speech tagging(词性标注)</h3><p>Use pos_tag method to do part of speech tagging(词性标注)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tagged = nltk.pos_tag(word_tokenize(new_text))</span><br><span class="line">print(tagged)</span><br></pre></td></tr></table></figure>
<pre><code>[(&#39;It&#39;, &#39;PRP&#39;), (&#39;is&#39;, &#39;VBZ&#39;), (&#39;very&#39;, &#39;RB&#39;), (&#39;important&#39;, &#39;JJ&#39;), (&#39;to&#39;, &#39;TO&#39;), (&#39;be&#39;, &#39;VB&#39;), (&#39;pythonly&#39;, &#39;RB&#39;), (&#39;while&#39;, &#39;IN&#39;), (&#39;you&#39;, &#39;PRP&#39;), (&#39;are&#39;, &#39;VBP&#39;), (&#39;pythoning&#39;, &#39;VBG&#39;), (&#39;with&#39;, &#39;IN&#39;), (&#39;python&#39;, &#39;NN&#39;), (&#39;.&#39;, &#39;.&#39;), (&#39;All&#39;, &#39;DT&#39;), (&#39;pythoners&#39;, &#39;NNS&#39;), (&#39;have&#39;, &#39;VBP&#39;), (&#39;pythoned&#39;, &#39;VBN&#39;), (&#39;poorly&#39;, &#39;RB&#39;), (&#39;at&#39;, &#39;IN&#39;), (&#39;least&#39;, &#39;JJS&#39;), (&#39;once&#39;, &#39;RB&#39;), (&#39;.&#39;, &#39;.&#39;)]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[w <span class="keyword">for</span> w,t <span class="keyword">in</span> tagged <span class="keyword">if</span> t == <span class="string">'RB'</span> ]</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;very&#39;, &#39;pythonly&#39;, &#39;poorly&#39;, &#39;once&#39;]
</code></pre><h3 id="5-Chunking-短语识别"><a href="#5-Chunking-短语识别" class="headerlink" title="5. Chunking(短语识别)"></a>5. Chunking(短语识别)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">chunkGram = <span class="string">r"""Chunk: &#123;&lt;RB.?&gt;*&lt;VB.?&gt;*&lt;NNP&gt;*&lt;NN&gt;&#125;"""</span></span><br><span class="line">chunkParser = nltk.RegexpParser(chunkGram)</span><br><span class="line">chunked = chunkParser.parse(tagged)</span><br><span class="line">print(chunked)</span><br></pre></td></tr></table></figure>
<pre><code>(S
  It/PRP
  is/VBZ
  very/RB
  important/JJ
  to/TO
  be/VB
  pythonly/RB
  while/IN
  you/PRP
  are/VBP
  pythoning/VBG
  with/IN
  (Chunk python/NN)
  ./.
  All/DT
  pythoners/NNS
  have/VBP
  pythoned/VBN
  poorly/RB
  at/IN
  least/JJS
  once/RB
  ./.)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chunked.draw()</span><br></pre></td></tr></table></figure>
<h3 id="6-Chinking-短语排除"><a href="#6-Chinking-短语排除" class="headerlink" title="6. Chinking(短语排除)"></a>6. Chinking(短语排除)</h3><p>The chinking is used to chunk something expect the chinking things.It’s effect is to remove something.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">chinkGram = <span class="string">r"""Chunk: &#123;&lt;.*&gt;&#125;</span></span><br><span class="line"><span class="string">                                           Chink: &#125;&lt;NN&gt;&#123;"""</span></span><br><span class="line">chinkParser = nltk.RegexpParser(chinkGram)</span><br><span class="line">chinked = chinkParser.parse(tagged)</span><br><span class="line">print(chinked)</span><br></pre></td></tr></table></figure>
<pre><code>(S
  (Chunk It/PRP)
  (Chunk is/VBZ)
  (Chunk very/RB)
  (Chunk important/JJ)
  (Chunk to/TO)
  (Chunk be/VB)
  (Chunk pythonly/RB)
  (Chunk while/IN)
  (Chunk you/PRP)
  (Chunk are/VBP)
  (Chunk pythoning/VBG)
  (Chunk with/IN)
  (Chunk python/NN)
  (Chunk ./.)
  (Chunk All/DT)
  (Chunk pythoners/NNS)
  (Chunk have/VBP)
  (Chunk pythoned/VBN)
  (Chunk poorly/RB)
  (Chunk at/IN)
  (Chunk least/JJS)
  (Chunk once/RB)
  (Chunk ./.))
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chinked.draw()</span><br></pre></td></tr></table></figure>
<h3 id="7-Named-Entity-Recognition-命名实体识别"><a href="#7-Named-Entity-Recognition-命名实体识别" class="headerlink" title="7. Named Entity Recognition(命名实体识别)"></a>7. Named Entity Recognition(命名实体识别)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">new_text2 = <span class="string">"The Obama,president of the United States,is walking by the Danube with his families.They'll go back home at 7:00 a.m.."</span></span><br><span class="line">tagged2 = nltk.pos_tag(word_tokenize(new_text2))</span><br><span class="line">nameEnt = nltk.ne_chunk(tagged2)</span><br><span class="line">nameEnt.draw()</span><br></pre></td></tr></table></figure>
<h3 id="8-Lemmatizing-词形还原"><a href="#8-Lemmatizing-词形还原" class="headerlink" title="8. Lemmatizing(词形还原)"></a>8. Lemmatizing(词形还原)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.stem <span class="keyword">import</span> WordNetLemmatizer</span><br><span class="line">lemmatizer = WordNetLemmatizer()</span><br><span class="line">entities = [<span class="string">"cats"</span>,<span class="string">"body"</span>,<span class="string">"shoes"</span>,<span class="string">"python"</span>,<span class="string">"shit"</span>,<span class="string">"park"</span>]</span><br><span class="line"><span class="keyword">for</span> entity <span class="keyword">in</span> entities:</span><br><span class="line">    print(lemmatizer.lemmatize(entity))</span><br></pre></td></tr></table></figure>
<pre><code>cat
body
shoe
python
shit
park
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nltk.__file__</span><br></pre></td></tr></table></figure>
<pre><code>&#39;C:\\Program Files\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py&#39;
</code></pre><h3 id="9-NLTK-Corpora-语料库"><a href="#9-NLTK-Corpora-语料库" class="headerlink" title="9. NLTK Corpora(语料库)"></a>9. NLTK Corpora(语料库)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> gutenberg</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> sent_tokenize</span><br><span class="line">sample = gutenberg.raw(<span class="string">'bible-kjv.txt'</span>)</span><br><span class="line">tok = sent_tokenize(sample)</span><br><span class="line">tok[:<span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;[The King James Bible]\n\nThe Old Testament of the King James Bible\n\nThe First Book of Moses:  Called Genesis\n\n\n1:1 In the beginning God created the heaven and the earth.&#39;,
 &#39;1:2 And the earth was without form, and void; and darkness was upon\nthe face of the deep.&#39;,
 &#39;And the Spirit of God moved upon the face of the\nwaters.&#39;,
 &#39;1:3 And God said, Let there be light: and there was light.&#39;,
 &#39;1:4 And God saw the light, that it was good: and God divided the light\nfrom the darkness.&#39;]
</code></pre><h3 id="10-WordNet-一个英语词汇数据库"><a href="#10-WordNet-一个英语词汇数据库" class="headerlink" title="10. WordNet(一个英语词汇数据库)"></a>10. WordNet(一个英语词汇数据库)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> wordnet</span><br><span class="line">syns = wordnet.synsets(<span class="string">"program"</span>)</span><br><span class="line">syns</span><br></pre></td></tr></table></figure>
<pre><code>[Synset(&#39;plan.n.01&#39;),
 Synset(&#39;program.n.02&#39;),
 Synset(&#39;broadcast.n.02&#39;),
 Synset(&#39;platform.n.02&#39;),
 Synset(&#39;program.n.05&#39;),
 Synset(&#39;course_of_study.n.01&#39;),
 Synset(&#39;program.n.07&#39;),
 Synset(&#39;program.n.08&#39;),
 Synset(&#39;program.v.01&#39;),
 Synset(&#39;program.v.02&#39;)]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">word = wordnet.synsets(<span class="string">'boy'</span>)</span><br><span class="line">synonyms =[]</span><br><span class="line">antonyms = []</span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> word:</span><br><span class="line">    <span class="keyword">for</span> l <span class="keyword">in</span> w.lemmas():</span><br><span class="line">        synonyms.append(l.name())</span><br><span class="line">        <span class="keyword">if</span> l.antonyms():</span><br><span class="line">            <span class="keyword">for</span> a <span class="keyword">in</span> l.antonyms():</span><br><span class="line">                antonyms.append(a.name())</span><br><span class="line">print(set(synonyms))</span><br><span class="line">print(set(antonyms))</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;male_child&#39;, &#39;son&#39;, &#39;boy&#39;}
{&#39;female_child&#39;, &#39;daughter&#39;, &#39;girl&#39;}
</code></pre><p>Use list comprehension(列表推导式)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">synonyms2 = set([l.name() <span class="keyword">for</span> w <span class="keyword">in</span> word <span class="keyword">for</span> l <span class="keyword">in</span> w.lemmas()])</span><br><span class="line">antonyms2 = set([a.name() <span class="keyword">for</span> w <span class="keyword">in</span> word <span class="keyword">for</span> l <span class="keyword">in</span> w.lemmas() <span class="keyword">for</span> a <span class="keyword">in</span> l.antonyms()])</span><br><span class="line">print(synonyms2)</span><br><span class="line">print(antonyms2)</span><br></pre></td></tr></table></figure>
<pre><code>{&#39;male_child&#39;, &#39;son&#39;, &#39;boy&#39;}
{&#39;female_child&#39;, &#39;daughter&#39;, &#39;girl&#39;}
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word[<span class="number">0</span>][<span class="string">"boy"</span>].antosyns()</span><br></pre></td></tr></table></figure>
<pre><code>---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&lt;ipython-input-77-93678c6743d6&gt; in &lt;module&gt;()
----&gt; 1 word[0][&quot;boy&quot;].antosyns()


TypeError: &#39;Synset&#39; object is not subscriptable
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cat = wordnet.synset(<span class="string">"cat.n.01"</span>)</span><br><span class="line">dog = wordnet.synset(<span class="string">"dog.n.01"</span>)</span><br><span class="line">dog.wup_similarity(cat)</span><br></pre></td></tr></table></figure>
<pre><code>0.8571428571428571
</code></pre><h3 id="11-Text-Classfication-文本分类"><a href="#11-Text-Classfication-文本分类" class="headerlink" title="11. Text Classfication(文本分类)"></a>11. Text Classfication(文本分类)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> movie_reviews</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">documents = [(list(movie_reviews.words(fileid)),category)</span><br><span class="line">              <span class="keyword">for</span> category <span class="keyword">in</span> movie_reviews.categories()</span><br><span class="line">             <span class="keyword">for</span> fileid <span class="keyword">in</span> movie_reviews.fileids(category)]</span><br><span class="line">random.shuffle(documents)</span><br><span class="line"></span><br><span class="line">documents[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>
<pre><code>([&#39;did&#39;,
  &#39;you&#39;,
  &#39;ever&#39;,
  &#39;wonder&#39;,
  &#39;if&#39;,
  &#39;dennis&#39;,
  &#39;rodman&#39;,
  &#39;was&#39;,
  &#39;actually&#39;,
  &#39;from&#39;,
  &#39;this&#39;,
  &#39;planet&#39;,
  &#39;?&#39;,
  &#39;or&#39;,
  &#39;if&#39;,
  &#39;sylvester&#39;,
  &#39;stallone&#39;,
  &#39;was&#39;,
  &#39;some&#39;,
  &#39;kind&#39;,
  &#39;of&#39;,
  &#39;weird&#39;,
  &#39;extra&#39;,
  &#39;-&#39;,
  &#39;terrestrial&#39;,
  &#39;?&#39;,
  &#39;i&#39;,
  &#39;used&#39;,
  &#39;to&#39;,
  &#39;think&#39;,
  &#39;that&#39;,
  &#39;about&#39;,
  &#39;my&#39;,
  &#39;7th&#39;,
  &#39;grade&#39;,
  &#39;english&#39;,
  &#39;teacher&#39;,
  &#39;,&#39;,
  &#39;ms&#39;,
  &#39;.&#39;,
  &#39;carey&#39;,
  &#39;.&#39;,
  &#39;but&#39;,
  &#39;after&#39;,
  &#39;seeing&#39;,
  &#39;this&#39;,
  &#39;movie&#39;,
  &#39;,&#39;,
  &#39;they&#39;,
  &#39;may&#39;,
  &#39;have&#39;,
  &#39;confirmed&#39;,
  &#39;my&#39;,
  &#39;suspicions&#39;,
  &#39;.&#39;,
  &#39;as&#39;,
  &#39;the&#39;,
  &#39;story&#39;,
  &#39;goes&#39;,
  &#39;,&#39;,
  &#39;at&#39;,
  &#39;any&#39;,
  &#39;time&#39;,
  &#39;,&#39;,
  &#39;there&#39;,
  &#39;are&#39;,
  &#39;over&#39;,
  &#39;a&#39;,
  &#39;thousand&#39;,
  &#39;aliens&#39;,
  &#39;living&#39;,
  &#39;among&#39;,
  &#39;us&#39;,
  &#39;here&#39;,
  &#39;on&#39;,
  &#39;earth&#39;,
  &#39;.&#39;,
  &#39;the&#39;,
  &#39;men&#39;,
  &#39;in&#39;,
  &#39;black&#39;,
  &#39;(&#39;,
  &#39;mib&#39;,
  &#39;)&#39;,
  &#39;are&#39;,
  &#39;the&#39;,
  &#39;watchdogs&#39;,
  &#39;that&#39;,
  &#39;oversee&#39;,
  &#39;the&#39;,
  &#39;cosmic&#39;,
  &#39;citizens&#39;,
  &#39;,&#39;,
  &#39;guardians&#39;,
  &#39;of&#39;,
  &#39;our&#39;,
  &#39;beloved&#39;,
  &#39;planet&#39;,
  &#39;from&#39;,
  &#39;nasty&#39;,
  &#39;-&#39;,
  &#39;tempered&#39;,
  &#39;aliens&#39;,
  &#39;,&#39;,
  &#39;and&#39;,
  &#39;secret&#39;,
  &#39;service&#39;,
  &#39;to&#39;,
  &#39;the&#39;,
  &#39;stars&#39;,
  &#39;.&#39;,
  &#39;based&#39;,
  &#39;in&#39;,
  &#39;new&#39;,
  &#39;york&#39;,
  &#39;city&#39;,
  &#39;(&#39;,
  &#39;where&#39;,
  &#39;weird&#39;,
  &#39;is&#39;,
  &#39;the&#39;,
  &#39;norm&#39;,
  &#39;)&#39;,
  &#39;,&#39;,
  &#39;the&#39;,
  &#39;mib&#39;,
  &#39;organization&#39;,
  &#39;gives&#39;,
  &#39;human&#39;,
  &#39;form&#39;,
  &#39;to&#39;,
  &#39;our&#39;,
  &#39;space&#39;,
  &#39;-&#39;,
  &#39;faring&#39;,
  &#39;emigrants&#39;,
  &#39;so&#39;,
  &#39;that&#39;,
  &#39;they&#39;,
  &#39;may&#39;,
  &#39;walk&#39;,
  &#39;and&#39;,
  &#39;live&#39;,
  &#39;among&#39;,
  &#39;us&#39;,
  &#39;unnoticed&#39;,
  &#39;.&#39;,
  &#39;but&#39;,
  &#39;to&#39;,
  &#39;enforce&#39;,
  &#39;the&#39;,
  &#39;laws&#39;,
  &#39;of&#39;,
  &#39;earth&#39;,
  &#39;,&#39;,
  &#39;the&#39;,
  &#39;mib&#39;,
  &#39;carry&#39;,
  &#39;weapons&#39;,
  &#39;that&#39;,
  &#39;are&#39;,
  &#39;powerful&#39;,
  &#39;enough&#39;,
  &#39;to&#39;,
  &#39;meet&#39;,
  &#39;or&#39;,
  &#39;exceed&#39;,
  &#39;destruction&#39;,
  &#39;quotas&#39;,
  &#39;in&#39;,
  &#39;one&#39;,
  &#39;single&#39;,
  &#39;blast&#39;,
  &#39;.&#39;,
  &#39;they&#39;,
  &#39;carry&#39;,
  &#39;other&#39;,
  &#39;-&#39;,
  &#39;worldly&#39;,
  &#39;technology&#39;,
  &#39;to&#39;,
  &#39;erase&#39;,
  &#39;people&#39;,
  &quot;&#39;&quot;,
  &#39;s&#39;,
  &#39;short&#39;,
  &#39;-&#39;,
  &#39;term&#39;,
  &#39;memory&#39;,
  &#39;when&#39;,
  &#39;common&#39;,
  &#39;folk&#39;,
  &#39;see&#39;,
  &#39;the&#39;,
  &#39;mib&#39;,
  &#39;in&#39;,
  &#39;action&#39;,
  &#39;.&#39;,
  &#39;and&#39;,
  &#39;their&#39;,
  &#39;best&#39;,
  &#39;leads&#39;,
  &#39;on&#39;,
  &#39;cosmic&#39;,
  &#39;things&#39;,
  &#39;-&#39;,
  &#39;gone&#39;,
  &#39;-&#39;,
  &#39;awry&#39;,
  &#39;are&#39;,
  &#39;the&#39;,
  &#39;supermarket&#39;,
  &#39;tabloids&#39;,
  &#39;.&#39;,
  &#39;little&#39;,
  &#39;do&#39;,
  &#39;we&#39;,
  &#39;know&#39;,
  &#39;that&#39;,
  &#39;there&#39;,
  &#39;are&#39;,
  &#39;much&#39;,
  &#39;stronger&#39;,
  &#39;battles&#39;,
  &#39;of&#39;,
  &#39;good&#39;,
  &#39;v&#39;,
  &#39;.&#39;,
  &#39;evil&#39;,
  &#39;going&#39;,
  &#39;on&#39;,
  &#39;in&#39;,
  &#39;the&#39;,
  &#39;depths&#39;,
  &#39;of&#39;,
  &#39;space&#39;,
  &#39;.&#39;,
  &#39;one&#39;,
  &#39;of&#39;,
  &#39;the&#39;,
  &#39;aliens&#39;,
  &#39;-&#39;,
  &#39;as&#39;,
  &#39;-&#39;,
  &#39;human&#39;,
  &#39;on&#39;,
  &#39;this&#39;,
  &#39;planet&#39;,
  &#39;is&#39;,
  &#39;an&#39;,
  &#39;important&#39;,
  &#39;diplomat&#39;,
  &#39;that&#39;,
  &#39;is&#39;,
  &#39;carrying&#39;,
  &#39;something&#39;,
  &#39;very&#39;,
  &#39;precious&#39;,
  &#39;.&#39;,
  &#39;it&#39;,
  &#39;holds&#39;,
  &#39;the&#39;,
  &quot;&#39;&quot;,
  &#39;key&#39;,
  &quot;&#39;&quot;,
  &#39;,&#39;,
  &#39;literally&#39;,
  &#39;,&#39;,
  &#39;to&#39;,
  &#39;universal&#39;,
  &#39;peace&#39;,
  &#39;.&#39;,
  &#39;a&#39;,
  &#39;giant&#39;,
  &#39;cockroach&#39;,
  &#39;-&#39;,
  &#39;like&#39;,
  &#39;alien&#39;,
  &#39;soon&#39;,
  &#39;arrives&#39;,
  &#39;on&#39;,
  &#39;the&#39;,
  &#39;planet&#39;,
  &#39;and&#39;,
  &#39;steals&#39;,
  &#39;this&#39;,
  &quot;&#39;&quot;,
  &#39;key&#39;,
  &quot;&#39;&quot;,
  &#39;.&#39;,
  &#39;in&#39;,
  &#39;the&#39;,
  &#39;wrong&#39;,
  &#39;alien&#39;,
  &#39;hands&#39;,
  &#39;(&#39;,
  &#39;flippers&#39;,
  &#39;?&#39;,
  &#39;mandibles&#39;,
  &#39;?&#39;,
  &#39;tentacles&#39;,
  &#39;?&#39;,
  &#39;)&#39;,
  &#39;,&#39;,
  &#39;it&#39;,
  &#39;can&#39;,
  &#39;be&#39;,
  &#39;used&#39;,
  &#39;as&#39;,
  &#39;a&#39;,
  &#39;weapon&#39;,
  &#39;.&#39;,
  &#39;therefore&#39;,
  &#39;,&#39;,
  &#39;it&#39;,
  &#39;must&#39;,
  &#39;be&#39;,
  &#39;recovered&#39;,
  &#39;and&#39;,
  &#39;returned&#39;,
  &#39;to&#39;,
  &#39;it&#39;,
  &quot;&#39;&quot;,
  &#39;s&#39;,
  &#39;rightful&#39;,
  &#39;owners&#39;,
  &#39;.&#39;,
  &#39;otherwise&#39;,
  &#39;,&#39;,
  &#39;to&#39;,
  &#39;ensure&#39;,
  &#39;universal&#39;,
  &#39;safety&#39;,
  &#39;,&#39;,
  &#39;earth&#39;,
  &#39;will&#39;,
  &#39;be&#39;,
  &#39;destroyed&#39;,
  &#39;,&#39;,
  &#39;along&#39;,
  &#39;with&#39;,
  &#39;the&#39;,
  &quot;&#39;&quot;,
  &#39;key&#39;,
  &quot;&#39;&quot;,
  &#39;.&#39;,
  &#39;now&#39;,
  &#39;,&#39;,
  &#39;it&#39;,
  &quot;&#39;&quot;,
  &#39;s&#39;,
  &#39;the&#39;,
  &#39;mib&#39;,
  &#39;who&#39;,
  &#39;must&#39;,
  &#39;prevent&#39;,
  &#39;this&#39;,
  &#39;catastrophe&#39;,
  &#39;.&#39;,
  &#39;the&#39;,
  &#39;mib&#39;,
  &#39;agents&#39;,
  &#39;on&#39;,
  &#39;the&#39;,
  &#39;case&#39;,
  &#39;are&#39;,
  &#39;&quot;&#39;,
  &#39;k&#39;,
  &#39;&quot;&#39;,
  &#39;,&#39;,
  &#39;played&#39;,
  &#39;by&#39;,
  &#39;tommy&#39;,
  &#39;lee&#39;,
  &#39;jones&#39;,
  &#39;.&#39;,
  &#39;he&#39;,
  &#39;is&#39;,
  &#39;crustier&#39;,
  &#39;than&#39;,
  &#39;burnt&#39;,
  &#39;toast&#39;,
  &#39;and&#39;,
  &#39;even&#39;,
  &#39;more&#39;,
  &#39;serious&#39;,
  &#39;than&#39;,
  &#39;al&#39;,
  &#39;gore&#39;,
  &#39;.&#39;,
  &#39;the&#39;,
  &#39;stars&#39;,
  &#39;in&#39;,
  &#39;the&#39;,
  &#39;sky&#39;,
  &#39;no&#39;,
  &#39;longer&#39;,
  &#39;spark&#39;,
  &#39;wonder&#39;,
  &#39;in&#39;,
  &#39;his&#39;,
  &#39;eyes&#39;,
  &#39;.&#39;,
  &#39;he&#39;,
  &#39;is&#39;,
  &#39;accompanied&#39;,
  &#39;by&#39;,
  &#39;a&#39;,
  &#39;flippant&#39;,
  &#39;rookie&#39;,
  &#39;,&#39;,
  &#39;&quot;&#39;,
  &#39;j&#39;,
  &#39;&quot;&#39;,
  &#39;,&#39;,
  &#39;played&#39;,
  &#39;by&#39;,
  &#39;will&#39;,
  &#39;smith&#39;,
  &#39;.&#39;,
  &#39;but&#39;,
  &#39;,&#39;,
  &#39;despite&#39;,
  &#39;this&#39;,
  &#39;shoot&#39;,
  &#39;-&#39;,
  &#39;em&#39;,
  &#39;-&#39;,
  &#39;up&#39;,
  &#39;,&#39;,
  &#39;protect&#39;,
  &#39;-&#39;,
  &#39;earth&#39;,
  &#39;-&#39;,
  &#39;from&#39;,
  &#39;-&#39;,
  &#39;destruction&#39;,
  &#39;premise&#39;,
  &#39;,&#39;,
  &#39;this&#39;,
  &#39;is&#39;,
  &#39;nothing&#39;,
  &#39;at&#39;,
  &#39;all&#39;,
  &#39;like&#39;,
  &#39;a&#39;,
  &#39;typical&#39;,
  &#39;summer&#39;,
  &#39;action&#39;,
  &#39;movie&#39;,
  &#39;.&#39;,
  &#39;and&#39;,
  &#39;,&#39;,
  &#39;this&#39;,
  &#39;isn&#39;,
  &quot;&#39;&quot;,
  &#39;t&#39;,
  &#39;an&#39;,
  &#39;independence&#39;,
  &#39;day&#39;,
  &#39;knockoff&#39;,
  &#39;.&#39;,
  &#39;rather&#39;,
  &#39;,&#39;,
  &#39;this&#39;,
  &#39;is&#39;,
  &#39;a&#39;,
  &#39;stylishly&#39;,
  &#39;offbeat&#39;,
  &#39;sci&#39;,
  &#39;-&#39;,
  &#39;fi&#39;,
  &#39;comedy&#39;,
  &#39;that&#39;,
  &#39;pokes&#39;,
  &#39;fun&#39;,
  &#39;at&#39;,
  &#39;what&#39;,
  &#39;the&#39;,
  &#39;government&#39;,
  &#39;always&#39;,
  &#39;denies&#39;,
  &#39;?&#39;,
  &#39;that&#39;,
  &#39;there&#39;,
  &#39;are&#39;,
  &#39;real&#39;,
  &#39;aliens&#39;,
  &#39;that&#39;,
  &#39;live&#39;,
  &#39;here&#39;,
  &#39;,&#39;,
  &#39;and&#39;,
  &#39;that&#39;,
  &#39;the&#39;,
  &#39;government&#39;,
  &#39;does&#39;,
  &#39;its&#39;,
  &#39;darndest&#39;,
  &#39;to&#39;,
  &#39;cover&#39;,
  &#39;them&#39;,
  &#39;up&#39;,
  &#39;.&#39;,
  &#39;but&#39;,
  &#39;to&#39;,
  &#39;give&#39;,
  &#39;it&#39;,
  &#39;some&#39;,
  &#39;sense&#39;,
  &#39;of&#39;,
  &#39;excitement&#39;,
  &#39;and&#39;,
  &#39;to&#39;,
  &#39;keep&#39;,
  &#39;it&#39;,
  &#39;within&#39;,
  &#39;the&#39;,
  &#39;parameters&#39;,
  &#39;of&#39;,
  &#39;the&#39;,
  &#39;summer&#39;,
  &#39;movie&#39;,
  &#39;recipe&#39;,
  &#39;,&#39;,
  &#39;there&#39;,
  &#39;must&#39;,
  &#39;be&#39;,
  &#39;some&#39;,
  &#39;kind&#39;,
  &#39;of&#39;,
  &#39;earth&#39;,
  &#39;-&#39;,
  &#39;hangs&#39;,
  &#39;-&#39;,
  &#39;in&#39;,
  &#39;-&#39;,
  &#39;the&#39;,
  &#39;-&#39;,
  &#39;balance&#39;,
  &#39;scenario&#39;,
  &#39;.&#39;,
  &#39;yet&#39;,
  &#39;,&#39;,
  &#39;this&#39;,
  &#39;movie&#39;,
  &#39;is&#39;,
  &#39;very&#39;,
  &#39;appealing&#39;,
  &#39;.&#39;,
  &#39;the&#39;,
  &#39;abundance&#39;,
  &#39;of&#39;,
  &#39;wierdness&#39;,
  &#39;(&#39;,
  &#39;talking&#39;,
  &#39;aliens&#39;,
  &#39;,&#39;,
  &#39;pee&#39;,
  &#39;-&#39;,
  &#39;wee&#39;,
  &#39;atomizers&#39;,
  &#39;,&#39;,
  &#39;a&#39;,
  &#39;mortician&#39;,
  &#39;who&#39;,
  &quot;&#39;&quot;,
  &#39;lives&#39;,
  &quot;&#39;&quot;,
  &#39;for&#39;,
  &#39;her&#39;,
  &#39;work&#39;,
  &#39;,&#39;,
  &#39;and&#39;,
  &#39;lots&#39;,
  &#39;of&#39;,
  &#39;yucky&#39;,
  &#39;bugs&#39;,
  &#39;and&#39;,
  &#39;slime&#39;,
  &#39;-&#39;,
  &#39;splattering&#39;,
  &#39;galore&#39;,
  &#39;)&#39;,
  &#39;,&#39;,
  &#39;is&#39;,
  &#39;played&#39;,
  &#39;straight&#39;,
  &#39;,&#39;,
  &#39;like&#39;,
  &#39;as&#39;,
  &#39;if&#39;,
  &#39;this&#39;,
  &#39;were&#39;,
  &#39;normal&#39;,
  &#39;(&#39;,
  &#39;of&#39;,
  &#39;course&#39;,
  &#39;,&#39;,
  &#39;we&#39;,
  &#39;are&#39;,
  &#39;in&#39;,
  &#39;nyc&#39;,
  &#39;)&#39;,
  &#39;.&#39;,
  &#39;it&#39;,
  &#39;gives&#39;,
  &#39;it&#39;,
  &#39;a&#39;,
  &#39;deadpan&#39;,
  &#39;feel&#39;,
  &#39;,&#39;,
  &#39;which&#39;,
  &#39;makes&#39;,
  &#39;it&#39;,
  &#39;all&#39;,
  &#39;the&#39;,
  &#39;more&#39;,
  &#39;funnier&#39;,
  &#39;and&#39;,
  &#39;odder&#39;,
  &#39;.&#39;,
  &#39;jones&#39;,
  &#39;plays&#39;,
  &#39;the&#39;,
  &#39;venerable&#39;,
  &#39;seen&#39;,
  &#39;-&#39;,
  &#39;it&#39;,
  &#39;-&#39;,
  &#39;all&#39;,
  &#39;agent&#39;,
  &#39;with&#39;,
  &#39;seriousness&#39;,
  &#39;and&#39;,
  &#39;maturity&#39;,
  &#39;.&#39;,
  &#39;smith&#39;,
  &#39;is&#39;,
  &#39;likeable&#39;,
  &#39;and&#39;,
  &#39;makes&#39;,
  &#39;a&#39;,
  &#39;great&#39;,
  &#39;comic&#39;,
  &#39;partner&#39;,
  &#39;to&#39;,
  &#39;jones&#39;,
  &quot;&#39;&quot;,
  &#39;straight&#39;,
  &#39;man&#39;,
  &#39;routine&#39;,
  &#39;.&#39;,
  &#39;they&#39;,
  &#39;click&#39;,
  &#39;like&#39;,
  &#39;dorothy&#39;,
  &quot;&#39;&quot;,
  &#39;s&#39;,
  &#39;ruby&#39;,
  &#39;red&#39;,
  &#39;shoes&#39;,
  &#39;.&#39;,
  &#39;the&#39;,
  &#39;look&#39;,
  &#39;and&#39;,
  &#39;feel&#39;,
  &#39;of&#39;,
  &#39;the&#39;,
  &#39;movie&#39;,
  &#39;is&#39;,
  &#39;made&#39;,
  &#39;even&#39;,
  &#39;better&#39;,
  &#39;with&#39;,
  &#39;direction&#39;,
  &#39;from&#39;,
  &#39;barry&#39;,
  &#39;sonnenfeld&#39;,
  &#39;(&#39;,
  &#39;the&#39;,
  &#39;addam&#39;,
  &quot;&#39;&quot;,
  &#39;s&#39;,
  &#39;family&#39;,
  &#39;)&#39;,
  &#39;.&#39;,
  &#39;this&#39;,
  &#39;guy&#39;,
  &#39;has&#39;,
  &#39;a&#39;,
  &#39;knack&#39;,
  &#39;for&#39;,
  &quot;&#39;&quot;,
  &#39;gothic&#39;,
  &quot;&#39;&quot;,
  &#39;comedy&#39;,
  &#39;,&#39;,
  &#39;and&#39;,
  &#39;successfully&#39;,
  &#39;transfers&#39;,
  &#39;his&#39;,
  &#39;macabre&#39;,
  &#39;sense&#39;,
  &#39;of&#39;,
  &#39;humor&#39;,
  &#39;onto&#39;,
  &#39;the&#39;,
  &#39;screen&#39;,
  &#39;.&#39;,
  &#39;and&#39;,
  &#39;,&#39;,
  &#39;an&#39;,
  &#39;appropriate&#39;,
  &#39;dose&#39;,
  &#39;of&#39;,
  &#39;special&#39;,
  &#39;effects&#39;,
  &#39;helps&#39;,
  &#39;to&#39;,
  &#39;bolster&#39;,
  &#39;the&#39;,
  &#39;oddness&#39;,
  &#39;of&#39;,
  &#39;their&#39;,
  &#39;task&#39;,
  &#39;without&#39;,
  &#39;diverting&#39;,
  &#39;attention&#39;,
  &#39;from&#39;,
  &#39;the&#39;,
  &#39;human&#39;,
  &#39;actors&#39;,
  &#39;.&#39;,
  &#39;the&#39;,
  &#39;story&#39;,
  &#39;moves&#39;,
  &#39;well&#39;,
  &#39;,&#39;,
  &#39;and&#39;,
  &#39;before&#39;,
  &#39;you&#39;,
  &#39;know&#39;,
  &#39;it&#39;,
  &#39;,&#39;,
  &#39;the&#39;,
  &#39;end&#39;,
  &#39;credits&#39;,
  &#39;are&#39;,
  &#39;already&#39;,
  &#39;rolling&#39;,
  &#39;!&#39;,
  &#39;the&#39;,
  &#39;result&#39;,
  &#39;is&#39;,
  &#39;100&#39;,
  &#39;minutes&#39;,
  &#39;worth&#39;,
  &#39;of&#39;,
  &#39;fun&#39;,
  &#39;in&#39;,
  &#39;the&#39;,
  &#39;form&#39;,
  &#39;of&#39;,
  &#39;ewwwws&#39;,
  &#39;and&#39;,
  &#39;blechhhs&#39;,
  &#39;,&#39;,
  &#39;aaaahhhs&#39;,
  &#39;and&#39;,
  &#39;wows&#39;,
  &#39;.&#39;,
  &#39;let&#39;,
  &#39;the&#39;,
  &#39;men&#39;,
  &#39;in&#39;,
  &#39;black&#39;,
  &#39;protect&#39;,
  &#39;and&#39;,
  &#39;color&#39;,
  &#39;your&#39;,
  &#39;world&#39;,
  &#39;.&#39;],
 &#39;pos&#39;)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_words = [w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> movie_reviews.words()]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">all_words</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;plot&#39;,
 &#39;:&#39;,
 &#39;two&#39;,
 &#39;teen&#39;,
 &#39;couples&#39;,
 &#39;go&#39;,
 &#39;to&#39;,
 &#39;a&#39;,
 &#39;church&#39;,
 &#39;party&#39;,
 &#39;,&#39;,
 &#39;drink&#39;,
 &#39;and&#39;,
 &#39;then&#39;,
 &#39;drive&#39;,
 &#39;.&#39;,
 &#39;they&#39;,
 &#39;get&#39;,
 &#39;into&#39;,
 &#39;an&#39;,
 &#39;accident&#39;,
 &#39;.&#39;,
 &#39;one&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;guys&#39;,
 &#39;dies&#39;,
 &#39;,&#39;,
 &#39;but&#39;,
 &#39;his&#39;,
 &#39;girlfriend&#39;,
 &#39;continues&#39;,
 &#39;to&#39;,
 &#39;see&#39;,
 &#39;him&#39;,
 &#39;in&#39;,
 &#39;her&#39;,
 &#39;life&#39;,
 &#39;,&#39;,
 &#39;and&#39;,
 &#39;has&#39;,
 &#39;nightmares&#39;,
 &#39;.&#39;,
 &#39;what&#39;,
 &quot;&#39;&quot;,
 &#39;s&#39;,
 &#39;the&#39;,
 &#39;deal&#39;,
 &#39;?&#39;,
 &#39;watch&#39;,
 &#39;the&#39;,
 &#39;movie&#39;,
 &#39;and&#39;,
 &#39;&quot;&#39;,
 &#39;sorta&#39;,
 &#39;&quot;&#39;,
 &#39;find&#39;,
 &#39;out&#39;,
 &#39;.&#39;,
 &#39;.&#39;,
 &#39;.&#39;,
 &#39;critique&#39;,
 &#39;:&#39;,
 &#39;a&#39;,
 &#39;mind&#39;,
 &#39;-&#39;,
 &#39;fuck&#39;,
 &#39;movie&#39;,
 &#39;for&#39;,
 &#39;the&#39;,
 &#39;teen&#39;,
 &#39;generation&#39;,
 &#39;that&#39;,
 &#39;touches&#39;,
 &#39;on&#39;,
 &#39;a&#39;,
 &#39;very&#39;,
 &#39;cool&#39;,
 &#39;idea&#39;,
 &#39;,&#39;,
 &#39;but&#39;,
 &#39;presents&#39;,
 &#39;it&#39;,
 &#39;in&#39;,
 &#39;a&#39;,
 &#39;very&#39;,
 &#39;bad&#39;,
 &#39;package&#39;,
 &#39;.&#39;,
 &#39;which&#39;,
 &#39;is&#39;,
 &#39;what&#39;,
 &#39;makes&#39;,
 &#39;this&#39;,
 &#39;review&#39;,
 &#39;an&#39;,
 &#39;even&#39;,
 &#39;harder&#39;,
 &#39;one&#39;,
 &#39;to&#39;,
 &#39;write&#39;,
 &#39;,&#39;,
 &#39;since&#39;,
 &#39;i&#39;,
 &#39;generally&#39;,
 &#39;applaud&#39;,
 &#39;films&#39;,
 &#39;which&#39;,
 &#39;attempt&#39;,
 &#39;to&#39;,
 &#39;break&#39;,
 &#39;the&#39;,
 &#39;mold&#39;,
 &#39;,&#39;,
 &#39;mess&#39;,
 &#39;with&#39;,
 &#39;your&#39;,
 &#39;head&#39;,
 &#39;and&#39;,
 &#39;such&#39;,
 &#39;(&#39;,
 &#39;lost&#39;,
 &#39;highway&#39;,
 &#39;&amp;&#39;,
 &#39;memento&#39;,
 &#39;)&#39;,
 &#39;,&#39;,
 &#39;but&#39;,
 &#39;there&#39;,
 &#39;are&#39;,
 &#39;good&#39;,
 &#39;and&#39;,
 &#39;bad&#39;,
 &#39;ways&#39;,
 &#39;of&#39;,
 &#39;making&#39;,
 &#39;all&#39;,
 &#39;types&#39;,
 &#39;of&#39;,
 &#39;films&#39;,
 &#39;,&#39;,
 &#39;and&#39;,
 &#39;these&#39;,
 &#39;folks&#39;,
 &#39;just&#39;,
 &#39;didn&#39;,
 &quot;&#39;&quot;,
 &#39;t&#39;,
 &#39;snag&#39;,
 &#39;this&#39;,
 &#39;one&#39;,
 &#39;correctly&#39;,
 &#39;.&#39;,
 &#39;they&#39;,
 &#39;seem&#39;,
 &#39;to&#39;,
 &#39;have&#39;,
 &#39;taken&#39;,
 &#39;this&#39;,
 &#39;pretty&#39;,
 &#39;neat&#39;,
 &#39;concept&#39;,
 &#39;,&#39;,
 &#39;but&#39;,
 &#39;executed&#39;,
 &#39;it&#39;,
 &#39;terribly&#39;,
 &#39;.&#39;,
 &#39;so&#39;,
 &#39;what&#39;,
 &#39;are&#39;,
 &#39;the&#39;,
 &#39;problems&#39;,
 &#39;with&#39;,
 &#39;the&#39;,
 &#39;movie&#39;,
 &#39;?&#39;,
 &#39;well&#39;,
 &#39;,&#39;,
 &#39;its&#39;,
 &#39;main&#39;,
 &#39;problem&#39;,
 &#39;is&#39;,
 &#39;that&#39;,
 &#39;it&#39;,
 &quot;&#39;&quot;,
 &#39;s&#39;,
 &#39;simply&#39;,
 &#39;too&#39;,
 &#39;jumbled&#39;,
 &#39;.&#39;,
 &#39;it&#39;,
 &#39;starts&#39;,
 &#39;off&#39;,
 &#39;&quot;&#39;,
 &#39;normal&#39;,
 &#39;&quot;&#39;,
 &#39;but&#39;,
 &#39;then&#39;,
 &#39;downshifts&#39;,
 &#39;into&#39;,
 &#39;this&#39;,
 &#39;&quot;&#39;,
 &#39;fantasy&#39;,
 &#39;&quot;&#39;,
 &#39;world&#39;,
 &#39;in&#39;,
 &#39;which&#39;,
 &#39;you&#39;,
 &#39;,&#39;,
 &#39;as&#39;,
 &#39;an&#39;,
 &#39;audience&#39;,
 &#39;member&#39;,
 &#39;,&#39;,
 &#39;have&#39;,
 &#39;no&#39;,
 &#39;idea&#39;,
 &#39;what&#39;,
 &quot;&#39;&quot;,
 &#39;s&#39;,
 &#39;going&#39;,
 &#39;on&#39;,
 &#39;.&#39;,
 &#39;there&#39;,
 &#39;are&#39;,
 &#39;dreams&#39;,
 &#39;,&#39;,
 &#39;there&#39;,
 &#39;are&#39;,
 &#39;characters&#39;,
 &#39;coming&#39;,
 &#39;back&#39;,
 &#39;from&#39;,
 &#39;the&#39;,
 &#39;dead&#39;,
 &#39;,&#39;,
 &#39;there&#39;,
 &#39;are&#39;,
 &#39;others&#39;,
 &#39;who&#39;,
 &#39;look&#39;,
 &#39;like&#39;,
 &#39;the&#39;,
 &#39;dead&#39;,
 &#39;,&#39;,
 &#39;there&#39;,
 &#39;are&#39;,
 &#39;strange&#39;,
 &#39;apparitions&#39;,
 &#39;,&#39;,
 &#39;there&#39;,
 &#39;are&#39;,
 &#39;disappearances&#39;,
 &#39;,&#39;,
 &#39;there&#39;,
 &#39;are&#39;,
 &#39;a&#39;,
 &#39;looooot&#39;,
 &#39;of&#39;,
 &#39;chase&#39;,
 &#39;scenes&#39;,
 &#39;,&#39;,
 &#39;there&#39;,
 &#39;are&#39;,
 &#39;tons&#39;,
 &#39;of&#39;,
 &#39;weird&#39;,
 &#39;things&#39;,
 &#39;that&#39;,
 &#39;happen&#39;,
 &#39;,&#39;,
 &#39;and&#39;,
 &#39;most&#39;,
 &#39;of&#39;,
 &#39;it&#39;,
 &#39;is&#39;,
 &#39;simply&#39;,
 &#39;not&#39;,
 &#39;explained&#39;,
 &#39;.&#39;,
 &#39;now&#39;,
 &#39;i&#39;,
 &#39;personally&#39;,
 &#39;don&#39;,
 &quot;&#39;&quot;,
 &#39;t&#39;,
 &#39;mind&#39;,
 &#39;trying&#39;,
 &#39;to&#39;,
 &#39;unravel&#39;,
 &#39;a&#39;,
 &#39;film&#39;,
 &#39;every&#39;,
 &#39;now&#39;,
 &#39;and&#39;,
 &#39;then&#39;,
 &#39;,&#39;,
 &#39;but&#39;,
 &#39;when&#39;,
 &#39;all&#39;,
 &#39;it&#39;,
 &#39;does&#39;,
 &#39;is&#39;,
 &#39;give&#39;,
 &#39;me&#39;,
 &#39;the&#39;,
 &#39;same&#39;,
 &#39;clue&#39;,
 &#39;over&#39;,
 &#39;and&#39;,
 &#39;over&#39;,
 &#39;again&#39;,
 &#39;,&#39;,
 &#39;i&#39;,
 &#39;get&#39;,
 &#39;kind&#39;,
 &#39;of&#39;,
 &#39;fed&#39;,
 &#39;up&#39;,
 &#39;after&#39;,
 &#39;a&#39;,
 &#39;while&#39;,
 &#39;,&#39;,
 &#39;which&#39;,
 &#39;is&#39;,
 &#39;this&#39;,
 &#39;film&#39;,
 &quot;&#39;&quot;,
 &#39;s&#39;,
 &#39;biggest&#39;,
 &#39;problem&#39;,
 &#39;.&#39;,
 &#39;it&#39;,
 &quot;&#39;&quot;,
 &#39;s&#39;,
 &#39;obviously&#39;,
 &#39;got&#39;,
 &#39;this&#39;,
 &#39;big&#39;,
 &#39;secret&#39;,
 &#39;to&#39;,
 &#39;hide&#39;,
 &#39;,&#39;,
 &#39;but&#39;,
 &#39;it&#39;,
 &#39;seems&#39;,
 &#39;to&#39;,
 &#39;want&#39;,
 &#39;to&#39;,
 &#39;hide&#39;,
 &#39;it&#39;,
 &#39;completely&#39;,
 &#39;until&#39;,
 &#39;its&#39;,
 &#39;final&#39;,
 &#39;five&#39;,
 &#39;minutes&#39;,
 &#39;.&#39;,
 &#39;and&#39;,
 &#39;do&#39;,
 &#39;they&#39;,
 &#39;make&#39;,
 &#39;things&#39;,
 &#39;entertaining&#39;,
 &#39;,&#39;,
 &#39;thrilling&#39;,
 &#39;or&#39;,
 &#39;even&#39;,
 &#39;engaging&#39;,
 &#39;,&#39;,
 &#39;in&#39;,
 &#39;the&#39;,
 &#39;meantime&#39;,
 &#39;?&#39;,
 &#39;not&#39;,
 &#39;really&#39;,
 &#39;.&#39;,
 &#39;the&#39;,
 &#39;sad&#39;,
 &#39;part&#39;,
 &#39;is&#39;,
 &#39;that&#39;,
 &#39;the&#39;,
 &#39;arrow&#39;,
 &#39;and&#39;,
 &#39;i&#39;,
 &#39;both&#39;,
 &#39;dig&#39;,
 &#39;on&#39;,
 &#39;flicks&#39;,
 &#39;like&#39;,
 &#39;this&#39;,
 &#39;,&#39;,
 &#39;so&#39;,
 &#39;we&#39;,
 &#39;actually&#39;,
 &#39;figured&#39;,
 &#39;most&#39;,
 &#39;of&#39;,
 &#39;it&#39;,
 &#39;out&#39;,
 &#39;by&#39;,
 &#39;the&#39;,
 &#39;half&#39;,
 &#39;-&#39;,
 &#39;way&#39;,
 &#39;point&#39;,
 &#39;,&#39;,
 &#39;so&#39;,
 &#39;all&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;strangeness&#39;,
 &#39;after&#39;,
 &#39;that&#39;,
 &#39;did&#39;,
 &#39;start&#39;,
 &#39;to&#39;,
 &#39;make&#39;,
 &#39;a&#39;,
 &#39;little&#39;,
 &#39;bit&#39;,
 &#39;of&#39;,
 &#39;sense&#39;,
 &#39;,&#39;,
 &#39;but&#39;,
 &#39;it&#39;,
 &#39;still&#39;,
 &#39;didn&#39;,
 &quot;&#39;&quot;,
 &#39;t&#39;,
 &#39;the&#39;,
 &#39;make&#39;,
 &#39;the&#39;,
 &#39;film&#39;,
 &#39;all&#39;,
 &#39;that&#39;,
 &#39;more&#39;,
 &#39;entertaining&#39;,
 &#39;.&#39;,
 &#39;i&#39;,
 &#39;guess&#39;,
 &#39;the&#39;,
 &#39;bottom&#39;,
 &#39;line&#39;,
 &#39;with&#39;,
 &#39;movies&#39;,
 &#39;like&#39;,
 &#39;this&#39;,
 &#39;is&#39;,
 &#39;that&#39;,
 &#39;you&#39;,
 &#39;should&#39;,
 &#39;always&#39;,
 &#39;make&#39;,
 &#39;sure&#39;,
 &#39;that&#39;,
 &#39;the&#39;,
 &#39;audience&#39;,
 &#39;is&#39;,
 &#39;&quot;&#39;,
 &#39;into&#39;,
 &#39;it&#39;,
 &#39;&quot;&#39;,
 &#39;even&#39;,
 &#39;before&#39;,
 &#39;they&#39;,
 &#39;are&#39;,
 &#39;given&#39;,
 &#39;the&#39;,
 &#39;secret&#39;,
 &#39;password&#39;,
 &#39;to&#39;,
 &#39;enter&#39;,
 &#39;your&#39;,
 &#39;world&#39;,
 &#39;of&#39;,
 &#39;understanding&#39;,
 &#39;.&#39;,
 &#39;i&#39;,
 &#39;mean&#39;,
 &#39;,&#39;,
 &#39;showing&#39;,
 &#39;melissa&#39;,
 &#39;sagemiller&#39;,
 &#39;running&#39;,
 &#39;away&#39;,
 &#39;from&#39;,
 &#39;visions&#39;,
 &#39;for&#39;,
 &#39;about&#39;,
 &#39;20&#39;,
 &#39;minutes&#39;,
 &#39;throughout&#39;,
 &#39;the&#39;,
 &#39;movie&#39;,
 &#39;is&#39;,
 &#39;just&#39;,
 &#39;plain&#39;,
 &#39;lazy&#39;,
 &#39;!&#39;,
 &#39;!&#39;,
 &#39;okay&#39;,
 &#39;,&#39;,
 &#39;we&#39;,
 &#39;get&#39;,
 &#39;it&#39;,
 &#39;.&#39;,
 &#39;.&#39;,
 &#39;.&#39;,
 &#39;there&#39;,
 &#39;are&#39;,
 &#39;people&#39;,
 &#39;chasing&#39;,
 &#39;her&#39;,
 &#39;and&#39;,
 &#39;we&#39;,
 &#39;don&#39;,
 &quot;&#39;&quot;,
 &#39;t&#39;,
 &#39;know&#39;,
 &#39;who&#39;,
 &#39;they&#39;,
 &#39;are&#39;,
 &#39;.&#39;,
 &#39;do&#39;,
 &#39;we&#39;,
 &#39;really&#39;,
 &#39;need&#39;,
 &#39;to&#39;,
 &#39;see&#39;,
 &#39;it&#39;,
 &#39;over&#39;,
 &#39;and&#39;,
 &#39;over&#39;,
 &#39;again&#39;,
 &#39;?&#39;,
 &#39;how&#39;,
 &#39;about&#39;,
 &#39;giving&#39;,
 &#39;us&#39;,
 &#39;different&#39;,
 &#39;scenes&#39;,
 &#39;offering&#39;,
 &#39;further&#39;,
 &#39;insight&#39;,
 &#39;into&#39;,
 &#39;all&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;strangeness&#39;,
 &#39;going&#39;,
 &#39;down&#39;,
 &#39;in&#39;,
 &#39;the&#39;,
 &#39;movie&#39;,
 &#39;?&#39;,
 &#39;apparently&#39;,
 &#39;,&#39;,
 &#39;the&#39;,
 &#39;studio&#39;,
 &#39;took&#39;,
 &#39;this&#39;,
 &#39;film&#39;,
 &#39;away&#39;,
 &#39;from&#39;,
 &#39;its&#39;,
 &#39;director&#39;,
 &#39;and&#39;,
 &#39;chopped&#39;,
 &#39;it&#39;,
 &#39;up&#39;,
 &#39;themselves&#39;,
 &#39;,&#39;,
 &#39;and&#39;,
 &#39;it&#39;,
 &#39;shows&#39;,
 &#39;.&#39;,
 &#39;there&#39;,
 &#39;might&#39;,
 &quot;&#39;&quot;,
 &#39;ve&#39;,
 &#39;been&#39;,
 &#39;a&#39;,
 &#39;pretty&#39;,
 &#39;decent&#39;,
 &#39;teen&#39;,
 &#39;mind&#39;,
 &#39;-&#39;,
 &#39;fuck&#39;,
 &#39;movie&#39;,
 &#39;in&#39;,
 &#39;here&#39;,
 &#39;somewhere&#39;,
 &#39;,&#39;,
 &#39;but&#39;,
 &#39;i&#39;,
 &#39;guess&#39;,
 &#39;&quot;&#39;,
 &#39;the&#39;,
 &#39;suits&#39;,
 &#39;&quot;&#39;,
 &#39;decided&#39;,
 &#39;that&#39;,
 &#39;turning&#39;,
 &#39;it&#39;,
 &#39;into&#39;,
 &#39;a&#39;,
 &#39;music&#39;,
 &#39;video&#39;,
 &#39;with&#39;,
 &#39;little&#39;,
 &#39;edge&#39;,
 &#39;,&#39;,
 &#39;would&#39;,
 &#39;make&#39;,
 &#39;more&#39;,
 &#39;sense&#39;,
 &#39;.&#39;,
 &#39;the&#39;,
 &#39;actors&#39;,
 &#39;are&#39;,
 &#39;pretty&#39;,
 &#39;good&#39;,
 &#39;for&#39;,
 &#39;the&#39;,
 &#39;most&#39;,
 &#39;part&#39;,
 &#39;,&#39;,
 &#39;although&#39;,
 &#39;wes&#39;,
 &#39;bentley&#39;,
 &#39;just&#39;,
 &#39;seemed&#39;,
 &#39;to&#39;,
 &#39;be&#39;,
 &#39;playing&#39;,
 &#39;the&#39;,
 &#39;exact&#39;,
 &#39;same&#39;,
 &#39;character&#39;,
 &#39;that&#39;,
 &#39;he&#39;,
 &#39;did&#39;,
 &#39;in&#39;,
 &#39;american&#39;,
 &#39;beauty&#39;,
 &#39;,&#39;,
 &#39;only&#39;,
 &#39;in&#39;,
 &#39;a&#39;,
 &#39;new&#39;,
 &#39;neighborhood&#39;,
 &#39;.&#39;,
 &#39;but&#39;,
 &#39;my&#39;,
 &#39;biggest&#39;,
 &#39;kudos&#39;,
 &#39;go&#39;,
 &#39;out&#39;,
 &#39;to&#39;,
 &#39;sagemiller&#39;,
 &#39;,&#39;,
 &#39;who&#39;,
 &#39;holds&#39;,
 &#39;her&#39;,
 &#39;own&#39;,
 &#39;throughout&#39;,
 &#39;the&#39;,
 &#39;entire&#39;,
 &#39;film&#39;,
 &#39;,&#39;,
 &#39;and&#39;,
 &#39;actually&#39;,
 &#39;has&#39;,
 &#39;you&#39;,
 &#39;feeling&#39;,
 &#39;her&#39;,
 &#39;character&#39;,
 &quot;&#39;&quot;,
 &#39;s&#39;,
 &#39;unraveling&#39;,
 &#39;.&#39;,
 &#39;overall&#39;,
 &#39;,&#39;,
 &#39;the&#39;,
 &#39;film&#39;,
 &#39;doesn&#39;,
 &quot;&#39;&quot;,
 &#39;t&#39;,
 &#39;stick&#39;,
 &#39;because&#39;,
 &#39;it&#39;,
 &#39;doesn&#39;,
 &quot;&#39;&quot;,
 &#39;t&#39;,
 &#39;entertain&#39;,
 &#39;,&#39;,
 &#39;it&#39;,
 &quot;&#39;&quot;,
 &#39;s&#39;,
 &#39;confusing&#39;,
 &#39;,&#39;,
 &#39;it&#39;,
 &#39;rarely&#39;,
 &#39;excites&#39;,
 &#39;and&#39;,
 &#39;it&#39;,
 &#39;feels&#39;,
 &#39;pretty&#39;,
 &#39;redundant&#39;,
 &#39;for&#39;,
 &#39;most&#39;,
 &#39;of&#39;,
 &#39;its&#39;,
 &#39;runtime&#39;,
 &#39;,&#39;,
 &#39;despite&#39;,
 &#39;a&#39;,
 &#39;pretty&#39;,
 &#39;cool&#39;,
 &#39;ending&#39;,
 &#39;and&#39;,
 &#39;explanation&#39;,
 &#39;to&#39;,
 &#39;all&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;craziness&#39;,
 &#39;that&#39;,
 &#39;came&#39;,
 &#39;before&#39;,
 &#39;it&#39;,
 &#39;.&#39;,
 &#39;oh&#39;,
 &#39;,&#39;,
 &#39;and&#39;,
 &#39;by&#39;,
 &#39;the&#39;,
 &#39;way&#39;,
 &#39;,&#39;,
 &#39;this&#39;,
 &#39;is&#39;,
 &#39;not&#39;,
 &#39;a&#39;,
 &#39;horror&#39;,
 &#39;or&#39;,
 &#39;teen&#39;,
 &#39;slasher&#39;,
 &#39;flick&#39;,
 &#39;.&#39;,
 &#39;.&#39;,
 &#39;.&#39;,
 &#39;it&#39;,
 &quot;&#39;&quot;,
 &#39;s&#39;,
 &#39;just&#39;,
 &#39;packaged&#39;,
 &#39;to&#39;,
 &#39;look&#39;,
 &#39;that&#39;,
 &#39;way&#39;,
 &#39;because&#39;,
 &#39;someone&#39;,
 &#39;is&#39;,
 &#39;apparently&#39;,
 &#39;assuming&#39;,
 &#39;that&#39;,
 &#39;the&#39;,
 &#39;genre&#39;,
 &#39;is&#39;,
 &#39;still&#39;,
 &#39;hot&#39;,
 &#39;with&#39;,
 &#39;the&#39;,
 &#39;kids&#39;,
 &#39;.&#39;,
 &#39;it&#39;,
 &#39;also&#39;,
 &#39;wrapped&#39;,
 &#39;production&#39;,
 &#39;two&#39;,
 &#39;years&#39;,
 &#39;ago&#39;,
 &#39;and&#39;,
 &#39;has&#39;,
 &#39;been&#39;,
 &#39;sitting&#39;,
 &#39;on&#39;,
 &#39;the&#39;,
 &#39;shelves&#39;,
 &#39;ever&#39;,
 &#39;since&#39;,
 &#39;.&#39;,
 &#39;whatever&#39;,
 &#39;.&#39;,
 &#39;.&#39;,
 &#39;.&#39;,
 &#39;skip&#39;,
 &#39;it&#39;,
 &#39;!&#39;,
 &#39;where&#39;,
 &quot;&#39;&quot;,
 &#39;s&#39;,
 &#39;joblo&#39;,
 &#39;coming&#39;,
 &#39;from&#39;,
 &#39;?&#39;,
 &#39;a&#39;,
 &#39;nightmare&#39;,
 &#39;of&#39;,
 &#39;elm&#39;,
 &#39;street&#39;,
 &#39;3&#39;,
 &#39;(&#39;,
 &#39;7&#39;,
 &#39;/&#39;,
 &#39;10&#39;,
 &#39;)&#39;,
 &#39;-&#39;,
 &#39;blair&#39;,
 &#39;witch&#39;,
 &#39;2&#39;,
 &#39;(&#39;,
 &#39;7&#39;,
 &#39;/&#39;,
 &#39;10&#39;,
 &#39;)&#39;,
 &#39;-&#39;,
 &#39;the&#39;,
 &#39;crow&#39;,
 &#39;(&#39;,
 &#39;9&#39;,
 &#39;/&#39;,
 &#39;10&#39;,
 &#39;)&#39;,
 &#39;-&#39;,
 &#39;the&#39;,
 &#39;crow&#39;,
 &#39;:&#39;,
 &#39;salvation&#39;,
 &#39;(&#39;,
 &#39;4&#39;,
 &#39;/&#39;,
 &#39;10&#39;,
 &#39;)&#39;,
 &#39;-&#39;,
 &#39;lost&#39;,
 &#39;highway&#39;,
 &#39;(&#39;,
 &#39;10&#39;,
 &#39;/&#39;,
 &#39;10&#39;,
 &#39;)&#39;,
 &#39;-&#39;,
 &#39;memento&#39;,
 &#39;(&#39;,
 &#39;10&#39;,
 &#39;/&#39;,
 &#39;10&#39;,
 &#39;)&#39;,
 &#39;-&#39;,
 &#39;the&#39;,
 &#39;others&#39;,
 &#39;(&#39;,
 &#39;9&#39;,
 &#39;/&#39;,
 &#39;10&#39;,
 &#39;)&#39;,
 &#39;-&#39;,
 &#39;stir&#39;,
 &#39;of&#39;,
 &#39;echoes&#39;,
 &#39;(&#39;,
 &#39;8&#39;,
 &#39;/&#39;,
 &#39;10&#39;,
 &#39;)&#39;,
 &#39;the&#39;,
 &#39;happy&#39;,
 &#39;bastard&#39;,
 &quot;&#39;&quot;,
 &#39;s&#39;,
 &#39;quick&#39;,
 &#39;movie&#39;,
 &#39;review&#39;,
 &#39;damn&#39;,
 &#39;that&#39;,
 &#39;y2k&#39;,
 &#39;bug&#39;,
 &#39;.&#39;,
 &#39;it&#39;,
 &quot;&#39;&quot;,
 &#39;s&#39;,
 &#39;got&#39;,
 &#39;a&#39;,
 &#39;head&#39;,
 &#39;start&#39;,
 &#39;in&#39;,
 &#39;this&#39;,
 &#39;movie&#39;,
 &#39;starring&#39;,
 &#39;jamie&#39;,
 &#39;lee&#39;,
 &#39;curtis&#39;,
 &#39;and&#39;,
 &#39;another&#39;,
 &#39;baldwin&#39;,
 &#39;brother&#39;,
 &#39;(&#39;,
 &#39;william&#39;,
 &#39;this&#39;,
 &#39;time&#39;,
 &#39;)&#39;,
 &#39;in&#39;,
 &#39;a&#39;,
 &#39;story&#39;,
 &#39;regarding&#39;,
 &#39;a&#39;,
 &#39;crew&#39;,
 &#39;of&#39;,
 &#39;a&#39;,
 &#39;tugboat&#39;,
 &#39;that&#39;,
 &#39;comes&#39;,
 &#39;across&#39;,
 &#39;a&#39;,
 &#39;deserted&#39;,
 &#39;russian&#39;,
 &#39;tech&#39;,
 &#39;ship&#39;,
 &#39;that&#39;,
 &#39;has&#39;,
 &#39;a&#39;,
 &#39;strangeness&#39;,
 &#39;to&#39;,
 &#39;it&#39;,
 &#39;when&#39;,
 &#39;they&#39;,
 &#39;kick&#39;,
 &#39;the&#39;,
 &#39;power&#39;,
 &#39;back&#39;,
 &#39;on&#39;,
 &#39;.&#39;,
 &#39;little&#39;,
 &#39;do&#39;,
 &#39;they&#39;,
 &#39;know&#39;,
 &#39;the&#39;,
 &#39;power&#39;,
 &#39;within&#39;,
 &#39;.&#39;,
 &#39;.&#39;,
 &#39;.&#39;,
 &#39;going&#39;,
 &#39;for&#39;,
 &#39;the&#39;,
 &#39;gore&#39;,
 &#39;and&#39;,
 &#39;bringing&#39;,
 &#39;on&#39;,
 &#39;a&#39;,
 &#39;few&#39;,
 &#39;action&#39;,
 &#39;sequences&#39;,
 &#39;here&#39;,
 &#39;and&#39;,
 &#39;there&#39;,
 &#39;,&#39;,
 &#39;virus&#39;,
 &#39;still&#39;,
 &#39;feels&#39;,
 &#39;very&#39;,
 &#39;empty&#39;,
 &#39;,&#39;,
 &#39;like&#39;,
 &#39;a&#39;,
 &#39;movie&#39;,
 &#39;going&#39;,
 &#39;for&#39;,
 &#39;all&#39;,
 &#39;flash&#39;,
 &#39;and&#39;,
 &#39;no&#39;,
 &#39;substance&#39;,
 &#39;.&#39;,
 &#39;we&#39;,
 &#39;don&#39;,
 &quot;&#39;&quot;,
 &#39;t&#39;,
 &#39;know&#39;,
 &#39;why&#39;,
 &#39;the&#39;,
 &#39;crew&#39;,
 &#39;was&#39;,
 &#39;really&#39;,
 &#39;out&#39;,
 &#39;in&#39;,
 ...]
</code></pre><p><strong>Warning</strong>:At the begining, I just writed the follow codes like this:<code>new_all_words = [w for w in all_words if w not in nltk.corpus.stopwords.words(&#39;english&#39;)</code>,however the code couldn’t complite successfully even I had been waiting for several minites. Finally, I found that the I/O operations can be 1583820 times, and the operation system read data from the hark disk again and again without saying any thing,it was so stupid.So when we programming,we should set the I/O resources as a variable if it will be used for several times. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stopwords = nltk.corpus.stopwords.words(<span class="string">'english'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_all_words = [w <span class="keyword">for</span> w <span class="keyword">in</span> all_words <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stopwords]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">new_all_words = [w <span class="keyword">for</span> w <span class="keyword">in</span> new_all_words <span class="keyword">if</span> w.isalpha()]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(all_words)</span><br></pre></td></tr></table></figure>
<pre><code>1583820
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(nltk.corpus.stopwords.words(<span class="string">'english'</span>))</span><br></pre></td></tr></table></figure>
<pre><code>179
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nltk.corpus.stopwords.words(<span class="string">'english'</span>)</span><br></pre></td></tr></table></figure>
<pre><code>[&#39;i&#39;,
 &#39;me&#39;,
 &#39;my&#39;,
 &#39;myself&#39;,
 &#39;we&#39;,
 &#39;our&#39;,
 &#39;ours&#39;,
 &#39;ourselves&#39;,
 &#39;you&#39;,
 &quot;you&#39;re&quot;,
 &quot;you&#39;ve&quot;,
 &quot;you&#39;ll&quot;,
 &quot;you&#39;d&quot;,
 &#39;your&#39;,
 &#39;yours&#39;,
 &#39;yourself&#39;,
 &#39;yourselves&#39;,
 &#39;he&#39;,
 &#39;him&#39;,
 &#39;his&#39;,
 &#39;himself&#39;,
 &#39;she&#39;,
 &quot;she&#39;s&quot;,
 &#39;her&#39;,
 &#39;hers&#39;,
 &#39;herself&#39;,
 &#39;it&#39;,
 &quot;it&#39;s&quot;,
 &#39;its&#39;,
 &#39;itself&#39;,
 &#39;they&#39;,
 &#39;them&#39;,
 &#39;their&#39;,
 &#39;theirs&#39;,
 &#39;themselves&#39;,
 &#39;what&#39;,
 &#39;which&#39;,
 &#39;who&#39;,
 &#39;whom&#39;,
 &#39;this&#39;,
 &#39;that&#39;,
 &quot;that&#39;ll&quot;,
 &#39;these&#39;,
 &#39;those&#39;,
 &#39;am&#39;,
 &#39;is&#39;,
 &#39;are&#39;,
 &#39;was&#39;,
 &#39;were&#39;,
 &#39;be&#39;,
 &#39;been&#39;,
 &#39;being&#39;,
 &#39;have&#39;,
 &#39;has&#39;,
 &#39;had&#39;,
 &#39;having&#39;,
 &#39;do&#39;,
 &#39;does&#39;,
 &#39;did&#39;,
 &#39;doing&#39;,
 &#39;a&#39;,
 &#39;an&#39;,
 &#39;the&#39;,
 &#39;and&#39;,
 &#39;but&#39;,
 &#39;if&#39;,
 &#39;or&#39;,
 &#39;because&#39;,
 &#39;as&#39;,
 &#39;until&#39;,
 &#39;while&#39;,
 &#39;of&#39;,
 &#39;at&#39;,
 &#39;by&#39;,
 &#39;for&#39;,
 &#39;with&#39;,
 &#39;about&#39;,
 &#39;against&#39;,
 &#39;between&#39;,
 &#39;into&#39;,
 &#39;through&#39;,
 &#39;during&#39;,
 &#39;before&#39;,
 &#39;after&#39;,
 &#39;above&#39;,
 &#39;below&#39;,
 &#39;to&#39;,
 &#39;from&#39;,
 &#39;up&#39;,
 &#39;down&#39;,
 &#39;in&#39;,
 &#39;out&#39;,
 &#39;on&#39;,
 &#39;off&#39;,
 &#39;over&#39;,
 &#39;under&#39;,
 &#39;again&#39;,
 &#39;further&#39;,
 &#39;then&#39;,
 &#39;once&#39;,
 &#39;here&#39;,
 &#39;there&#39;,
 &#39;when&#39;,
 &#39;where&#39;,
 &#39;why&#39;,
 &#39;how&#39;,
 &#39;all&#39;,
 &#39;any&#39;,
 &#39;both&#39;,
 &#39;each&#39;,
 &#39;few&#39;,
 &#39;more&#39;,
 &#39;most&#39;,
 &#39;other&#39;,
 &#39;some&#39;,
 &#39;such&#39;,
 &#39;no&#39;,
 &#39;nor&#39;,
 &#39;not&#39;,
 &#39;only&#39;,
 &#39;own&#39;,
 &#39;same&#39;,
 &#39;so&#39;,
 &#39;than&#39;,
 &#39;too&#39;,
 &#39;very&#39;,
 &#39;s&#39;,
 &#39;t&#39;,
 &#39;can&#39;,
 &#39;will&#39;,
 &#39;just&#39;,
 &#39;don&#39;,
 &quot;don&#39;t&quot;,
 &#39;should&#39;,
 &quot;should&#39;ve&quot;,
 &#39;now&#39;,
 &#39;d&#39;,
 &#39;ll&#39;,
 &#39;m&#39;,
 &#39;o&#39;,
 &#39;re&#39;,
 &#39;ve&#39;,
 &#39;y&#39;,
 &#39;ain&#39;,
 &#39;aren&#39;,
 &quot;aren&#39;t&quot;,
 &#39;couldn&#39;,
 &quot;couldn&#39;t&quot;,
 &#39;didn&#39;,
 &quot;didn&#39;t&quot;,
 &#39;doesn&#39;,
 &quot;doesn&#39;t&quot;,
 &#39;hadn&#39;,
 &quot;hadn&#39;t&quot;,
 &#39;hasn&#39;,
 &quot;hasn&#39;t&quot;,
 &#39;haven&#39;,
 &quot;haven&#39;t&quot;,
 &#39;isn&#39;,
 &quot;isn&#39;t&quot;,
 &#39;ma&#39;,
 &#39;mightn&#39;,
 &quot;mightn&#39;t&quot;,
 &#39;mustn&#39;,
 &quot;mustn&#39;t&quot;,
 &#39;needn&#39;,
 &quot;needn&#39;t&quot;,
 &#39;shan&#39;,
 &quot;shan&#39;t&quot;,
 &#39;shouldn&#39;,
 &quot;shouldn&#39;t&quot;,
 &#39;wasn&#39;,
 &quot;wasn&#39;t&quot;,
 &#39;weren&#39;,
 &quot;weren&#39;t&quot;,
 &#39;won&#39;,
 &quot;won&#39;t&quot;,
 &#39;wouldn&#39;,
 &quot;wouldn&#39;t&quot;]
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">words_freqlist = nltk.FreqDist(new_all_words)</span><br><span class="line">print(words_freqlist.most_common(<span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<pre><code>[(&#39;film&#39;, 9517), (&#39;one&#39;, 5852), (&#39;movie&#39;, 5771), (&#39;like&#39;, 3690), (&#39;even&#39;, 2565), (&#39;good&#39;, 2411), (&#39;time&#39;, 2411), (&#39;story&#39;, 2169), (&#39;would&#39;, 2109), (&#39;much&#39;, 2049)]
</code></pre><h3 id="12-Words-as-Features-for-Learning-用来学习的特征词汇"><a href="#12-Words-as-Features-for-Learning-用来学习的特征词汇" class="headerlink" title="12. Words as Features for Learning(用来学习的特征词汇)"></a>12. Words as Features for Learning(用来学习的特征词汇)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">word_features = list(words_freqlist.keys())[:<span class="number">3000</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_features</span><span class="params">(document)</span>:</span></span><br><span class="line">    words = set(document)</span><br><span class="line">    features = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> word_features:</span><br><span class="line">        features[w] = (w <span class="keyword">in</span> words)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line">featuresets = [(find_features(rev), category) <span class="keyword">for</span> (rev, category) <span class="keyword">in</span> documents]</span><br></pre></td></tr></table></figure>
<h3 id="13-Naive-Bayes-朴素贝叶斯"><a href="#13-Naive-Bayes-朴素贝叶斯" class="headerlink" title="13. Naive Bayes(朴素贝叶斯)"></a>13. Naive Bayes(朴素贝叶斯)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">training_set = featuresets[:<span class="number">1900</span>]</span><br><span class="line">testing_set = featuresets[<span class="number">1900</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classifier = nltk.NaiveBayesClassifier.train(training_set)</span><br><span class="line">print(<span class="string">"Naive Bayes Algo accuracy:"</span>,(nltk.classify.accuracy(classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Naive Bayes Algo accuracy: 84.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classifier.show_most_informative_features(<span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Most Informative Features
                   sucks = True              neg : pos    =      8.7 : 1.0
                  annual = True              pos : neg    =      8.2 : 1.0
                 frances = True              pos : neg    =      8.2 : 1.0
           unimaginative = True              neg : pos    =      7.8 : 1.0
                 idiotic = True              neg : pos    =      7.3 : 1.0
              schumacher = True              neg : pos    =      7.1 : 1.0
                    mena = True              neg : pos    =      7.1 : 1.0
               atrocious = True              neg : pos    =      7.1 : 1.0
             silverstone = True              neg : pos    =      7.1 : 1.0
                  suvari = True              neg : pos    =      7.1 : 1.0
                  turkey = True              neg : pos    =      6.7 : 1.0
                  regard = True              pos : neg    =      6.5 : 1.0
                 kidding = True              neg : pos    =      6.4 : 1.0
                  crappy = True              neg : pos    =      6.4 : 1.0
                  shoddy = True              neg : pos    =      6.4 : 1.0
</code></pre><h3 id="14-Save-Classifier-with-Pickle-使用Pickle保存分类器"><a href="#14-Save-Classifier-with-Pickle-使用Pickle保存分类器" class="headerlink" title="14. Save Classifier with Pickle(使用Pickle保存分类器)"></a>14. Save Classifier with Pickle(使用Pickle保存分类器)</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_classifier = open(<span class="string">'naivebayes.pickle'</span>,<span class="string">"wb"</span>)</span><br><span class="line">pickle.dump(classifier,save_classifier)</span><br><span class="line">save_classifier.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">classifier_f = open(<span class="string">'naivebayes.pickle'</span>,<span class="string">"rb"</span>)</span><br><span class="line">classifier = pickle.load(classifier_f)</span><br><span class="line">classifier_f.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Naive Bayes Algo accuracy:"</span>,(nltk.classify.accuracy(classifier,testing_set))*<span class="number">100</span>)</span><br><span class="line">classifier.show_most_informative_features(<span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Naive Bayes Algo accuracy: 84.0
Most Informative Features
                   sucks = True              neg : pos    =      8.7 : 1.0
                  annual = True              pos : neg    =      8.2 : 1.0
                 frances = True              pos : neg    =      8.2 : 1.0
           unimaginative = True              neg : pos    =      7.8 : 1.0
                 idiotic = True              neg : pos    =      7.3 : 1.0
              schumacher = True              neg : pos    =      7.1 : 1.0
                    mena = True              neg : pos    =      7.1 : 1.0
               atrocious = True              neg : pos    =      7.1 : 1.0
             silverstone = True              neg : pos    =      7.1 : 1.0
                  suvari = True              neg : pos    =      7.1 : 1.0
                  turkey = True              neg : pos    =      6.7 : 1.0
                  regard = True              pos : neg    =      6.5 : 1.0
                 kidding = True              neg : pos    =      6.4 : 1.0
                  crappy = True              neg : pos    =      6.4 : 1.0
                  shoddy = True              neg : pos    =      6.4 : 1.0
</code></pre><p>training again</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">random.shuffle(documents)</span><br><span class="line">all_words = [w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> movie_reviews.words()]</span><br><span class="line">new_all_words = [w <span class="keyword">for</span> w <span class="keyword">in</span> all_words <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stopwords]</span><br><span class="line">new_all_words = [w <span class="keyword">for</span> w <span class="keyword">in</span> new_all_words <span class="keyword">if</span> w.isalpha()]</span><br><span class="line">words_freqlist = nltk.FreqDist(new_all_words)</span><br><span class="line">word_features = list(words_freqlist.keys())[:<span class="number">3000</span>]</span><br><span class="line">training_set = featuresets[:<span class="number">1900</span>]</span><br><span class="line">testing_set = featuresets[<span class="number">1900</span>:]</span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(training_set)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Naive Bayes Algo accuracy:"</span>,(nltk.classify.accuracy(classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Naive Bayes Algo accuracy: 84.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">classifier_f = open(<span class="string">'naivebayes.pickle'</span>,<span class="string">"rb"</span>)</span><br><span class="line">classifier = pickle.load(classifier_f)</span><br><span class="line">classifier_f.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Naive Bayes Algo accuracy:"</span>,(nltk.classify.accuracy(classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Naive Bayes Algo accuracy: 84.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Naive Bayes Algo accuracy:"</span>,(nltk.classify.accuracy(classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Naive Bayes Algo accuracy: 84.0
</code></pre><h3 id="15-Scikit-Learn-incorporation"><a href="#15-Scikit-Learn-incorporation" class="headerlink" title="15. Scikit-Learn incorporation()"></a>15. Scikit-Learn incorporation()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.classify.scikitlearn <span class="keyword">import</span> SklearnClassifier</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB, GaussianNB, BernoulliNB</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MNB_classifier = SklearnClassifier(MultinomialNB())</span><br><span class="line">MNB_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"MNB_classifier accuracy percent:"</span>,(nltk.classify.accuracy(MNB_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>MNB_classifier accuracy percent: 82.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这段代码有问题，不可以运行。</span></span><br><span class="line">GNB_classifier = SklearnClassifier(GaussianNB())</span><br><span class="line">GNB_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"GNB_classifier accuracy percent:"</span>,(nltk.classify.accuracy(GNB_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>---------------------------------------------------------------------------

TypeError                                 Traceback (most recent call last)

&lt;ipython-input-149-dbf69e211330&gt; in &lt;module&gt;()
      1 GNB_classifier = SklearnClassifier(GaussianNB())
----&gt; 2 GNB_classifier.train(training_set)
      3 print(&quot;GNB_classifier accuracy percent:&quot;,(nltk.classify.accuracy(GNB_classifier,testing_set))*100)


C:\Program Files\Anaconda3\lib\site-packages\nltk\classify\scikitlearn.py in train(self, labeled_featuresets)
    117         X = self._vectorizer.fit_transform(X)
    118         y = self._encoder.fit_transform(y)
--&gt; 119         self._clf.fit(X, y)
    120 
    121         return self


C:\Program Files\Anaconda3\lib\site-packages\sklearn\naive_bayes.py in fit(self, X, y, sample_weight)
    180             Returns self.
    181         &quot;&quot;&quot;
--&gt; 182         X, y = check_X_y(X, y)
    183         return self._partial_fit(X, y, np.unique(y), _refit=True,
    184                                  sample_weight=sample_weight)


C:\Program Files\Anaconda3\lib\site-packages\sklearn\utils\validation.py in check_X_y(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)
    519     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,
    520                     ensure_2d, allow_nd, ensure_min_samples,
--&gt; 521                     ensure_min_features, warn_on_dtype, estimator)
    522     if multi_output:
    523         y = check_array(y, &#39;csr&#39;, force_all_finite=True, ensure_2d=False,


C:\Program Files\Anaconda3\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
    378     if sp.issparse(array):
    379         array = _ensure_sparse_format(array, accept_sparse, dtype, copy,
--&gt; 380                                       force_all_finite)
    381     else:
    382         array = np.array(array, dtype=dtype, order=order, copy=copy)


C:\Program Files\Anaconda3\lib\site-packages\sklearn\utils\validation.py in _ensure_sparse_format(spmatrix, accept_sparse, dtype, copy, force_all_finite)
    241     &quot;&quot;&quot;
    242     if accept_sparse in [None, False]:
--&gt; 243         raise TypeError(&#39;A sparse matrix was passed, but dense &#39;
    244                         &#39;data is required. Use X.toarray() to &#39;
    245                         &#39;convert to a dense numpy array.&#39;)


TypeError: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">BNB_classifier = SklearnClassifier(BernoulliNB())</span><br><span class="line">BNB_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"BNB_classifier accuracy percent:"</span>,(nltk.classify.accuracy(BNB_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>BNB_classifier accuracy percent: 84.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression, SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC, NuSVC</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LogisticRegression_classifier = SklearnClassifier(LogisticRegression())</span><br><span class="line">LogisticRegression_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"LogisticRegression_classifier accuracy percent:"</span>,(nltk.classify.accuracy(LogisticRegression_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>LogisticRegression_classifier accuracy percent: 82.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SGDClassifier_classifier = SklearnClassifier(SGDClassifier())</span><br><span class="line">SGDClassifier_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"SGDClassifier_classifier accuracy percent:"</span>,(nltk.classify.accuracy(SGDClassifier_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>SGDClassifier_classifier accuracy percent: 82.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SVC_classifier = SklearnClassifier(SVC())</span><br><span class="line">SVC_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"SVC_classifier accuracy percent:"</span>,(nltk.classify.accuracy(SVC_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>SVC_classifier accuracy percent: 82.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LinearSVC_classifier = SklearnClassifier(LinearSVC())</span><br><span class="line">LinearSVC_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"LinearSVC_classifier accuracy percent:"</span>,(nltk.classify.accuracy(LinearSVC_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>LinearSVC_classifier accuracy percent: 80.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">NuSVC_classifier = SklearnClassifier(NuSVC())</span><br><span class="line">NuSVC_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"NuSVC_classifier accuracy percent:"</span>,(nltk.classify.accuracy(NuSVC_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>NuSVC_classifier accuracy percent: 82.0
</code></pre><h3 id="16-Combining-Algos-with-a-Vote"><a href="#16-Combining-Algos-with-a-Vote" class="headerlink" title="16. Combining Algos with a Vote"></a>16. Combining Algos with a Vote</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.classify <span class="keyword">import</span> ClassifierI</span><br><span class="line"><span class="keyword">from</span> statistics <span class="keyword">import</span> mode</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VoteClassifier</span><span class="params">(ClassifierI)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *classifiers)</span>:</span></span><br><span class="line">        self._classifiers = classifiers</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(self, features)</span>:</span></span><br><span class="line">        votes = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> self._classifiers:</span><br><span class="line">            v = c.classify(features)</span><br><span class="line">            votes.append(v)</span><br><span class="line">        <span class="keyword">return</span> mode(votes)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">confidence</span><span class="params">(self,features)</span>:</span></span><br><span class="line">        votes = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> self._classifiers:</span><br><span class="line">            v = c.classify(features)</span><br><span class="line">            votes.append(v)</span><br><span class="line">        </span><br><span class="line">        choice_votes = votes.count(mode(votes))</span><br><span class="line">        conf = choice_votes / len(votes)</span><br><span class="line">        <span class="keyword">return</span> conf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">voted_classifier = VoteClassifier(classifier,</span><br><span class="line">                                  MNB_classifier,</span><br><span class="line">                                  BNB_classifier,</span><br><span class="line">                                  LogisticRegression_classifier,</span><br><span class="line">                                  SGDClassifier_classifier,</span><br><span class="line">                                 <span class="comment">#SVC_classifier,视频中没有这个，况且如果不注释掉就会报统计错误，说有两个相同的值。</span></span><br><span class="line">                                 <span class="comment">#如： http://blog.csdn.net/dongfuguo/article/details/50163757 中 mode错误一般</span></span><br><span class="line">                                 LinearSVC_classifier,</span><br><span class="line">                                 NuSVC_classifier)</span><br><span class="line">print(<span class="string">"voted_classifier accuracy percent:"</span>,(nltk.classify.accuracy(voted_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>voted_classifier accuracy percent: 81.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Classification:"</span>,voted_classifier.classify(testing_set[<span class="number">0</span>][<span class="number">0</span>]),<span class="string">"Confidence %:"</span>,voted_classifier.confidence(testing_set[<span class="number">0</span>][<span class="number">0</span>])*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Classification: neg Confidence %: 100.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Classification:"</span>,voted_classifier.classify(testing_set[<span class="number">1</span>][<span class="number">0</span>]),<span class="string">"Confidence %:"</span>,voted_classifier.confidence(testing_set[<span class="number">1</span>][<span class="number">0</span>])*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Classification: pos Confidence %: 100.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Classification:"</span>,voted_classifier.classify(testing_set[<span class="number">2</span>][<span class="number">0</span>]),<span class="string">"Confidence %:"</span>,voted_classifier.confidence(testing_set[<span class="number">2</span>][<span class="number">0</span>])*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Classification: pos Confidence %: 100.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Classification:"</span>,voted_classifier.classify(testing_set[<span class="number">3</span>][<span class="number">0</span>]),<span class="string">"Confidence %:"</span>,voted_classifier.confidence(testing_set[<span class="number">3</span>][<span class="number">0</span>])*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Classification: neg Confidence %: 87.5
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Classification:"</span>,voted_classifier.classify(testing_set[<span class="number">4</span>][<span class="number">0</span>]),<span class="string">"Confidence %:"</span>,voted_classifier.confidence(testing_set[<span class="number">4</span>][<span class="number">0</span>])*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Classification: pos Confidence %: 100.0
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Classification:"</span>,voted_classifier.classify(testing_set[<span class="number">5</span>][<span class="number">0</span>]),<span class="string">"Confidence %:"</span>,voted_classifier.confidence(testing_set[<span class="number">5</span>][<span class="number">0</span>])*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Classification: neg Confidence %: 75.0
</code></pre><h3 id="17-Investigating-Bias"><a href="#17-Investigating-Bias" class="headerlink" title="17. Investigating Bias()"></a>17. Investigating Bias()</h3><h3 id="18-Better-training-data"><a href="#18-Better-training-data" class="headerlink" title="18. Better training data()"></a>18. Better training data()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">short_pos = open(<span class="string">"short_reviews/positive.txt"</span>,<span class="string">"r"</span>,encoding=<span class="string">"unicode-escape"</span>).read()</span><br><span class="line">short_neg = open(<span class="string">"short_reviews/negative.txt"</span>,<span class="string">"r"</span>,encoding=<span class="string">"unicode-escape"</span>).read()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">short_pos[:<span class="number">300</span>]</span><br></pre></td></tr></table></figure>
<pre><code>&#39;the rock is destined to be the 21st century\&#39;s new &quot; conan &quot; and that he\&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \nthe gorgeously elaborate continuation of &quot; the lord of the rings &quot; trilogy is so huge that a column of words cannot adequ&#39;
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">documents = []</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># map(lambda r : document.append(r,'pos'), [r for r in short_pos.split('\n')])</span></span><br><span class="line"><span class="comment"># 本来想通过类似foreach实现类似的功能，不过好像并不能成功，目前原因还不清楚。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">documents.extend([(r,<span class="string">"pos"</span>) <span class="keyword">for</span> r <span class="keyword">in</span> short_pos.split(<span class="string">'\n'</span>)])</span><br><span class="line"><span class="comment"># 和下面语句的作用是一样的，不过不知道哪个效率更高一些</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> r <span class="keyword">in</span> short_pos.split(<span class="string">'\n'</span>):</span><br><span class="line">    documents.append((r,<span class="string">'pos'</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">documents[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<pre><code>(&#39;the rock is destined to be the 21st century\&#39;s new &quot; conan &quot; and that he\&#39;s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . &#39;,
 &#39;pos&#39;)
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">documents.extend([(r,<span class="string">"neg"</span>) <span class="keyword">for</span> r <span class="keyword">in</span> short_neg.split(<span class="string">'\n'</span>)])</span><br><span class="line"><span class="comment"># 和下面语句的作用是一样的，不过不知道哪个效率更高一些</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> short_neg.split(<span class="string">'\n'</span>):</span><br><span class="line">    documents.append(w.lower())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="comment"># 这里之所以再次导入，仅仅是因为我是几次使用这个notebook，懒得运行前面的cell了。</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">all_words = []</span><br><span class="line">short_pos_words = nltk.word_tokenize(short_pos)</span><br><span class="line">short_neg_words = nltk.word_tokenize(short_neg)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_words.extend([w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> short_pos_words])</span><br><span class="line">all_words.extend([w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> short_neg_words])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以上代码应该也可以写成：</span></span><br><span class="line">all_words = [w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> short_pos_words] + [w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> short_pos_words]</span><br><span class="line"><span class="comment">#甚至是这样：</span></span><br><span class="line">all_words = [w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> short_pos_words+short_neg_words]</span><br><span class="line"><span class="comment"># 不过如果先：</span></span><br><span class="line">all_words = short_pos_words + short_neg_words</span><br><span class="line"><span class="comment"># 再：</span></span><br><span class="line">all_words = [w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> all_words]</span><br><span class="line"><span class="comment"># 可能效率更高一些吧？</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">stopwords = nltk.corpus.stopwords.words(<span class="string">'english'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_words = [w <span class="keyword">for</span> w <span class="keyword">in</span> all_words <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stopwords]</span><br><span class="line"><span class="comment"># 我自己添加的去除停用词等无关信息，以使得特征提取和训练的效率更高</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_words = nltk.FreqDist(all_words)</span><br><span class="line">word_features = list(all_words.keys())[:<span class="number">5000</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_features</span><span class="params">(document)</span>:</span></span><br><span class="line">    words = nltk.word_tokenize(document)</span><br><span class="line">    features = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> word_features:</span><br><span class="line">        features[w] = (w <span class="keyword">in</span> words)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line">featuresets = [(find_features(rev), category) <span class="keyword">for</span> (rev, category) <span class="keyword">in</span> documents]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.shuffle(featuresets)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">training_set = featuresets[:<span class="number">10000</span>]</span><br><span class="line">testing_set = featuresets[<span class="number">10000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">classifier = nltk.NaiveBayesClassifier.train(training_set)</span><br><span class="line">print(<span class="string">"Naive Bayes Algo accuracy:"</span>,(nltk.classify.accuracy(classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Naive Bayes Algo accuracy: 68.82530120481928
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.classify.scikitlearn <span class="keyword">import</span> SklearnClassifier</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB, GaussianNB, BernoulliNB</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">MNB_classifier = SklearnClassifier(MultinomialNB())</span><br><span class="line">MNB_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"MNB_classifier accuracy percent:"</span>,(nltk.classify.accuracy(MNB_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>MNB_classifier accuracy percent: 67.46987951807229
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 这段代码有问题，不可以运行</span></span><br><span class="line">GNB_classifier = SklearnClassifier(GaussianNB())</span><br><span class="line">GNB_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"GNB_classifier accuracy percent:"</span>,(nltk.classify.accuracy(GNB_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">BNB_classifier = SklearnClassifier(BernoulliNB())</span><br><span class="line">BNB_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"BNB_classifier accuracy percent:"</span>,(nltk.classify.accuracy(BNB_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>BNB_classifier accuracy percent: 68.97590361445783
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression, SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC, NuSVC</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LogisticRegression_classifier = SklearnClassifier(LogisticRegression())</span><br><span class="line">LogisticRegression_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"LogisticRegression_classifier accuracy percent:"</span>,(nltk.classify.accuracy(LogisticRegression_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>LogisticRegression_classifier accuracy percent: 70.78313253012048
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SGDClassifier_classifier = SklearnClassifier(SGDClassifier())</span><br><span class="line">SGDClassifier_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"SGDClassifier_classifier accuracy percent:"</span>,(nltk.classify.accuracy(SGDClassifier_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>SGDClassifier_classifier accuracy percent: 66.1144578313253
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">SVC_classifier = SklearnClassifier(SVC())</span><br><span class="line">SVC_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"SVC_classifier accuracy percent:"</span>,(nltk.classify.accuracy(SVC_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>SVC_classifier accuracy percent: 49.096385542168676
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">LinearSVC_classifier = SklearnClassifier(LinearSVC())</span><br><span class="line">LinearSVC_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"LinearSVC_classifier accuracy percent:"</span>,(nltk.classify.accuracy(LinearSVC_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>LinearSVC_classifier accuracy percent: 70.48192771084338
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">NuSVC_classifier = SklearnClassifier(NuSVC())</span><br><span class="line">NuSVC_classifier.train(training_set)</span><br><span class="line">print(<span class="string">"NuSVC_classifier accuracy percent:"</span>,(nltk.classify.accuracy(NuSVC_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>NuSVC_classifier accuracy percent: 69.7289156626506
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> nltk.classify <span class="keyword">import</span> ClassifierI</span><br><span class="line"><span class="keyword">from</span> statistics <span class="keyword">import</span> mode</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VoteClassifier</span><span class="params">(ClassifierI)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *classifiers)</span>:</span></span><br><span class="line">        self._classifiers = classifiers</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(self, features)</span>:</span></span><br><span class="line">        votes = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> self._classifiers:</span><br><span class="line">            v = c.classify(features)</span><br><span class="line">            votes.append(v)</span><br><span class="line">        <span class="keyword">return</span> mode(votes)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">confidence</span><span class="params">(self,features)</span>:</span></span><br><span class="line">        votes = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> self._classifiers:</span><br><span class="line">            v = c.classify(features)</span><br><span class="line">            votes.append(v)</span><br><span class="line">        </span><br><span class="line">        choice_votes = votes.count(mode(votes))</span><br><span class="line">        conf = choice_votes / len(votes)</span><br><span class="line">        <span class="keyword">return</span> conf</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">voted_classifier = VoteClassifier(classifier,</span><br><span class="line">                                  MNB_classifier,</span><br><span class="line">                                  BNB_classifier,</span><br><span class="line">                                  LogisticRegression_classifier,</span><br><span class="line">                                  SGDClassifier_classifier,</span><br><span class="line">                                 <span class="comment">#SVC_classifier,视频中没有这个，况且如果不注释掉就会报统计错误，说有两个相同的值。</span></span><br><span class="line">                                 <span class="comment">#如： http://blog.csdn.net/dongfuguo/article/details/50163757 中 mode错误一般</span></span><br><span class="line">                                 LinearSVC_classifier,</span><br><span class="line">                                 NuSVC_classifier)</span><br><span class="line">print(<span class="string">"voted_classifier accuracy percent:"</span>,(nltk.classify.accuracy(voted_classifier,testing_set))*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>voted_classifier accuracy percent: 69.42771084337349
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"Classification:"</span>,voted_classifier.classify(testing_set[<span class="number">0</span>][<span class="number">0</span>]),<span class="string">"Confidence %:"</span>,voted_classifier.confidence(testing_set[<span class="number">0</span>][<span class="number">0</span>])*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>Classification: pos Confidence %: 100.0
</code></pre><h3 id="19-Sentiment-Analysis-Module"><a href="#19-Sentiment-Analysis-Module" class="headerlink" title="19. Sentiment Analysis Module()"></a>19. Sentiment Analysis Module()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_words = []</span><br><span class="line">documents = []</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">allowed_word_types = [<span class="string">"J"</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> short_pos.split(<span class="string">'\n'</span>):</span><br><span class="line">    documents.append((p,<span class="string">"pos"</span>))</span><br><span class="line">    words = nltk.word_tokenize(p)</span><br><span class="line">    pos = nltk.pos_tag(words)</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> pos:</span><br><span class="line">        <span class="keyword">if</span> w[<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">in</span> allowed_word_types:</span><br><span class="line">            all_words.append(w[<span class="number">0</span>].lower())</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> short_neg.split(<span class="string">'\n'</span>):</span><br><span class="line">    documents.append((p,<span class="string">"neg"</span>))</span><br><span class="line">    words = nltk.word_tokenize(p)</span><br><span class="line">    neg = nltk.pos_tag(words)</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> neg:</span><br><span class="line">        <span class="keyword">if</span> w[<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">in</span> allowed_word_types:</span><br><span class="line">            all_words.append(w[<span class="number">0</span>].lower())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pickle</span><br></pre></td></tr></table></figure>
<p>提醒一下：直接运行以下的cell 会报错，应该先创建一个pickled_algos文件夹，然后再运行cell</p>
<p>保存文档</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_documents = open(<span class="string">'pickled_algos/documents.pickle'</span>,<span class="string">"wb"</span>)</span><br><span class="line">pickle.dump(documents, save_documents)</span><br><span class="line">save_documents.close()</span><br></pre></td></tr></table></figure>
<p>保存文本特征</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">all_words = nltk.FreqDist(all_words)</span><br><span class="line">word_features = list(all_words.keys())[:<span class="number">5000</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_word_features = open(<span class="string">'pickled_algos/word_features5k.pickle'</span>,<span class="string">"wb"</span>)</span><br><span class="line">pickle.dump(word_features,save_word_features)</span><br><span class="line">save_word_features.close()</span><br></pre></td></tr></table></figure>
<p>保存朴素贝叶斯算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_classifier = open(<span class="string">"pickled_algos/originalnaivebayes5k.pickle"</span>,<span class="string">"wb"</span>)</span><br><span class="line">pickle.dump(classifier,save_classifier)</span><br><span class="line">save_classifier.close()</span><br></pre></td></tr></table></figure>
<p>保存MultinomialNB算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_classifier = open(<span class="string">"pickled_algos/MNB_classifier5k.pickle"</span>,<span class="string">"wb"</span>)</span><br><span class="line">pickle.dump(MNB_classifier,save_classifier)</span><br><span class="line">save_classifier.close()</span><br></pre></td></tr></table></figure>
<p>保存BernoulliNB算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_classifier = open(<span class="string">"pickled_algos/BNB_classifier5k.pickle"</span>,<span class="string">"wb"</span>)</span><br><span class="line">pickle.dump(BNB_classifier,save_classifier)</span><br><span class="line">save_classifier.close()</span><br></pre></td></tr></table></figure>
<p>保存LogisticRegression算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_classifier = open(<span class="string">"pickled_algos/LogisticRegression_classifier5k.pickle"</span>,<span class="string">"wb"</span>)</span><br><span class="line">pickle.dump(LogisticRegression_classifier,save_classifier)</span><br><span class="line">save_classifier.close()</span><br></pre></td></tr></table></figure>
<p>保存LinearSVC算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_classifier = open(<span class="string">"pickled_algos/LinearSVC_classifier5k.pickle"</span>,<span class="string">"wb"</span>)</span><br><span class="line">pickle.dump(LinearSVC_classifier,save_classifier)</span><br><span class="line">save_classifier.close()</span><br></pre></td></tr></table></figure>
<p>保存SGDC算法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">save_classifier = open(<span class="string">"pickled_algos/SGDClassifier_classifier5k.pickle"</span>,<span class="string">"wb"</span>)</span><br><span class="line">pickle.dump(SGDClassifier_classifier,save_classifier)</span><br><span class="line">save_classifier.close()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">voted_classifier = VoteClassifier(classifier,</span><br><span class="line">                                 LinearSVC_classifier,</span><br><span class="line">                                 MNB_classifier,</span><br><span class="line">                                 BNB_classifier,</span><br><span class="line">                                 LogisticRegression_classifier)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentiment</span><span class="params">(text)</span>:</span></span><br><span class="line">    feats = find_features(text)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> voted_classifier.classify(feats)</span><br></pre></td></tr></table></figure>
<p>最终我们编写的模块长成这个样子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#File: sentiment_mod.py 只是一个文件名而已，可以按照自己的想法取，但应做到见名知意</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> nltk.classify.scikitlearn <span class="keyword">import</span> SklearnClassifier</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB,BernoulliNB</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression,SGDClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC,NuSVC</span><br><span class="line"><span class="keyword">from</span> nltk.classify <span class="keyword">import</span> ClassifierI</span><br><span class="line"><span class="keyword">from</span> statistics <span class="keyword">import</span> mode</span><br><span class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> word_tokenize</span><br><span class="line"></span><br><span class="line"><span class="comment"># 以上许多类模块虽然在代码中看似并没有用到，可是在用pickle还原为相关实例在被外部调用执行的时候还是需要的。</span></span><br><span class="line"><span class="comment"># 这里由于我们之前已经训练好了几个分类器，并且已经将文档内容和文本特征等通过pickle持久化保存起来了，所以在此模块中直接用pickle还原就可以直接拿来用了，而不是再次训练。</span></span><br><span class="line"><span class="comment"># 并且该模块仅当同一路径下的pickled_algos文件夹及里面的各pickle文件同时存在时才可以正常使用，当然，项目中也要导入本模块需要使用的一些基础模块，如nltk等等。</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">VoteClassifier</span><span class="params">(ClassifierI)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, *classifiers)</span>:</span></span><br><span class="line">        self._classifiers = classifiers</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">classify</span><span class="params">(self, features)</span>:</span></span><br><span class="line">        votes = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> self._classifiers:</span><br><span class="line">            v = c.classify(features)</span><br><span class="line">            votes.append(v)</span><br><span class="line">        <span class="keyword">return</span> mode(votes)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">confidence</span><span class="params">(self,features)</span>:</span></span><br><span class="line">        votes = []</span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> self._classifiers:</span><br><span class="line">            v = c.classify(features)</span><br><span class="line">            votes.append(v)</span><br><span class="line">        </span><br><span class="line">        choice_votes = votes.count(mode(votes))</span><br><span class="line">        conf = choice_votes / len(votes)</span><br><span class="line">        <span class="keyword">return</span> conf </span><br><span class="line"></span><br><span class="line">documents_f = open(<span class="string">'pickled_algos/documents.pickle'</span>,<span class="string">"rb"</span>)</span><br><span class="line">documents = pickle.load(documents_f)</span><br><span class="line">documents_f.close()</span><br><span class="line"></span><br><span class="line">word_features5k_f = open(<span class="string">'pickled_algos/word_features5k.pickle'</span>,<span class="string">"rb"</span>)</span><br><span class="line">word_features = pickle.load(word_features5k_f)</span><br><span class="line">word_features5k_f.close()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_features</span><span class="params">(document)</span>:</span></span><br><span class="line">    words = nltk.word_tokenize(document)</span><br><span class="line">    features = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> word_features:</span><br><span class="line">        features[w] = (w <span class="keyword">in</span> words)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> features</span><br><span class="line"></span><br><span class="line">open_file = open(<span class="string">"pickled_algos/originalnaivebayes5k.pickle"</span>,<span class="string">"rb"</span>)</span><br><span class="line">classifier = pickle.load(open_file)</span><br><span class="line">open_file.close()</span><br><span class="line"></span><br><span class="line">open_file = open(<span class="string">"pickled_algos/MNB_classifier5k.pickle"</span>,<span class="string">"rb"</span>)</span><br><span class="line">MNB_classifier = pickle.load(open_file)</span><br><span class="line">open_file.close()</span><br><span class="line"></span><br><span class="line">open_file = open(<span class="string">"pickled_algos/BNB_classifier5k.pickle"</span>,<span class="string">"rb"</span>)</span><br><span class="line">BNB_classifier = pickle.load(open_file)</span><br><span class="line">open_file.close()</span><br><span class="line"></span><br><span class="line">open_file = open(<span class="string">"pickled_algos/LogisticRegression_classifier5k.pickle"</span>,<span class="string">"rb"</span>)</span><br><span class="line">LogisticRegression_classifier = pickle.load(open_file)</span><br><span class="line">open_file.close()</span><br><span class="line"></span><br><span class="line">open_file = open(<span class="string">"pickled_algos/LinearSVC_classifier5k.pickle"</span>,<span class="string">"rb"</span>)</span><br><span class="line">LinearSVC_classifier = pickle.load(open_file)</span><br><span class="line">open_file.close()</span><br><span class="line"></span><br><span class="line">open_file = open(<span class="string">"pickled_algos/SGDClassifier_classifier5k.pickle"</span>,<span class="string">"rb"</span>)</span><br><span class="line">SGDClassifier_classifier = pickle.load(open_file)</span><br><span class="line">open_file.close()</span><br><span class="line"></span><br><span class="line">voted_classifier = VoteClassifier(</span><br><span class="line">                                 classifier,</span><br><span class="line">                                 LinearSVC_classifier,</span><br><span class="line">                                 MNB_classifier,</span><br><span class="line">                                 BNB_classifier,</span><br><span class="line">                                 LogisticRegression_classifier)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sentiment</span><span class="params">(text)</span>:</span></span><br><span class="line">    feats = find_features(text)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> voted_classifier.classify(feats),voted_classifier.confidence(feats)</span><br><span class="line"></span><br><span class="line"><span class="comment"># save me as sentiment_mod.py</span></span><br></pre></td></tr></table></figure>
<p>下面来使用一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sentiment_mod <span class="keyword">as</span> s</span><br><span class="line"></span><br><span class="line">print(s.sentiment(<span class="string">"This movie was awesome! The acting was great, plot was wonderful, and there were pythons...so yea!"</span>))</span><br><span class="line"></span><br><span class="line">print(s.sentiment(<span class="string">"This movie was utter junk. There were absolutely 0 pythons. I don't see what the point was at all. Horrible movie, 0/10"</span>))</span><br></pre></td></tr></table></figure>
<pre><code>(&#39;pos&#39;, 1.0)
(&#39;neg&#39;, 1.0)
</code></pre><p>好吧，接下来的实践要使用Twitter 创建APP，可能还要使用个人网站，有点麻烦，所以接下来我只是看了看并没有照着实践。<br>总之，在这一系列的跟着敲代码的过程中，自己初步建立起了很浅的自然语言处理的概念~</p>
<h3 id="20-Twitter-Sentiment-Analysis"><a href="#20-Twitter-Sentiment-Analysis" class="headerlink" title="20. Twitter Sentiment Analysis()"></a>20. Twitter Sentiment Analysis()</h3><h3 id="21-Graphing-Live-Twitter-Sentiment"><a href="#21-Graphing-Live-Twitter-Sentiment" class="headerlink" title="21. Graphing Live Twitter Sentiment()"></a>21. Graphing Live Twitter Sentiment()</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/Python/"><i class="fas fa-hashtag fa-fw"></i>Python</a>
                
                    <a href="/tags/自然语言处理/"><i class="fas fa-hashtag fa-fw"></i>自然语言处理</a>
                
                    <a href="/tags/NLTK/"><i class="fas fa-hashtag fa-fw"></i>NLTK</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class='post-wrapper'>
          <article class="post reveal ">
    
<section class='meta'>
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/03/06/英文表达与写作练习宣告/">
              
                  英文表达与写作练习宣告
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-03-06
      </time>
    

    
      

    

    

    

  </div>
</section>

    <section class="article typo">
        <p><code>汉语版</code>:为了更好的提高自己的英文表达和写作能力，我决定在以后的技术博客中尽可能的使用英文。仅在此声明！</p>
<p><code>我的英文版</code>:In order to improve my English express and writing ablility,I decide that I’ll use English to write my tecnoligy blogs as possible.The statesment is above.</p>
<p><code>有道翻译英语版</code>:In order to improve my English expression and writing ability, I decided to use English as much as possible in future technical blogs.<br>Only in this statement!</p>
<p><code>学习笔记</code>:</p>
<ul>
<li>expression :n,表达</li>
<li>alility:n,能力</li>
<li>decided to do something:决定做某事</li>
<li>as much as possible:尽可能多的</li>
<li>technical:adj,技术的</li>
</ul>

        

        
    </section>
</article>

        </div>
      
    
      
        <div class='post-wrapper'>
          <article class="post reveal ">
    
<section class='meta'>
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/03/05/常见10种自然语言处理技术（转载）/">
              
                  常见10种自然语言处理技术（转载）
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-03-05
      </time>
    

    
      
    
    <div class='metatag cats'>
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/自然语言处理/">自然语言处理</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <p><a href="https://www.felayman.com/articles/2018/02/28/1519818174444.html" title="点击查看原文" target="_blank" rel="noopener">原文</a></p>
<p><a href="https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/" target="_blank" rel="noopener">该作者也是翻译的外文，英文原文链接</a></p>
<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>自然语言处理（NLP）是一种艺术与科学的结合，旨在从文本数据中提取信息。在它的帮助下，我们从文本中提炼出适用于计算机算法的信息。从自动翻译、文本分类到情绪分析，自然语言处理成为所有数据科学家的必备技能之一。</p>
<p>常见的10个NLP任务如下：</p>
<ol>
<li><code>词干提取</code></li>
<li><code>词形还原</code></li>
<li><code>词向量化</code></li>
<li><code>词性标注</code></li>
<li><code>命名实体消岐</code></li>
<li><code>命名实体识别</code></li>
<li><code>情感分析</code></li>
<li><code>文本语义相似分析</code></li>
<li><code>语种辨识</code></li>
<li><code>文本总结</code></li>
</ol>
<h3 id="以下将详细展开："><a href="#以下将详细展开：" class="headerlink" title="以下将详细展开："></a>以下将详细展开：</h3><h1 id="1-词干提取"><a href="#1-词干提取" class="headerlink" title="1.词干提取"></a>1.词干提取</h1><p>什么是词干提取？词干提取是将词语去除变化或衍生形式，转换为词干或原型形式的过程。词干提取的目标是将相关词语还原为同样的词干，哪怕词干并非词典的词目。例如，英文中:</p>
<ol>
<li>beautiful和beautifully的词干同为beauti</li>
<li>Good,better和best 的词干分别为good,better和best。</li>
</ol>
<p>相关论文：<a href="https://tartarus.org/martin/PorterStemmer/def.txt" target="_blank" rel="noopener">Martin Porter的波特词干算法原文</a></p>
<p>相关算法：<a href="https://bitbucket.org/mchaput/stemming/src/5c242aa592a6d4f0e9a0b2e1afdca4fd757b8e8a/stemming/porter2.py?at=default&amp;fileviewer=file-view-default" target="_blank" rel="noopener">Porter2词干算法的Python实现</a></p>
<p>程序实现：Porter2算法做词干提取的代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!pip install stemming</span></span><br><span class="line"><span class="keyword">from</span> stemming.porter2 <span class="keyword">import</span> stem</span><br><span class="line">stem(<span class="string">"casually"</span>)</span><br></pre></td></tr></table></figure>
<h1 id="2-词形还原"><a href="#2-词形还原" class="headerlink" title="2. 词形还原"></a>2. 词形还原</h1><p>什么是词形还原？ 词形还原是将一组词语还原为词源或词典的词目形式的过程。还原过程考虑到了POS问题，即词语在句中的语义，词语对相邻语句的语义等。例如，英语中：</p>
<ol>
<li>beautiful和beautifully被分别还原为beautiful和beautifully。</li>
<li>good, better和best被分别还原为good, good和good</li>
</ol>
<p>相关论文1: <a href="http://www.ijrat.org/downloads/icatest2015/ICATEST-2015127.pdf" target="_blank" rel="noopener">这篇文章详细讨论了词形还原的不同方法。想要了解传统词形还原的工作原理必读。</a></p>
<p>相关论文2: <a href="https://academic.oup.com/dsh/article-abstract/doi/10.1093/llc/fqw034/2669790/Lemmatization-for-variation-rich-languages-using" target="_blank" rel="noopener">这篇论文非常出色，讨论了运用深度学习对变化丰富的语种做词形还原时会遇到的问题。</a></p>
<p>数据集: <a href="https://catalog.ldc.upenn.edu/ldc99t42" target="_blank" rel="noopener">这里是Treebank-3数据集的链接，你可以使用它创建一个自己的词形还原工具。</a></p>
<p>程序实现：下面给出了在spacy上的英语词形还原代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!pip install spacy</span></span><br><span class="line"><span class="comment">#python -m spacy download en</span></span><br><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp=spacy.load(<span class="string">"en"</span>)</span><br><span class="line">doc=<span class="string">"good better best"</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> nlp(doc):</span><br><span class="line">    print(token,token.lemma_)</span><br></pre></td></tr></table></figure>
<h1 id="3-词向量化"><a href="#3-词向量化" class="headerlink" title="3. 词向量化"></a>3. 词向量化</h1><p>什么是词向量化？词向量化是用一组实数构成的向量代表自然语言的叫法。这种技术非常实用，因为电脑无法处理自然语言。词向量化可以捕捉到自然语言和实数间的本质关系。通过词向量化，一个词语或者一段短语可以用一个定维的向量表示，例如向量的长度可以为100。</p>
<p>例如：<code>Man</code>这个词语可以用一个五维向量表示。</p>
<p><img src="https://github.com/smilelight/images/raw/master/%E5%B8%B8%E8%A7%8110%E7%A7%8D%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/word-vector.png" alt="Man"></p>
<p>这里的每个数字代表了词语在某个特定方向上的量级。<br><img src="https://github.com/smilelight/images/raw/master/%E5%B8%B8%E8%A7%8110%E7%A7%8D%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/Word-Vectors.png" alt="word-vectors"></p>
<p>相关博文：<a href="https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/" target="_blank" rel="noopener">这篇文章详细解释了词向量化</a></p>
<p>相关论文：<a href="https://www.analyticsvidhya.com/blog/2017/10/essential-nlp-guide-data-scientists-top-10-nlp-tasks/" target="_blank" rel="noopener">这篇论文解释了词向量化的细节。深入理解词向量化必读。</a></p>
<p>相关工具：<a href="https://ronxin.github.io/wevi/" target="_blank" rel="noopener">这是个基于浏览器的词向量可视化工具。</a></p>
<p>预训练词向量：<a href="https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md?spm=a2c4e.11153959.blogcont236723.10.1a815c301CEF2o&amp;file=pretrained-vectors.md" target="_blank" rel="noopener">这里有一份facebook的预训练词向量列表，包含294种语言。</a></p>
<p><a href="https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit" target="_blank" rel="noopener">这里可以下载google news的预训练词向量。</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!pip install gensim</span></span><br><span class="line"><span class="keyword">from</span> gensim.models.keyedvectors <span class="keyword">import</span> KeyedVectors</span><br><span class="line">word_vectors=KeyedVectors.load_word2vec_format(<span class="string">'GoogleNews-vectors-negative300.bin'</span>,binary=<span class="keyword">True</span>)</span><br><span class="line">word_vectors[<span class="string">'human'</span>]</span><br></pre></td></tr></table></figure>
<p>程序实现：这段代码可以用gensim训练你自己的词向量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sentence=[[<span class="string">'first'</span>,<span class="string">'sentence'</span>],[<span class="string">'second'</span>,<span class="string">'sentence'</span>]]</span><br><span class="line">model = gensim.models.Word2Vec(sentence, min_count=<span class="number">1</span>,size=<span class="number">300</span>,workers=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>
<h1 id="4-词性标注"><a href="#4-词性标注" class="headerlink" title="4.词性标注"></a>4.词性标注</h1><p>什么事词性标注？简单来说，词性标注是对句子中的词语标注为名字、动词、形容词、副词等的过程。例如，对句子“Ashok killed the snake with a stick”，词性标注会识别：</p>
<ul>
<li>Ashok 代词</li>
<li>killed 动词</li>
<li>the 限定词</li>
<li>snake 名词</li>
<li>with 连词</li>
<li>a 限定词</li>
<li>stick 名词</li>
<li>. 标点</li>
</ul>
<p>论文1：<a href="https://aclweb.org/anthology/N16-1031.pdf" target="_blank" rel="noopener">choi aptly的这篇《The Last Gist to theState-of-the-Art 》介绍了一种叫动态特征归纳的新方法。这是目前词性标注最先进的方法。</a></p>
<p>论文2：<a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/837/192" target="_blank" rel="noopener">这篇文章介绍了通过隐马尔科夫模型做无监督词性标注学习的方法。</a></p>
<p>程序实现：这段代码可以在spacy上做词性标注</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!pip install spacy</span></span><br><span class="line"><span class="comment">#!python -m spacy download en </span></span><br><span class="line">nlp=spacy.load(<span class="string">'en'</span>)</span><br><span class="line">sentence=<span class="string">"Ashok killed the snake with a stick"</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> nlp(sentence):</span><br><span class="line">   print(token,token.pos_)</span><br></pre></td></tr></table></figure>
<h1 id="5-命名实体消歧"><a href="#5-命名实体消歧" class="headerlink" title="5. 命名实体消歧"></a>5. 命名实体消歧</h1><p>什么是命名实体消岐？命名实体消岐是对句子中的提到的实体识别的过程。例如，对句子“Apple earned a revenue of 200 Billion USD in 2016”，命名实体消岐会推断出句子中的Apple是苹果公司而不是指一种水果。一般来说，命名实体要求有一个实体知识库，能够将句子中提到的实体和知识库联系起来。</p>
<p>论文1：<a href="https://arxiv.org/pdf/1504.07678.pdf" target="_blank" rel="noopener">Huang的这篇论文运用了基于深度神经网络和知识库的深层语义关联模型，在命名实体消岐上达到了领先水平。</a></p>
<p>论文2：<a href="https://arxiv.org/pdf/1704.04920.pdf" target="_blank" rel="noopener">Ganea and Hofmann的这篇文章运用了局部神经关注模型和词向量化，没有人为设置特征。</a></p>
<h1 id="6-命名实体识别"><a href="#6-命名实体识别" class="headerlink" title="6. 命名实体识别"></a>6. 命名实体识别</h1><p>体识别是识别一个句子中有特定意义的实体并将其区分为人名，机构名，日期，地名，时间等类别的任务。例如，一个NER会将一个这样的句子：</p>
<blockquote>
<p>“Ram of Apple Inc. travelled to Sydney on 5th October 2017”</p>
</blockquote>
<p>返回如下的结果：</p>
<blockquote>
<p>Ram<br>of<br>Apple ORG<br>Inc. ORG<br>travelled<br>to<br>Sydney GPE<br>on<br>5th DATE<br>October DATE<br>2017 DATE</p>
</blockquote>
<p>这里，ORG代表机构组织名，GPE代表地名。</p>
<p>然而，当NER被用在不同于该NER被训练的数据领域时，即使是最先进的NER也往往表现不佳。</p>
<p><img src="https://github.com/smilelight/images/raw/master/%E5%B8%B8%E8%A7%8110%E7%A7%8D%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E6%8A%80%E6%9C%AF/ner.png" alt="ner"></p>
<p>论文：<a href="https://arxiv.org/pdf/1603.01360.pdf" target="_blank" rel="noopener">这篇优秀的论文使用双向LSTM（长短期记忆网络）神经网络结合监督学习和非监督学习方法，在4种语言领域实现了命名实体识别的最新成果。</a></p>
<p>程序实现：以下使用spacy执行命名实体识别。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp=spacy.load(<span class="string">'en'</span>)sentence=<span class="string">"Ram of Apple Inc. travelled to Sydney on 5th October 2017"</span></span><br><span class="line"><span class="keyword">for</span> token <span class="keyword">in</span> nlp(sentence):</span><br><span class="line">   print(token, token.ent_type_)</span><br></pre></td></tr></table></figure>
<h1 id="7-情感分析"><a href="#7-情感分析" class="headerlink" title="7. 情感分析"></a>7. 情感分析</h1><p>什么是情感分析？情感分析是一种广泛的主观分析，它使用自然语言处理技术来识别客户评论的语义情感，语句表达的情绪正负面以及通过语音分析或书面文字判断其表达的情感等等。例如：</p>
<p>“我不喜欢巧克力冰淇淋”—是对该冰淇淋的负面评价。</p>
<p>“我并不讨厌巧克力冰激凌”—可以被认为是一种中性的评价。</p>
<p>从使用LSTMs和Word嵌入来计算一个句子中的正负词数开始，有很多方法都可以用来进行情感分析。</p>
<p>博文1：<a href="https://www.analyticsvidhya.com/blog/2016/02/step-step-guide-building-sentiment-analysis-model-graphlab/" target="_blank" rel="noopener">本文重点对电影推文进行情感分析。</a></p>
<p>博文2：<a href="https://www.analyticsvidhya.com/blog/2017/01/sentiment-analysis-of-twitter-posts-on-chennai-floods-using-python/" target="_blank" rel="noopener">本文重点对印度金奈洪水期间的推文进行情感分析。</a></p>
<p>论文1：<a href="https://arxiv.org/pdf/1305.6143.pdf" target="_blank" rel="noopener">本文采用朴素贝叶斯的监督学习方法对IMDB评论进行分类。</a></p>
<p>论文2：<a href="http://www.cs.cmu.edu/~yohanj/research/papers/WSDM11.pdf" target="_blank" rel="noopener">本文利用LDA的无监督学习方法来识别用户生成评论的观点和情感。本文在解决注释评论短缺的问题上表现突出。</a></p>
<p>资料库：<a href="https://github.com/xiamx/awesome-sentiment-analysis" target="_blank" rel="noopener">这是一个很好的包含相关研究论文和各种语言情感分析程序实现的资料库。</a></p>
<p>数据集1<a href="http://www.cs.jhu.edu/~mdredze/datasets/sentiment/" target="_blank" rel="noopener">：多域情感数据集版本2.0</a></p>
<p>数据集2：<a href="http://www.sananalytics.com/lab/twitter-sentiment/" target="_blank" rel="noopener">Twitter情感分析数据集</a></p>
<p>竞赛：<a href="https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews" target="_blank" rel="noopener">一个非常好的比赛，你可以检查你的模型在烂番茄电影评论的情感分析任务中的表现。</a></p>
<h1 id="8-语义文本相似度"><a href="#8-语义文本相似度" class="headerlink" title="8. 语义文本相似度"></a>8. 语义文本相似度</h1><p>什么是语义文本相似度分析？语义文本相似度分析是对两段文本的意义和本质之间的相似度进行分析的过程。注意，相似性与相关性是不同的。</p>
<p>例如：</p>
<blockquote>
<p>汽车和公共汽车是相似的，但是汽车和燃料是相关的。</p>
</blockquote>
<p>论文1：<a href="https://pdfs.semanticscholar.org/5b5c/a878c534aee3882a038ef9e82f46e102131b.pdf" target="_blank" rel="noopener">本文详细介绍了文本相似度测量的不同方法。是一篇可以一站式了解目前所有方法的必读文章。</a></p>
<p>论文2：<a href="http://casa.disi.unitn.it/~moschitt/since2013/2015_SIGIR_Severyn_LearningRankShort.pdf" target="_blank" rel="noopener">本文介绍了用CNN神经网络去比对两个短文本。</a></p>
<p>论文3：<a href="https://nlp.stanford.edu/pubs/tai-socher-manning-acl2015.pdf" target="_blank" rel="noopener">本文利用Tree-LSTMs方法得到了文本的语义相关和语义分类的最新成果。</a></p>
<h1 id="9-语言识别"><a href="#9-语言识别" class="headerlink" title="9. 语言识别"></a>9. 语言识别</h1><p>什么是语言识别？语言识别指的是将不同语言的文本区分出来。其利用语言的统计和语法属性来执行此任务。语言识别也可以被认为是文本分类的特殊情况。</p>
<p>博文：<a href="https://fasttext.cc/blog/2017/10/02/blog-post.html" target="_blank" rel="noopener">在这篇由fastText撰写的博文中介绍了一种新的工具，其可以在1MB的内存使用情况下识别170种语言。</a></p>
<p>论文1：<a href="http://www.ep.liu.se/ecp/131/021/ecp17131021.pdf" target="_blank" rel="noopener">本文讨论了285种语言的7种语言识别方法。</a></p>
<p>论文2：<a href="https://repositorio.uam.es/bitstream/handle/10486/666848/automatic_lopez-moreno_ICASSP_2014_ps.pdf?sequence=1" target="_blank" rel="noopener">本文描述了如何使用深度神经网络来实现自动语言识别的最新成果。</a></p>
<h1 id="10-文本摘要"><a href="#10-文本摘要" class="headerlink" title="10. 文本摘要"></a>10. 文本摘要</h1><p>什么是文本摘要？文本摘要是通过识别文本的重点并使用这些要点创建摘要来缩短文本的过程。文本摘要的目的是在不改变文本含义的前提下最大限度地缩短文本。</p>
<p>论文1：<a href="https://arxiv.org/pdf/1509.00685.pdf" target="_blank" rel="noopener">本文描述了基于神经注意模型的抽象语句梗概方法。</a></p>
<p>论文2：<a href="https://arxiv.org/pdf/1602.06023.pdf" target="_blank" rel="noopener">本文描述了使用序列到序列的RNN在文本摘要中达到的最新结果。</a></p>
<p>资料库：<a href="https://github.com/tensorflow/models/tree/master/research/textsum" target="_blank" rel="noopener">Google Brain团队的这个资料库拥有使用为文本摘要定制的序列到序列模型的代码。该模型在Gigaword数据集上进行训练。</a></p>
<p>应用程序：<a href="https://www.reddit.com/r/autotldr/comments/31b9fm/faq_autotldr_bot/" target="_blank" rel="noopener">Reddit的autotldr机器人使用文本摘要来梗概从文章到帖子的各种评论。这个功能在Reddit用户中非常有名。</a></p>
<p>程序实现：以下是如何用gensim包快速实现文本摘要。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fromgensim.summarization <span class="keyword">import</span> summarize</span><br><span class="line">sentence=<span class="string">"Automatic summarization is the process of shortening a text document with software, in order to create a summary with the major points of the original document. Technologies that can make a coherent summary take into account variables such as length, writing style and syntax.Automatic data summarization is part of machine learning and data mining. The main idea of summarization is to find a subset of data which contains the information of the entire set. Such techniques are widely used in industry today. Search engines are an example; others include summarization of documents, image collections and videos. Document summarization tries to create a representative summary or abstract of the entire document, by finding the most informative sentences, while in image summarization the system finds the most representative and important (i.e. salient) images. For surveillance videos, one might want to extract the important events from the uneventful context.There are two general approaches to automatic summarization: extraction and abstraction. Extractive methods work by selecting a subset of existing words, phrases, or sentences in the original text to form the summary. In contrast, abstractive methods build an internal semantic representation and then use natural language generation techniques to create a summary that is closer to what a human might express. Such a summary might include verbal innovations. Research to date has focused primarily on extractive methods, which are appropriate for image collection summarization and video summarization."</span></span><br><span class="line">summarize(sentence)</span><br></pre></td></tr></table></figure>
<h1 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h1><p>以上所有是最流行的NLP任务以及相关的博客、研究论文、资料库、应用等资源。</p>
<p>祝你学习愉快！</p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/自然语言处理/"><i class="fas fa-hashtag fa-fw"></i>自然语言处理</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class='post-wrapper'>
          <article class="post reveal ">
    
<section class='meta'>
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/03/05/朕的感情史-编程语言篇！/">
              
                  朕的感情史-编程语言篇！
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-03-05
      </time>
    

    
      

    

    

    

  </div>
</section>

    <section class="article typo">
        <p>对于每一个有理想有追求的程序猿而言，他们可以没有对象，但是不能没有自己的喜欢的语言。</p>
<p>古言道：兄弟诚可贵，老婆价更高。若为编程故（指死亡之die~），躲也躲不掉。</p>
<p>好吧，接下来我要正式介绍一下朕的后宫！</p>
<p><code>第一任</code>：<code>C++</code></p>
<p>大一学习编程语言最开始上手的就是C++，不过学了半天也没学出个所以然来，经常挣扎于混杂的概念之中，却又缺少电脑无法有效开展实战编程。后来随着考试结束，我和她的感情也就慢慢的越来越淡了。</p>
<p>好吧，不过呢，作为系统编程和游戏开发的无可替代的语言，她的高性能与开发效率综合而言目前怕是所有所有的都比不上。</p>
<p>大部分企业面试笔试好像都会问到C++，感觉不会点C++真的不配说自己是程序员，但是我觉得人生苦短，何必找死呢。感觉C++是我等屁民不可轻易染指的高冷女神。</p>
<p><code>第二任</code>：<code>C</code></p>
<p>为了更好的学习C++，曾经又跑去学了C，因为我觉得她是C++的姐姐吧。不过还是由于驱动力不足之类的原因吧，最终也没能学出个卵子，不过也在某种程度上理解了C++相对于C而言做的改进，例如多态、名称空间、类等对于结构化的编程还是挺有用的吧。</p>
<p>C语言也可以做的很吊，但是由于语言的设计里本身并没有许多高级概念，所以用起来不是很直接方便，搞系统编程或者其他对效率要求极高的才会用吧。看上去没有C++的高冷，但是也不好追，并且语言特性相对低级，自己本身不太喜欢。</p>
<p><code>第三任</code>：<code>Java</code></p>
<p>Java这门语言其实真的是相当不错的。如所有的类都有一个共同的基类，只能继承自一个父类，其他的通过接口来实现，相对于C++的名称空间来说要好不少。</p>
<p>Java的语言世界里，程序总是在一个个实例化的类或类本身中的属性和方法中穿梭，某种程度上来说其实挺好的。相比C++而言，普通的人很容易上手并能培养起初步良好的编程习惯。</p>
<p>我在Java实验课程中手撸一个什么系统，在此过程中，初步培养起了软件编程的极初始的经验体会。后来学习Android过程中，由于也要用到Java，所以功底还算可以。</p>
<p>但是在定义和使用的过程中，还是会感觉到比较冗杂，当然规矩多了某种程度上也算好事。那些之后在JVM之上出现的其他语言，也是同样在JVM上运行程序，但是相对而言可能更简洁、更少束缚、更强表现力，如Scala、Kotlin等等。</p>
<p>曾经问我家邻居学软件怎么入手，他说你就学Java吧。Java语言在企业级应用系统开发中的主导地位怕是很难被动摇的。</p>
<p>无论什么时候，Java都是一个值得选择拥抱的小姐姐。</p>
<p><code>第四任</code>：<code>C#</code></p>
<p>C#开发最初的目的可能是微软用于应对Java吧。</p>
<p>C#这门语言在面向对象的语言家族中还是很不错的，某些设计理念比Java还要先进。然而正如以前的VBScript等更多的是被捆绑在Windows操作系统中，不开源，在某种程度上也算束缚了它的发展进步。虽说现在开源了，但是由于Java已经占领了大部分市场，C#的份额还是挺小的了。</p>
<p>因课结缘，课完缘尽。<br>定位和Java类似，但是对Java更有好感。</p>
<p><code>第五任</code>：<code>Lua</code></p>
<p>不得不说，Lua真的是个好语言，只是之前没怎么好好学，并且它上手极为简单容易。现在的话，很少接触，因为基本上用不到。对于它的C源码还是有必要看一下的。感觉很有好感的小妹妹。</p>
<p><code>第六任</code>：<code>Python</code></p>
<p>Python的设计哲学现在看来真的是极好，这些年的发展壮大足以说明一切，在大部分编程应用领域里几乎都有了它的身影。极强的代码阅读性，上手很容易。于我而言，她现在是一个贤妻良母型的。</p>
<p><code>第七任</code>：<code>Ruby</code></p>
<p>Python和Ruby各领风骚，许多爱好者各爱各的好。Ruby我以前也学过一下下，Ruby好像是基于对象的，里面啥都是对象。相对而言，用法超级灵活，而且比较简洁炫酷，但是可能因为用的少，我看着Ruby代码好像阅读起来不是很爽，比Python而言差远了。开发效率极高，Ruby on Rails嘛，但是好像项目做大之后就会受限于它的运行效率。现在也早就分了。感觉Ruby和我三观不符，私人感觉Ruby是一个比较追求时尚新颖的女生，比较开放外向。</p>
<p><code>第八任</code>：<code>Scala</code></p>
<p>我曾经学习过很短时间的Scala，网上的说法是学习起来很复杂，我当时感觉还不错，后来因为用不到也没在学了，可能还是不太感兴趣吧。也是一个不错的小姐姐，但是如果太专情于此，怕是不好吃饭。。。</p>
<p><code>第九任</code>：<code>aardio</code></p>
<p>最爱，没有之一。虽然它生于Windows，长于Windows，最终也将死于Windows，但是这还是无法阻挠我对它的喜爱，况且正是由于仅限于Windows平台，她的美才更加表现出来。她的不开源我也理解，是我我也不开源，正是一鹤校长非常爱她，才不允许其他误解开源精神的染指搞事情。</p>
<p>动态语言，快速开发，名称空间、类，句法灵活，调用简单，几行代码就可以实现复杂的功能，并且很方便的就可以开发出控制台、桌面、网络、后台程序，可以方便与其他语言实现互相调用。</p>
<p>自从无意间了解到这门语言之后，感觉一下子就被吸引到了，因为她当时完全符合我对编程语言的所有需求，他妈的当时把老纸激动坏了，差点一宿没睡着。当时学了编程两年多，都写不出个什么桌面程序来，用Java写的太丑，用C#开发的还得运行在.Net框架上。一点成就感都没有，我对可视化的界面还是很执着的。</p>
<p>后来花钱报了班，看视频学习，有时会在群里探讨问题，有时会直接向校长询问，感觉真的很值。后来我数据库实验、编译原理实验、操作系统实验等等全都是用的aardio语言，因为用起来实在是太爽了。</p>
<p>于我而言，aardio是我的初恋，是我第一门喜欢上的语言，虽然现在由于没有需求已经暂停和她失去了联系，但是我会永远喜欢她！</p>
<p><code>第十任</code>：<code>Processing</code></p>
<p>Processing这门语言用来做艺术设计以及自然模拟都是很不错的，并且跨平台，可以生成可执行程序，本身使用Java语言写的，同时也有JavaScript版和Python版，也是用不到所以没在用了。在我看来，像是一个很可爱的萝莉。</p>
<p><code>第十一任</code>：<code>JavaScript</code></p>
<p>个人感觉，JavaScript最屌了。它的语言哲学相对来说和我的三观最为符合，原型继承，闭包等等概念都很不错。随着es6的出现，它的语法规范感觉十分高级，个人十分喜欢，在某种程度上，部分设计已经超越了aardio（感觉aardio是集许多好的语言的特性于一身）。事件驱动、非阻塞I/O、回调的NodeJs感觉也很棒。是一个我非常想深入学习的语言，对我来说是一个极具诱惑力的女神。</p>
<p><code>第十二任</code>：<code>Racket</code></p>
<p>Racket是Scheme的一种方言，而Scheme则是Lisp的一种方言，语言中处处都是函数式编程，感觉非常新颖且高大上，但是程序复杂起来了之后阅读体验极差，远远比不上Python。接下来打算有时间接触一下下，多吸取里面的先进思想。</p>
<p><code>第十三任</code>：<code>Julia</code></p>
<p>2018/3/5刚接触Julia语言，感觉真的挺厉害的，元编程，并行计算，超高性能，语法像Python、用法像Lisp、速度像C，渍渍，真是厉害了~</p>
<p><code>第十四任</code>：<code>？</code></p>
<p><code>第十五任</code>：<code>？</code></p>
<p><code>第十六任</code>：<code>？</code></p>
<p><code>第十七任</code>：<code>？</code></p>
<p>……</p>
<p>接下来的语言还没出现或者还没接触到，可能今后才会冒出来，或者是我自己设计定义实现的吧……</p>

        

        
    </section>
</article>

        </div>
      
    
      
        <div class='post-wrapper'>
          <article class="post reveal ">
    
<section class='meta'>
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/03/04/Python学习笔记（1）骚操作之Unicode编码/">
              
                  Python学习笔记（1）骚操作之Unicode编码
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-03-04
      </time>
    

    
      
    
    <div class='metatag cats'>
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/Python/">Python</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <p>Python3内部使用的是Unicode编码，所以在变量定义的时候或许可以搞点事情。<br>例如创建一个文件,名为：<code>人.py</code>，里面的代码是：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> 人<span class="params">(object)</span>:</span></span><br><span class="line">    性别 = <span class="string">"男"</span></span><br></pre></td></tr></table></figure></p>
<p>然后同目录下创建一个<code>test.py</code>文件，里面内容为：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> 人 <span class="keyword">import</span> 人</span><br><span class="line">李明 = 人()</span><br><span class="line">print(李明.性别)</span><br></pre></td></tr></table></figure></p>
<p>结果输出：<code>男</code>，是不是有点骚~</p>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/Python/"><i class="fas fa-hashtag fa-fw"></i>Python</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
      
        <div class='post-wrapper'>
          <article class="post reveal ">
    
<section class='meta'>
  
  <div class="meta" id="header-meta">
    
      <h2 class="title">
          <a href="/2018/03/04/《和谐辩证法》笔记与心得（3）吾之平衡辩证法/">
              
                  《和谐辩证法》笔记与心得（3）吾之平衡辩证法
              
          </a>
      </h2>
    

    
      <time class="metatag time">
        <i class="fas fa-calendar-alt fa-fw" aria-hidden="true"></i>&nbsp;2018-03-04
      </time>
    

    
      
    
    <div class='metatag cats'>
        <i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;<a class="categories" href="/categories/哲学/">哲学</a>
    </div>


    

    

    

  </div>
</section>

    <section class="article typo">
        <ol>
<li>吾之所见为，宇宙的发展其实就是<strong>能量间从不平衡状态到平衡状态的迁移</strong>罢了。</li>
<li>事物的稳定与不稳定两种状态可理解为运动之趋势变化抑或不变耳。</li>
<li>事物总是从不平衡的状态朝向平衡的状态运动和发展的，而之所以运动不止，发展不断，则是由于外界的作用又使得事物可能从平衡又趋于不平衡。</li>
<li>事物内部或事物之间的平衡或不平衡的状态实际上就是指在没有外物的作用下，事物之间或事物内部各要素之间能否保持稳定而不变化的状态，对应的是时刻，状态或趋势。而运动或静止对应的是时间，过程或阶段。平衡或不平衡是决定事物或事物之间运动以相互作用的决定因素，而运动与静止则是从不平衡态向平衡态迁移或平衡态迁移到不平衡态的实现过程。</li>
<li>然而吾仍不解的是宇宙之始态为何，宇宙之终态又为何，同时为什么能量非要从不平衡态迁移到平衡态。</li>
<li>举例如：生产关系之调整是生产关系与生产力从不平衡到平衡的过程；生产力之进步是生产力与人类生存发展不平衡到平衡的过程；人类之生存发展是理想与现实之不平衡到平衡的过程；但理想之追求非平衡，而是贪念和占有，故此之平衡终为不可持续之平衡，故马克思理想之共产主义社会终究不会实现与持久，缘何？人类与大自然和环境之极大不平衡也。<br>当然，最好的结果自然是人类延续，欲望无休止，继续开采资源直至到其他星球为止。最坏的结果当为人类自掘坟墓、自取灭亡。</li>
</ol>

        

        
            <div class="full-width auto-padding tags">
                
                    <a href="/tags/哲学/"><i class="fas fa-hashtag fa-fw"></i>哲学</a>
                
            </div>
        
    </section>
</article>

        </div>
      
    
</section>


    <br>
    <div class="prev-next">
        <div class="prev-next">
            
                <a class="prev" rel="prev" href="/">
                    <section class="post prev" >
                        <i class="fas fa-chevron-left" aria-hidden="true"></i>&nbsp;上一页&nbsp;
                    </section>
                </a>
            
            <p class="current">
                2 / 3
            </p>
            
                <a class="next" rel="next" href="/page/3/">
                    <section class="post next">
                        &nbsp;下一页&nbsp;<i class="fas fa-chevron-right" aria-hidden="true"></i>
                    </section>
                </a>
            

        </div>
    </div>



<!-- 根据主题中的设置决定是否在archive中针对摘要部分的MathJax公式加载mathjax.js文件 -->





        </div>
        <aside class='l_side'>
            
  
  
    
      
      
        <section class='author'>
  <div class='content pure'>
    
      <div class='avatar'>
        <img class='avatar' src='https://github.com/smilelight/images/raw/master/new_icon.jpg'/>
      </div>
    
    
      <div class='text'>
        
        
        
          <p><span id="jinrishici-sentence">lightsmile's Blog</span></p>
          <script src="https://sdk.jinrishici.com/v2/browser/jinrishici.js" charset="utf-8"></script>
        
      </div>
    
    
      <div class="social-wrapper">
        
          
            <a href="mailto:iamlightsmile@qq.com" class="social flat-btn" target="_blank" rel="external"><i class="social fas fa-envelope" aria-hidden="true"></i></a>
          
        
          
            <a href="https://github.com/smilelight" class="social flat-btn" target="_blank" rel="external"><i class="social fab fa-github" aria-hidden="true"></i></a>
          
        
          
            <a href="https://music.163.com/m/user/home?id=515917285" class="social flat-btn" target="_blank" rel="external"><i class="social fas fa-music" aria-hidden="true"></i></a>
          
        
          
            <a href="https://www.zhihu.com/people/qian-xiao-80" class="social flat-btn" target="_blank" rel="external"><i class="social fab fa-zhihu" aria-hidden="true"></i></a>
          
        
      </div>
    
  </div>
</section>

      
    
  
    
      
      
        <section class='plain'>
  
<header class='pure'>
  <div><i class="fas fa-bullhorn fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;注意啦～</div>
  
    <a class="rightBtn" target="_blank"
    rel="external nofollow noopener noreferrer"
    href="https://xaoxuu.com/wiki/material-x/"
    title="https://xaoxuu.com/wiki/material-x/">
    <i class="fas fa-question-circle fa-fw"></i></a>
  
</header>

  <div class='content pure'>
    <p>本站使用 <a href="https://xaoxuu.com/wiki/material-x/">Material X</a> 作为主题，喜欢这个主题的朋友可以阅读文档进行安装哦，超喜欢的话还可以安利给身边的朋友哦～</p>

  </div>
</section>

      
    
  
    
      
      
        

      
    
  
    
      
      
        
  <section class='category'>
    
<header class='pure'>
  <div><i class="fas fa-folder-open fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;所有分类</div>
  
</header>

    <div class='content pure'>
      <ul class="entry">
        
          <li><a class="flat-box" title="/categories/Python/" href="/categories/Python/"><div class='name'>Python</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/lua/" href="/categories/lua/"><div class='name'>lua</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/哲学/" href="/categories/哲学/"><div class='name'>哲学</div><div class='badge'>(6)</div></a></li>
        
          <li><a class="flat-box" title="/categories/数学/" href="/categories/数学/"><div class='name'>数学</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/自然语言处理/" href="/categories/自然语言处理/"><div class='name'>自然语言处理</div><div class='badge'>(1)</div></a></li>
        
          <li><a class="flat-box" title="/categories/计算机/" href="/categories/计算机/"><div class='name'>计算机</div><div class='badge'>(2)</div></a></li>
        
      </ul>
    </div>
  </section>


      
    
  
    
      
      
        
  <section class='tagcloud'>
    
<header class='pure'>
  <div><i class="fas fa-fire fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;热门标签</div>
  
</header>

    <div class='content pure'>
      <a href="/tags/GitHub/" style="font-size: 14px; color: #999">GitHub</a> <a href="/tags/NLTK/" style="font-size: 14px; color: #999">NLTK</a> <a href="/tags/Python/" style="font-size: 20.67px; color: #6c6c6c">Python</a> <a href="/tags/Scrapy/" style="font-size: 14px; color: #999">Scrapy</a> <a href="/tags/github/" style="font-size: 14px; color: #999">github</a> <a href="/tags/ltp/" style="font-size: 14px; color: #999">ltp</a> <a href="/tags/lua/" style="font-size: 14px; color: #999">lua</a> <a href="/tags/markdown/" style="font-size: 14px; color: #999">markdown</a> <a href="/tags/哲学/" style="font-size: 24px; color: #555">哲学</a> <a href="/tags/图片绝对地址/" style="font-size: 14px; color: #999">图片绝对地址</a> <a href="/tags/微信小程序/" style="font-size: 14px; color: #999">微信小程序</a> <a href="/tags/抽象/" style="font-size: 14px; color: #999">抽象</a> <a href="/tags/数学/" style="font-size: 14px; color: #999">数学</a> <a href="/tags/概念/" style="font-size: 14px; color: #999">概念</a> <a href="/tags/浏览器插件/" style="font-size: 17.33px; color: #828282">浏览器插件</a> <a href="/tags/爬虫/" style="font-size: 14px; color: #999">爬虫</a> <a href="/tags/算法/" style="font-size: 17.33px; color: #828282">算法</a> <a href="/tags/统计学/" style="font-size: 14px; color: #999">统计学</a> <a href="/tags/自然语言处理/" style="font-size: 20.67px; color: #6c6c6c">自然语言处理</a>
    </div>
  </section>


      
    
  
    
      
      
        <section class='list'>
  
<header class='pure'>
  <div><i class="fas fa-medal fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;我的项目</div>
  
    <a class="rightBtn" target="_blank"
    rel="external nofollow noopener noreferrer"
    href="https://github.com/smilelight?tab=repositories"
    title="https://github.com/smilelight?tab=repositories">
    <i class="fas fa-arrow-right fa-fw"></i></a>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" title="https://github.com/smilelight/todolist/" href="https://github.com/smilelight/todolist/">
          <div class='name'>
            
              <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;微计划日程管理
          </div>
          
            <div class='badge'>(微信小程序)</div>
          
        </a></li>
      
        <li><a class="flat-box" title="https://github.com/smilelight/GithubImagePace/" href="https://github.com/smilelight/GithubImagePace/">
          <div class='name'>
            
              <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;chrome插件
          </div>
          
            <div class='badge'>(chromium)</div>
          
        </a></li>
      
        <li><a class="flat-box" title="https://github.com/smilelight/SchoolInfoPublishSystem/" href="https://github.com/smilelight/SchoolInfoPublishSystem/">
          <div class='name'>
            
              <i class="fas fa-heartbeat fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;校园信息发布系统
          </div>
          
            <div class='badge'>(Android)</div>
          
        </a></li>
      
        <li><a class="flat-box" title="https://github.com/smilelight/MyShoppingWeb/" href="https://github.com/smilelight/MyShoppingWeb/">
          <div class='name'>
            
              <i class="fas fa-cube fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;网上购物系统
          </div>
          
            <div class='badge'>(aardio)</div>
          
        </a></li>
      
    </ul>
  </div>
</section>

      
    
  
    
      
      
        <section class='list'>
  
<header class='pure'>
  <div><i class="fas fa-link fa-fw" aria-hidden="true"></i>&nbsp;&nbsp;特别链接</div>
  
</header>

  <div class='content pure'>
    <ul class="entry">
      
        <li><a class="flat-box" title="https://www.iamlightsmile.com/about/" href="https://www.iamlightsmile.com/about/">
          <div class='name'>
            
              <i class="fas fa-comment-dots fa-fw" aria-hidden="true"></i>
            
            &nbsp;&nbsp;关于我 / 留言板
          </div>
          
        </a></li>
      
    </ul>
  </div>
</section>

      
    
  
    
      
      
        



      
    
  


        </aside>
        <script>setLoadingBarProgress(60);</script>
    </div>
    <a class="s-top fas fa-arrow-up fa-fw" href='javascript:void(0)'></a>
    </div>
    <footer id="footer" class="clearfix">
  
    <div class="social-wrapper">
      
        
          <a href="mailto:iamlightsmile@qq.com" class="social fas fa-envelope flat-btn" target="_blank" rel="external"></a>
        
      
        
          <a href="https://github.com/smilelight" class="social fab fa-github flat-btn" target="_blank" rel="external"></a>
        
      
        
          <a href="https://music.163.com/m/user/home?id=515917285" class="social fas fa-music flat-btn" target="_blank" rel="external"></a>
        
      
        
          <a href="https://www.zhihu.com/people/qian-xiao-80" class="social fab fa-zhihu flat-btn" target="_blank" rel="external"></a>
        
      
    </div>
  
  <br>
  <div><p>博客内容遵循 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh">署名-非商业性使用-相同方式共享 4.0 国际 (CC BY-NC-SA 4.0) 协议</a></p>
</div>
  <div>本站使用 <a href="https://xaoxuu.com/wiki/material-x/" target="_blank" class="codename">Material X</a> 作为主题，总访问量为 <span id="busuanzi_value_site_pv"><i class="fas fa-spinner fa-spin fa-fw" aria-hidden="true"></i></span> 次。
  </div>
  <div>
    <span id="timeDate">载入天数...</span><span id="times">载入时分秒...</span>
    <script>
        var now = new Date();
        function createtime() {
            var grt= new Date("12/22/2017 00:00:00");
            now.setTime(now.getTime()+250);
            days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
            hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
            if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
            mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
            seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
            snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
            document.getElementById("timeDate").innerHTML = "本小站已苟延残喘 "+dnum+" 天 ";
            document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
        }
    setInterval("createtime()",250);
    </script>
    </div>
</footer>


    <script>setLoadingBarProgress(80);</script>
    <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js"></script>

  <script>
    var GOOGLE_CUSTOM_SEARCH_API_KEY = "";
    var GOOGLE_CUSTOM_SEARCH_ENGINE_ID = "";
    var ALGOLIA_API_KEY = "";
    var ALGOLIA_APP_ID = "";
    var ALGOLIA_INDEX_NAME = "";
    var AZURE_SERVICE_NAME = "";
    var AZURE_INDEX_NAME = "";
    var AZURE_QUERY_KEY = "";
    var BAIDU_API_ID = "";
    var SEARCH_SERVICE = "hexo" || "hexo";
    var ROOT = "/"||"/";
    if(!ROOT.endsWith('/'))ROOT += '/';
  </script>


  
    <script src="https://cdn.jsdelivr.net/npm/scrollreveal@4.0.5/dist/scrollreveal.min.js"></script>
    <script type="text/javascript">
      $(function() {
        const $reveal = $('.reveal');
    		if ($reveal.length === 0) return;
    		const sr = ScrollReveal({ distance: 0 });
    		sr.reveal('.reveal');
      });
    </script>
  
  
    <script src="https://cdn.jsdelivr.net/npm/node-waves@0.7.6/dist/waves.min.js"></script>
    <script type="text/javascript">
      $(function() {
        Waves.attach('.flat-btn', ['waves-button']);
        Waves.attach('.float-btn', ['waves-button', 'waves-float']);
        Waves.attach('.float-btn-light', ['waves-button', 'waves-float', 'waves-light']);
        Waves.attach('.flat-box', ['waves-block']);
        Waves.attach('.float-box', ['waves-block', 'waves-float']);
        Waves.attach('.waves-image');
        Waves.init();
      });
    </script>
  
  
    <script async src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-busuanzi@2.3/js/busuanzi.pure.mini.js"></script>
  
  
  


  
  
  
  
    
    <script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@1.0/js/app.js"></script>
<script src="https://cdn.jsdelivr.net/gh/xaoxuu/cdn-material-x@1.0/js/search.js"></script>
    
  






    <script>setLoadingBarProgress(100);</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</body>
</html>
